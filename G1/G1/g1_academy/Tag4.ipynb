{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tag 4: Wahrnehmung, Anwendung & Transfer in Projekte\n",
    "## Unitree G1 Humanoid Roboter Academy\n",
    "\n",
    "### Agenda Tag 4:\n",
    "1. Visuelle Wahrnehmung & Deep Learning\n",
    "2. Objekt- und Umgebungserkennung\n",
    "3. Multi-Sensor-Fusion\n",
    "4. Vollständige System-Integration (Geoff-Stack)\n",
    "5. Projektarbeit: Real-World Challenge\n",
    "6. Transfer: \"Wie setze ich es im Unternehmen um?\"\n",
    "7. Abschlusspräsentationen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Visuelle Wahrnehmung mit Deep Learning\n",
    "\n",
    "### 1.1 Warum Visuelle Wahrnehmung?\n",
    "\n",
    "**LiDAR alleine ist nicht genug:**\n",
    "- ✓ Sehr genaue 3D-Geometrie\n",
    "- ✗ Keine Farb-/Textur-Information\n",
    "- ✗ Keine semantische Information (Was ist ein Tisch? Eine Tür?)\n",
    "\n",
    "**RGB-Kameras + Deep Learning:**\n",
    "- ✓ Objekterkennung (\"Das ist eine Tasse\")\n",
    "- ✓ Szenenverständnis (\"Das ist ein Büro\")\n",
    "- ✓ Texterkennung (OCR)\n",
    "- ✓ Personenerkennung, Gestenerkennung\n",
    "\n",
    "**Kombination LiDAR + RGB:**\n",
    "- **3D Bounding Boxes** um erkannte Objekte\n",
    "- **Semantisches Mapping** (\"Hier ist der Schreibtisch\")\n",
    "- **Robuste Navigation** (Geometrie + Semantik)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Objekterkennung mit YOLO\n",
    "\n",
    "**YOLO (You Only Look Once)** - Echtzeit-Objekterkennung\n",
    "\n",
    "**Setup:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installation (falls nicht vorhanden):\n",
    "# pip install ultralytics opencv-python\n",
    "\n",
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Lade vortrainiertes Modell\n",
    "# model = YOLO('yolov8n.pt')  # Nano (schnell)\n",
    "# model = YOLO('yolov8s.pt')  # Small\n",
    "# model = YOLO('yolov8m.pt')  # Medium (empfohlen für G1)\n",
    "\n",
    "print(\"YOLO Setup:\")\n",
    "print(\"- yolov8n: ~3ms/frame (Jetson)\")\n",
    "print(\"- yolov8s: ~8ms/frame\")\n",
    "print(\"- yolov8m: ~20ms/frame\")\n",
    "print(\"\\nModell erkennt 80 COCO-Klassen:\")\n",
    "print(\"Person, Stuhl, Tisch, Laptop, Tasse, Flasche, ...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Live-Detection auf RealSense Stream:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beispiel: Objekterkennung auf RGB-Frame\n",
    "def detect_objects_in_frame(rgb_frame, model):\n",
    "    \"\"\"\n",
    "    Führt YOLO-Detection auf RGB-Frame aus\n",
    "    \n",
    "    rgb_frame: numpy array (H, W, 3)\n",
    "    model: YOLO Modell\n",
    "    \n",
    "    Returns: Liste von Detektionen\n",
    "    \"\"\"\n",
    "    # Inference\n",
    "    results = model(rgb_frame, verbose=False)\n",
    "    \n",
    "    detections = []\n",
    "    for result in results:\n",
    "        boxes = result.boxes\n",
    "        for box in boxes:\n",
    "            # Extrahiere Info\n",
    "            x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()\n",
    "            confidence = box.conf[0].cpu().numpy()\n",
    "            class_id = int(box.cls[0].cpu().numpy())\n",
    "            class_name = model.names[class_id]\n",
    "            \n",
    "            detections.append({\n",
    "                'bbox': (x1, y1, x2, y2),\n",
    "                'confidence': confidence,\n",
    "                'class': class_name\n",
    "            })\n",
    "    \n",
    "    return detections\n",
    "\n",
    "def draw_detections(frame, detections):\n",
    "    \"\"\"\n",
    "    Zeichnet Bounding Boxes auf Frame\n",
    "    \"\"\"\n",
    "    for det in detections:\n",
    "        x1, y1, x2, y2 = det['bbox']\n",
    "        conf = det['confidence']\n",
    "        cls = det['class']\n",
    "        \n",
    "        # Zeichne Box\n",
    "        cv2.rectangle(frame, (int(x1), int(y1)), (int(x2), int(y2)), \n",
    "                     (0, 255, 0), 2)\n",
    "        \n",
    "        # Label\n",
    "        label = f\"{cls} {conf:.2f}\"\n",
    "        cv2.putText(frame, label, (int(x1), int(y1)-10),\n",
    "                   cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "    \n",
    "    return frame\n",
    "\n",
    "print(\"Objekterkennungs-Funktionen definiert.\")\n",
    "print(\"\\nVerwendung:\")\n",
    "print(\"detections = detect_objects_in_frame(rgb_frame, model)\")\n",
    "print(\"annotated_frame = draw_detections(rgb_frame, detections)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 3D Object Localization (RGB-D)\n",
    "\n",
    "**Ziel:** Von 2D Bounding Box → 3D Position im Raum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_3d_position_from_bbox(bbox, depth_frame, camera_intrinsics):\n",
    "    \"\"\"\n",
    "    Berechnet 3D-Position eines Objekts aus 2D BBox + Tiefenbild\n",
    "    \n",
    "    bbox: (x1, y1, x2, y2)\n",
    "    depth_frame: Tiefenbild (H, W) in Metern\n",
    "    camera_intrinsics: Dict mit fx, fy, cx, cy\n",
    "    \n",
    "    Returns: (x, y, z) in Kamera-Koordinaten\n",
    "    \"\"\"\n",
    "    x1, y1, x2, y2 = bbox\n",
    "    \n",
    "    # Zentrum der BBox\n",
    "    center_x = int((x1 + x2) / 2)\n",
    "    center_y = int((y1 + y2) / 2)\n",
    "    \n",
    "    # Tiefe am Zentrum (Median für Robustheit)\n",
    "    depth_patch = depth_frame[\n",
    "        int(y1):int(y2),\n",
    "        int(x1):int(x2)\n",
    "    ]\n",
    "    \n",
    "    # Filtere Null-Werte\n",
    "    valid_depths = depth_patch[depth_patch > 0]\n",
    "    if len(valid_depths) == 0:\n",
    "        return None\n",
    "    \n",
    "    depth = np.median(valid_depths)\n",
    "    \n",
    "    # Deprojection (Pixel → 3D)\n",
    "    fx = camera_intrinsics['fx']\n",
    "    fy = camera_intrinsics['fy']\n",
    "    cx = camera_intrinsics['cx']\n",
    "    cy = camera_intrinsics['cy']\n",
    "    \n",
    "    x_3d = (center_x - cx) * depth / fx\n",
    "    y_3d = (center_y - cy) * depth / fy\n",
    "    z_3d = depth\n",
    "    \n",
    "    return (x_3d, y_3d, z_3d)\n",
    "\n",
    "# RealSense D435i Intrinsics (typisch)\n",
    "realsense_intrinsics = {\n",
    "    'fx': 615.0,\n",
    "    'fy': 615.0,\n",
    "    'cx': 320.0,\n",
    "    'cy': 240.0\n",
    "}\n",
    "\n",
    "# Beispiel:\n",
    "example_bbox = (100, 150, 200, 300)  # Pixel\n",
    "example_depth = np.random.rand(480, 640) * 3.0  # Simuliert\n",
    "\n",
    "pos_3d = get_3d_position_from_bbox(example_bbox, example_depth, realsense_intrinsics)\n",
    "if pos_3d:\n",
    "    print(f\"Objekt-Position: x={pos_3d[0]:.2f}m, y={pos_3d[1]:.2f}m, z={pos_3d[2]:.2f}m\")\n",
    "    print(\"(relativ zur Kamera)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Semantisches Mapping\n",
    "\n",
    "### 2.1 Konzept\n",
    "\n",
    "**Klassische Karte:**\n",
    "```\n",
    "Grid: [0, 0, 1, 1, 0, ...]\n",
    "      (frei, frei, belegt, belegt, frei)\n",
    "```\n",
    "\n",
    "**Semantische Karte:**\n",
    "```\n",
    "Grid: [(x, y): \"Stuhl\",\n",
    "       (x, y): \"Tisch\",\n",
    "       (x, y): \"Tür\",\n",
    "       ...]\n",
    "```\n",
    "\n",
    "**Vorteil:**\n",
    "- High-Level Navigation: \"Gehe zum Tisch\"\n",
    "- Kontext-bewusste Planung: \"Vermeide Stühle, aber nutze Türen\"\n",
    "- Besseres Szenenverständnis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Semantic Map Datenstruktur\n",
    "class SemanticMap:\n",
    "    def __init__(self):\n",
    "        self.objects = []  # Liste von semantischen Objekten\n",
    "    \n",
    "    def add_object(self, class_name, position_3d, bbox_2d=None):\n",
    "        \"\"\"\n",
    "        Fügt Objekt zur Karte hinzu\n",
    "        \n",
    "        class_name: z.B. \"chair\", \"table\"\n",
    "        position_3d: (x, y, z) in Weltkoordinaten\n",
    "        bbox_2d: Optional, für Visualisierung\n",
    "        \"\"\"\n",
    "        obj = {\n",
    "            'class': class_name,\n",
    "            'position': position_3d,\n",
    "            'bbox': bbox_2d,\n",
    "            'timestamp': time.time()\n",
    "        }\n",
    "        \n",
    "        # Prüfe ob Objekt bereits existiert (Duplikate vermeiden)\n",
    "        for existing in self.objects:\n",
    "            if (existing['class'] == class_name and\n",
    "                np.linalg.norm(np.array(existing['position']) - \n",
    "                              np.array(position_3d)) < 0.5):  # 50cm Toleranz\n",
    "                # Update statt neu hinzufügen\n",
    "                existing['position'] = position_3d\n",
    "                existing['timestamp'] = time.time()\n",
    "                return\n",
    "        \n",
    "        self.objects.append(obj)\n",
    "    \n",
    "    def find_objects_by_class(self, class_name):\n",
    "        \"\"\"Finde alle Objekte einer bestimmten Klasse\"\"\"\n",
    "        return [obj for obj in self.objects if obj['class'] == class_name]\n",
    "    \n",
    "    def find_nearest_object(self, class_name, robot_position):\n",
    "        \"\"\"Finde nächstes Objekt einer Klasse\"\"\"\n",
    "        candidates = self.find_objects_by_class(class_name)\n",
    "        if not candidates:\n",
    "            return None\n",
    "        \n",
    "        nearest = min(candidates, \n",
    "                     key=lambda obj: np.linalg.norm(\n",
    "                         np.array(obj['position'][:2]) - \n",
    "                         np.array(robot_position)))\n",
    "        return nearest\n",
    "\n",
    "# Test:\n",
    "import time\n",
    "semantic_map = SemanticMap()\n",
    "semantic_map.add_object('chair', (2.0, 1.0, 0.5))\n",
    "semantic_map.add_object('table', (3.0, 0.0, 0.8))\n",
    "semantic_map.add_object('chair', (2.5, -1.0, 0.5))\n",
    "\n",
    "print(f\"Karte enthält {len(semantic_map.objects)} Objekte\")\n",
    "chairs = semantic_map.find_objects_by_class('chair')\n",
    "print(f\"Davon {len(chairs)} Stühle\")\n",
    "\n",
    "robot_pos = (0.0, 0.0)\n",
    "nearest_chair = semantic_map.find_nearest_object('chair', robot_pos)\n",
    "if nearest_chair:\n",
    "    print(f\"Nächster Stuhl bei: {nearest_chair['position']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Multi-Sensor-Fusion\n",
    "\n",
    "### 3.1 Sensor-Komplementarität\n",
    "\n",
    "| Sensor | Stärken | Schwächen |\n",
    "|--------|---------|----------|\n",
    "| **LiDAR** | Präzise 3D-Geometrie, große Reichweite | Keine Farbe/Semantik, teuer |\n",
    "| **RGB** | Farbe, Textur, Semantik (DL) | Lichtabhängig, keine Tiefe |\n",
    "| **Depth (RealSense)** | Tiefe + RGB, günstig | Begrenzte Reichweite (~10m) |\n",
    "| **IMU** | Schnelle Orientierung, drift | Akkumulierter Fehler, keine Position |\n",
    "\n",
    "**Fusion-Strategie:**\n",
    "- **LiDAR**: Globale Karte, präzise Lokalisierung\n",
    "- **RGB-D**: Objekterkennung, nahe Hindernisvermeidung\n",
    "- **IMU**: Schnelle Orientierungs-Updates zwischen LiDAR-Scans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Koordinatentransformationen\n",
    "\n",
    "**Problem:** Sensoren haben unterschiedliche Koordinatensysteme\n",
    "\n",
    "```\n",
    "         Base (Roboter)\n",
    "              |\n",
    "      ┌───────┴───────┐\n",
    "      │               │\n",
    "   LiDAR          RealSense\n",
    "   (0, 0, 0.3)    (0.1, 0, 0.25)\n",
    "```\n",
    "\n",
    "**Lösung: TF Tree (Transformation Tree)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class TransformTree:\n",
    "    def __init__(self):\n",
    "        # Statische Transformationen (fest montiert)\n",
    "        self.transforms = {\n",
    "            'base_to_lidar': np.array([\n",
    "                [1, 0, 0, 0.0],\n",
    "                [0, 1, 0, 0.0],\n",
    "                [0, 0, 1, 0.3],  # 30cm über Base\n",
    "                [0, 0, 0, 1]\n",
    "            ]),\n",
    "            'base_to_realsense': np.array([\n",
    "                [1, 0, 0, 0.1],   # 10cm nach vorne\n",
    "                [0, 1, 0, 0.0],\n",
    "                [0, 0, 1, 0.25],  # 25cm hoch\n",
    "                [0, 0, 0, 1]\n",
    "            ])\n",
    "        }\n",
    "    \n",
    "    def transform_point(self, point, from_frame, to_frame):\n",
    "        \"\"\"\n",
    "        Transformiert Punkt von einem Frame zu anderem\n",
    "        \n",
    "        point: (x, y, z)\n",
    "        from_frame: z.B. 'realsense'\n",
    "        to_frame: z.B. 'base' oder 'lidar'\n",
    "        \"\"\"\n",
    "        # Konvertiere zu homogenen Koordinaten\n",
    "        point_homo = np.array([point[0], point[1], point[2], 1.0])\n",
    "        \n",
    "        # Transformation\n",
    "        if from_frame == 'realsense' and to_frame == 'base':\n",
    "            # Inverse Transformation\n",
    "            T_inv = np.linalg.inv(self.transforms['base_to_realsense'])\n",
    "            point_transformed = T_inv @ point_homo\n",
    "        elif from_frame == 'base' and to_frame == 'realsense':\n",
    "            point_transformed = self.transforms['base_to_realsense'] @ point_homo\n",
    "        else:\n",
    "            # Generisch: via base frame\n",
    "            # from_frame → base → to_frame\n",
    "            pass\n",
    "        \n",
    "        return point_transformed[:3]\n",
    "\n",
    "# Test:\n",
    "tf_tree = TransformTree()\n",
    "\n",
    "# Objekt bei (1.0, 0.0, 0.5) in RealSense-Frame\n",
    "obj_realsense = (1.0, 0.0, 0.5)\n",
    "obj_base = tf_tree.transform_point(obj_realsense, 'realsense', 'base')\n",
    "\n",
    "print(f\"Objekt in RealSense-Frame: {obj_realsense}\")\n",
    "print(f\"Objekt in Base-Frame: {obj_base}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Geoff-Stack: Vollständige Integration\n",
    "\n",
    "### 4.1 Geoff-Stack Architektur\n",
    "\n",
    "Das Repository enthält **Geoff-Stack** - eine vollständige GUI-basierte Integration:\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────┐\n",
    "│          run_geoff_gui.py (PySide6)             │\n",
    "├──────────────┬────────────┬─────────────────────┤\n",
    "│ RGB View     │ Depth View │ 2D SLAM Map         │\n",
    "│ (640x480)    │ (Colorized)│ (Bird's Eye)        │\n",
    "│              │            │                     │\n",
    "│  [Live       │  [Live     │  [Occupancy Grid]   │\n",
    "│   Camera]    │   Depth]   │                     │\n",
    "├──────────────┴────────────┴─────────────────────┤\n",
    "│         3D Point Cloud (pyqtgraph)              │\n",
    "│         [Interactive 3D Visualization]          │\n",
    "├─────────────────────────────────────────────────┤\n",
    "│  Status: vx=0.2 vy=0.0 ω=0.0 | Battery: 85%    │\n",
    "└─────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "**Background Threads:**\n",
    "1. **RealSense RX**: Empfängt RGB+Depth via GStreamer\n",
    "2. **SLAM Thread**: KISS-ICP Verarbeitung\n",
    "3. **Battery Monitor**: Liest Akku-Status\n",
    "4. **Keyboard Controller**: WASD-Steuerung\n",
    "5. **GUI Render**: 30 Hz Update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Geoff-GUI starten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Im Terminal:\n",
    "# cd ../unitree_g1_vibes\n",
    "# python3 run_geoff_gui.py --iface enp68s0f1\n",
    "\n",
    "print(\"Geoff-GUI Features:\")\n",
    "print(\"====================\")\n",
    "print(\"✓ Vier-Panel-Ansicht (RGB, Depth, 2D Map, 3D Cloud)\")\n",
    "print(\"✓ Echtzeit SLAM-Visualisierung\")\n",
    "print(\"✓ Keyboard-Steuerung integriert (WASD)\")\n",
    "print(\"✓ Batterie-Monitor\")\n",
    "print(\"✓ FPS-Overlay\")\n",
    "print(\"✓ Thread-basiert, non-blocking\")\n",
    "print(\"\\nStart:\")\n",
    "print(\"python3 run_geoff_gui.py --iface <network_interface>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Eigene Integration erweitern\n",
    "\n",
    "**Beispiel: YOLO in Geoff-GUI integrieren**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pseudo-Code: YOLO Integration\n",
    "# (Basierend auf run_geoff_gui.py)\n",
    "\n",
    "from ultralytics import YOLO\n",
    "import threading\n",
    "\n",
    "class GeoffGuiExtended:\n",
    "    def __init__(self):\n",
    "        # ... existing setup ...\n",
    "        \n",
    "        # YOLO hinzufügen\n",
    "        self.yolo_model = YOLO('yolov8m.pt')\n",
    "        self.detections = []\n",
    "        self.detection_lock = threading.Lock()\n",
    "        \n",
    "        # Detection Thread\n",
    "        self.detection_thread = threading.Thread(\n",
    "            target=self.detection_loop,\n",
    "            daemon=True\n",
    "        )\n",
    "        self.detection_thread.start()\n",
    "    \n",
    "    def detection_loop(self):\n",
    "        \"\"\"Läuft mit ~10 Hz (nicht bei jedem Frame)\"\"\"\n",
    "        while self.running:\n",
    "            # Hole aktuellen RGB-Frame\n",
    "            with self.rgb_lock:\n",
    "                rgb_frame = self.rgb_frame.copy()\n",
    "            \n",
    "            # YOLO Inference\n",
    "            results = self.yolo_model(rgb_frame, verbose=False)\n",
    "            \n",
    "            # Extrahiere Detektionen\n",
    "            detections = []\n",
    "            for result in results:\n",
    "                for box in result.boxes:\n",
    "                    # ... extract info ...\n",
    "                    detections.append({...})\n",
    "            \n",
    "            # Update shared state\n",
    "            with self.detection_lock:\n",
    "                self.detections = detections\n",
    "            \n",
    "            time.sleep(0.1)  # 10 Hz\n",
    "    \n",
    "    def render_rgb_panel(self):\n",
    "        \"\"\"Überschreibe Render-Funktion\"\"\"\n",
    "        # ... get rgb_frame ...\n",
    "        \n",
    "        # Zeichne Detektionen\n",
    "        with self.detection_lock:\n",
    "            for det in self.detections:\n",
    "                # Draw bounding box\n",
    "                cv2.rectangle(rgb_frame, ...)\n",
    "        \n",
    "        # ... update Qt widget ...\n",
    "\n",
    "print(\"YOLO kann in Geoff-GUI als zusätzlicher Thread integriert werden.\")\n",
    "print(\"Wichtig: Lock-Protected Shared State für Thread-Safety!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Projektarbeit: Real-World Challenge\n",
    "\n",
    "### 5.1 Challenge-Beschreibung\n",
    "\n",
    "**Titel: \"Autonomer Büro-Assistent\"**\n",
    "\n",
    "**Aufgabe:**\n",
    "Der G1 soll autonom:\n",
    "1. Einen Raum scannen und Karte erstellen\n",
    "2. Bestimmte Objekte finden (z.B. \"Finde eine Tasse\")\n",
    "3. Zum Objekt navigieren\n",
    "4. Greifbewegung ausführen (simuliert oder real)\n",
    "5. Zurück zu Startposition\n",
    "\n",
    "**Technologien:**\n",
    "- SLAM (KISS-ICP)\n",
    "- Objekterkennung (YOLO)\n",
    "- Pfadplanung (A*)\n",
    "- Lokale Steuerung (DWA)\n",
    "- Arm-RL-Policy (optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Implementierungs-Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Challenge Implementation Template\n",
    "\n",
    "class OfficeAssistantChallenge:\n",
    "    def __init__(self, bot, slam, yolo_model):\n",
    "        self.bot = bot\n",
    "        self.slam = slam\n",
    "        self.yolo = yolo_model\n",
    "        self.semantic_map = SemanticMap()\n",
    "    \n",
    "    def run_challenge(self, target_object='cup'):\n",
    "        \"\"\"\n",
    "        Hauptschleife der Challenge\n",
    "        \"\"\"\n",
    "        print(f\"=== Starte Challenge: Finde '{target_object}' ===\")\n",
    "        \n",
    "        # Phase 1: Exploration & Mapping\n",
    "        print(\"\\n[Phase 1] Raum scannen...\")\n",
    "        self.explore_room(duration=30.0)\n",
    "        \n",
    "        # Phase 2: Object Detection\n",
    "        print(f\"\\n[Phase 2] Suche nach '{target_object}'...\")\n",
    "        target_pos = self.find_object(target_object)\n",
    "        \n",
    "        if target_pos is None:\n",
    "            print(f\"✗ '{target_object}' nicht gefunden!\")\n",
    "            return False\n",
    "        \n",
    "        print(f\"✓ '{target_object}' gefunden bei {target_pos}\")\n",
    "        \n",
    "        # Phase 3: Navigation\n",
    "        print(f\"\\n[Phase 3] Navigiere zu {target_pos}...\")\n",
    "        success = self.navigate_to_position(target_pos)\n",
    "        \n",
    "        if not success:\n",
    "            print(\"✗ Navigation fehlgeschlagen!\")\n",
    "            return False\n",
    "        \n",
    "        # Phase 4: Manipulation (optional)\n",
    "        print(\"\\n[Phase 4] Greife Objekt...\")\n",
    "        # self.reach_and_grasp(target_pos)\n",
    "        print(\"(Greifen simuliert)\")\n",
    "        \n",
    "        # Phase 5: Return Home\n",
    "        print(\"\\n[Phase 5] Kehre zurück zu Start...\")\n",
    "        self.navigate_to_position((0, 0, 0))\n",
    "        \n",
    "        print(\"\\n=== Challenge abgeschlossen! ===\")\n",
    "        return True\n",
    "    \n",
    "    def explore_room(self, duration):\n",
    "        \"\"\"\n",
    "        Frontier-based Exploration\n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        while time.time() - start_time < duration:\n",
    "            # 1. Update SLAM\n",
    "            # points = lidar.get_latest_points()\n",
    "            # pose = slam.register_frame(points)\n",
    "            \n",
    "            # 2. Run Object Detection\n",
    "            # rgb_frame = realsense.get_rgb()\n",
    "            # detections = yolo(rgb_frame)\n",
    "            # for det in detections:\n",
    "            #     pos_3d = get_3d_position(...)\n",
    "            #     self.semantic_map.add_object(det['class'], pos_3d)\n",
    "            \n",
    "            # 3. Simple Exploration Strategy\n",
    "            # bot.Move(vx=0.1, vy=0.0, omega=0.2)  # Spirale\n",
    "            \n",
    "            time.sleep(0.1)\n",
    "        \n",
    "        # bot.StopMove()\n",
    "        print(f\"Exploration abgeschlossen. {len(self.semantic_map.objects)} Objekte gefunden.\")\n",
    "    \n",
    "    def find_object(self, class_name):\n",
    "        \"\"\"\n",
    "        Sucht Objekt in semantischer Karte\n",
    "        \"\"\"\n",
    "        objects = self.semantic_map.find_objects_by_class(class_name)\n",
    "        if not objects:\n",
    "            return None\n",
    "        \n",
    "        # Wähle nächstes\n",
    "        robot_pos = (0, 0)  # TODO: Von SLAM holen\n",
    "        nearest = self.semantic_map.find_nearest_object(class_name, robot_pos)\n",
    "        return nearest['position']\n",
    "    \n",
    "    def navigate_to_position(self, goal_pos):\n",
    "        \"\"\"\n",
    "        Navigation mit A* + DWA\n",
    "        \"\"\"\n",
    "        # (Siehe Tag 3 für Details)\n",
    "        print(f\"Navigation zu {goal_pos} (implementiert in Tag 3)\")\n",
    "        return True\n",
    "\n",
    "print(\"Challenge-Template definiert.\")\n",
    "print(\"\\nVerwendung:\")\n",
    "print(\"challenge = OfficeAssistantChallenge(bot, slam, yolo_model)\")\n",
    "print(\"challenge.run_challenge(target_object='cup')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Team-Aufgaben\n",
    "\n",
    "**Teilen Sie sich in 3-4er Teams auf:**\n",
    "\n",
    "**Team 1: Perception**\n",
    "- Optimiere YOLO-Performance\n",
    "- Implementiere 3D Object Localization\n",
    "- Baue Semantische Karte auf\n",
    "\n",
    "**Team 2: Navigation**\n",
    "- Implementiere robuste Pfadplanung\n",
    "- Tune DWA-Parameter\n",
    "- Handle Edge-Cases (kein Pfad, Deadlocks)\n",
    "\n",
    "**Team 3: Integration**\n",
    "- Verbinde alle Komponenten\n",
    "- Baue State Machine für Challenge-Ablauf\n",
    "- Erstelle GUI/Monitoring\n",
    "\n",
    "**Team 4: Manipulation (optional)**\n",
    "- Trainiere/tune RL-Policy für Arm\n",
    "- Implementiere Greif-Logik\n",
    "- Teste Sim-to-Real Transfer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Transfer: \"Wie setze ich es im Unternehmen um?\"\n",
    "\n",
    "### 6.1 Von Prototyp zu Produktion\n",
    "\n",
    "**Prototyp (Academy):**\n",
    "- Einzelner Roboter\n",
    "- Manuelle Starts\n",
    "- Entwickler-Umgebung\n",
    "- Jupyter Notebooks\n",
    "\n",
    "**Produktion (Unternehmen):**\n",
    "- Flotte von Robotern\n",
    "- Automatisierte Deployment\n",
    "- 24/7 Betrieb\n",
    "- Monitoring & Logging\n",
    "\n",
    "### 6.2 Deployment-Checkliste\n",
    "\n",
    "**1. Software-Engineering:**\n",
    "```python\n",
    "# Von Notebook zu Modul\n",
    "# Bad:\n",
    "# - Code in .ipynb\n",
    "# - Globale Variablen\n",
    "# - Keine Tests\n",
    "\n",
    "# Good:\n",
    "# - Strukturierte Packages (src/perception, src/navigation, ...)\n",
    "# - Unit Tests (pytest)\n",
    "# - CI/CD Pipeline (GitHub Actions)\n",
    "# - Docker Container\n",
    "```\n",
    "\n",
    "**2. Monitoring & Logging:**\n",
    "```python\n",
    "import logging\n",
    "\n",
    "# Setup centralized logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s [%(name)s] %(levelname)s: %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler('/var/log/g1_robot.log'),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "\n",
    "logger = logging.getLogger('g1.navigation')\n",
    "logger.info(\"Started navigation to waypoint (2.0, 3.0)\")\n",
    "```\n",
    "\n",
    "**3. Fehlerbehandlung:**\n",
    "```python\n",
    "# Robust gegen Fehler\n",
    "try:\n",
    "    path = astar_planning(grid, start, goal)\n",
    "except Exception as e:\n",
    "    logger.error(f\"Path planning failed: {e}\")\n",
    "    # Fallback: Return to safe position\n",
    "    bot.navigate_to_home()\n",
    "    # Alert operator\n",
    "    send_alert(\"Navigation failure\", severity=\"high\")\n",
    "```\n",
    "\n",
    "**4. Configuration Management:**\n",
    "```yaml\n",
    "# config.yaml\n",
    "robot:\n",
    "  id: \"G1-001\"\n",
    "  network_interface: \"enp68s0f1\"\n",
    "\n",
    "perception:\n",
    "  yolo_model: \"yolov8m.pt\"\n",
    "  confidence_threshold: 0.6\n",
    "\n",
    "navigation:\n",
    "  max_velocity: 0.5\n",
    "  safety_margin: 0.3\n",
    "  planner: \"astar\"\n",
    "```\n",
    "\n",
    "**5. Remote Monitoring:**\n",
    "```python\n",
    "# Metrics export (Prometheus/Grafana)\n",
    "from prometheus_client import Counter, Gauge, start_http_server\n",
    "\n",
    "navigation_success = Counter('navigation_success_total', 'Successful navigations')\n",
    "battery_level = Gauge('battery_level_percent', 'Current battery level')\n",
    "cpu_usage = Gauge('cpu_usage_percent', 'CPU usage')\n",
    "\n",
    "# Start metrics server\n",
    "start_http_server(8000)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Use-Cases im Unternehmen\n",
    "\n",
    "**Logistik & Warehousing:**\n",
    "- Inventur-Automatisierung (Regale scannen)\n",
    "- Waren-Transport (Pick & Place)\n",
    "- Lager-Mapping\n",
    "\n",
    "**Inspektion & Wartung:**\n",
    "- Anlageninspektion (Ventile ablesen, Lecks erkennen)\n",
    "- Routinekontrollen (täglich gleichr Rundgang)\n",
    "- Dokumentation (Fotos, Messwerte sammeln)\n",
    "\n",
    "**Service & Kundeninteraktion:**\n",
    "- Empfangsroboter (Gäste begrüßen, Führungen)\n",
    "- Informations-Terminal (Fragen beantworten)\n",
    "- Lieferservice (innerhalb Gebäude)\n",
    "\n",
    "**Forschung & Entwicklung:**\n",
    "- Plattform für Algorithmen-Entwicklung\n",
    "- Benchmark-Tests (Vergleich verschiedener Ansätze)\n",
    "- Sim-to-Real Validierung"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4 ROI-Kalkulation\n",
    "\n",
    "**Beispiel: Inventur-Automatisierung**\n",
    "\n",
    "**Kosten:**\n",
    "- Unitree G1: ~€80,000\n",
    "- Entwicklung (3 Monate): €30,000\n",
    "- Wartung (jährlich): €5,000\n",
    "- **Total (Jahr 1)**: €115,000\n",
    "\n",
    "**Einsparungen:**\n",
    "- 2 Mitarbeiter à €40,000/Jahr = €80,000\n",
    "- Zeitersparnis (24/7 Betrieb): +30% Durchsatz\n",
    "- Fehlerreduktion: -20% Fehlbestände → €20,000/Jahr\n",
    "- **Total Einsparungen**: €100,000/Jahr\n",
    "\n",
    "**Break-Even:** ~14 Monate\n",
    "\n",
    "**Wichtig:**\n",
    "- Realistische Erwartungen\n",
    "- Pilotphase einplanen\n",
    "- Edge-Cases berücksichtigen\n",
    "- Training & Support einkalkulieren"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Best Practices & Lessons Learned\n",
    "\n",
    "### 7.1 Technische Best Practices\n",
    "\n",
    "**1. Modularität:**\n",
    "```python\n",
    "# Good: Austauschbare Komponenten\n",
    "class Planner(ABC):\n",
    "    @abstractmethod\n",
    "    def plan(self, start, goal):\n",
    "        pass\n",
    "\n",
    "class AStarPlanner(Planner):\n",
    "    def plan(self, start, goal):\n",
    "        # A* implementation\n",
    "        pass\n",
    "\n",
    "class RRTPlanner(Planner):\n",
    "    def plan(self, start, goal):\n",
    "        # RRT implementation\n",
    "        pass\n",
    "\n",
    "# Einfach austauschbar:\n",
    "planner = AStarPlanner()  # oder RRTPlanner()\n",
    "```\n",
    "\n",
    "**2. Fail-Safe Design:**\n",
    "- Immer Fallback-Strategien\n",
    "- Watchdogs für kritische Threads\n",
    "- Graceful Degradation (lieber langsam als Absturz)\n",
    "\n",
    "**3. Testing:**\n",
    "```python\n",
    "# Unit Tests\n",
    "def test_astar_simple_path():\n",
    "    grid = np.zeros((100, 100))\n",
    "    start = (10, 10)\n",
    "    goal = (90, 90)\n",
    "    path = astar_planning(grid, start, goal)\n",
    "    assert path is not None\n",
    "    assert path[0] == start\n",
    "    assert path[-1] == goal\n",
    "\n",
    "# Integration Tests\n",
    "def test_full_navigation_pipeline():\n",
    "    # Test entire chain: SLAM → Planning → Control\n",
    "    pass\n",
    "```\n",
    "\n",
    "**4. Documentation:**\n",
    "```python\n",
    "def navigate_to_goal(bot, slam, goal, max_retries=3):\n",
    "    \"\"\"\n",
    "    Navigiert Roboter zu Zielpunkt mit Retry-Logik.\n",
    "    \n",
    "    Args:\n",
    "        bot (LocoClient): Roboter-Steuerung\n",
    "        slam (KissICP): SLAM-Instanz für Lokalisierung\n",
    "        goal (tuple): (x, y) Zielposition in Metern\n",
    "        max_retries (int): Maximale Wiederholungen bei Fehler\n",
    "    \n",
    "    Returns:\n",
    "        bool: True wenn erfolgreich, False sonst\n",
    "    \n",
    "    Raises:\n",
    "        NavigationError: Bei kritischen Fehlern\n",
    "    \n",
    "    Example:\n",
    "        >>> success = navigate_to_goal(bot, slam, (5.0, 3.0))\n",
    "        >>> if success:\n",
    "        ...     print(\"Ziel erreicht\")\n",
    "    \"\"\"\n",
    "    pass\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Organizational Best Practices\n",
    "\n",
    "**1. Cross-Functional Teams:**\n",
    "- Robotik-Ingenieure\n",
    "- Software-Entwickler\n",
    "- Domain-Experten (z.B. Logistik)\n",
    "- Safety-Officers\n",
    "\n",
    "**2. Iterative Development:**\n",
    "- Sprint 1: Basic Movement\n",
    "- Sprint 2: SLAM Integration\n",
    "- Sprint 3: Object Detection\n",
    "- Sprint 4: Full Navigation\n",
    "- Sprint 5: Error Handling\n",
    "- Sprint 6: Production Deployment\n",
    "\n",
    "**3. Risk Management:**\n",
    "- Identify risks early (Sensor failures, network issues)\n",
    "- Mitigation strategies\n",
    "- Regular safety audits\n",
    "\n",
    "**4. Knowledge Transfer:**\n",
    "- Internal workshops (wie diese Academy!)\n",
    "- Documentation wiki\n",
    "- Code reviews\n",
    "- Pair programming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Abschlusspräsentation - Template\n",
    "\n",
    "### Präsentations-Struktur (10 Minuten)\n",
    "\n",
    "**Folie 1: Titel**\n",
    "- Team-Name\n",
    "- Challenge-Titel\n",
    "- Team-Mitglieder\n",
    "\n",
    "**Folie 2: Problem Statement**\n",
    "- Was war die Aufgabe?\n",
    "- Warum ist das wichtig?\n",
    "- Was waren die Herausforderungen?\n",
    "\n",
    "**Folie 3: Lösungsansatz**\n",
    "- Architektur-Diagramm\n",
    "- Welche Technologien?\n",
    "- Warum diese Wahl?\n",
    "\n",
    "**Folie 4: Implementation**\n",
    "- Kernalgorithmen\n",
    "- Code-Snippets (max. 10 Zeilen)\n",
    "- Besondere Tricks/Optimierungen\n",
    "\n",
    "**Folie 5: Demo**\n",
    "- Video (1-2 Minuten)\n",
    "- Oder Live-Demo\n",
    "- Zeige Erfolge UND Fehler\n",
    "\n",
    "**Folie 6: Ergebnisse**\n",
    "- Quantitative Metriken (Erfolgsrate, Laufzeit, ...)\n",
    "- Qualitative Beobachtungen\n",
    "- Vergleich zu Baseline/Alternativen\n",
    "\n",
    "**Folie 7: Lessons Learned**\n",
    "- Was hat funktioniert?\n",
    "- Was nicht?\n",
    "- Was würden Sie anders machen?\n",
    "\n",
    "**Folie 8: Future Work**\n",
    "- Nächste Schritte\n",
    "- Wie zu Produktion skalieren?\n",
    "- Weitere Anwendungsfälle\n",
    "\n",
    "**Folie 9: Q&A**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. Zusammenfassung: 4-Tage Academy\n",
    "\n",
    "### Tag 1: Grundlagen\n",
    "✓ Hardware-Architektur (DOF, Sensoren, Aktuatoren)  \n",
    "✓ Software-Stack (FSM, SDK, Threading)  \n",
    "✓ Setup & Netzwerk  \n",
    "✓ Hanger Boot Sequence  \n",
    "✓ Erste Bewegungen  \n",
    "\n",
    "### Tag 2: Bewegungssteuerung\n",
    "✓ Dynamisches Gehen & Stabilität  \n",
    "✓ PID-Regelung  \n",
    "✓ Reinforcement Learning  \n",
    "✓ Keyboard-Tele-Operation  \n",
    "✓ Trajektorien-Planung  \n",
    "\n",
    "### Tag 3: Navigation\n",
    "✓ SLAM (KISS-ICP)  \n",
    "✓ LiDAR-Integration  \n",
    "✓ Globale Pfadplanung (A*, RRT)  \n",
    "✓ Lokale Hindernisvermeidung (DWA)  \n",
    "✓ RealSense RGB-D  \n",
    "\n",
    "### Tag 4: Wahrnehmung & Integration\n",
    "✓ Objekterkennung (YOLO)  \n",
    "✓ Semantisches Mapping  \n",
    "✓ Multi-Sensor-Fusion  \n",
    "✓ Geoff-Stack Integration  \n",
    "✓ Real-World Challenge  \n",
    "✓ Transfer ins Unternehmen  \n",
    "\n",
    "### Wichtigste Takeaways:\n",
    "1. **Modularität**: Komponenten austauschbar halten\n",
    "2. **Robustheit**: Fehlerbehandlung von Anfang an\n",
    "3. **Testing**: Unit + Integration Tests essentiell\n",
    "4. **Monitoring**: Logging & Metrics für Produktion\n",
    "5. **Iteration**: Von einfach zu komplex, schrittweise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 10. Weiterführende Ressourcen\n",
    "\n",
    "### Dokumentation\n",
    "- **Unitree G1 Docs**: `docs/` Verzeichnis im Repository\n",
    "- **KISS-ICP Paper**: https://github.com/PRBonn/kiss-icp\n",
    "- **YOLO Docs**: https://docs.ultralytics.com/\n",
    "\n",
    "### Kurse & Tutorials\n",
    "- **Modern Robotics** (Northwestern): Comprehensive textbook + videos\n",
    "- **Probabilistic Robotics** (Thrun et al.): SLAM bible\n",
    "- **Deep RL Course** (Hugging Face): Für RL-Vertiefung\n",
    "\n",
    "### Communities\n",
    "- **ROS Discourse**: https://discourse.ros.org/\n",
    "- **Reddit r/robotics**: Active community\n",
    "- **Unitree Forum**: Official support\n",
    "\n",
    "### Tools\n",
    "- **MuJoCo**: Simulation (bereits im Repo)\n",
    "- **Gazebo**: Alternative Simulation\n",
    "- **RViz**: Visualisierung (wenn ROS verwendet)\n",
    "- **Weights & Biases**: ML Experiment Tracking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 11. Abschlusswort\n",
    "\n",
    "**Herzlichen Glückwunsch!** Sie haben die 4-tägige Unitree G1 Academy abgeschlossen.\n",
    "\n",
    "Sie haben gelernt:\n",
    "- Wie man einen humanoiden Roboter steuert\n",
    "- Wie man autonom navigiert\n",
    "- Wie man Objekte erkennt und manipuliert\n",
    "- Wie man alles zu einem System integriert\n",
    "- Wie man es in Produktion bringt\n",
    "\n",
    "**Nächste Schritte:**\n",
    "1. Experimentieren Sie weiter mit dem Code\n",
    "2. Erweitern Sie die Funktionalität\n",
    "3. Teilen Sie Ihre Ergebnisse\n",
    "4. Starten Sie eigene Projekte!\n",
    "\n",
    "**Danke für Ihre Teilnahme!**\n",
    "\n",
    "**Fragen? Kontakt:**\n",
    "- EF Robotics: info@ef-robotics.com\n",
    "- GitHub Issues: (Repository URL)\n",
    "- Slack/Discord: (Community Link)\n",
    "\n",
    "---\n",
    "\n",
    "*Happy Hacking! Viel Erfolg mit Ihren Robotik-Projekten!*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unitree G1 Advanced Use Cases\n",
    "\n",
    "This notebook covers advanced applications and techniques for the Unitree G1 humanoid robot, including:\n",
    "\n",
    "1. **Object Manipulation** - Grasping and holding objects (coffee cup example)\n",
    "2. **Reinforcement Learning** - Training custom behaviors with MimicKit/IsaacGym\n",
    "3. **Dynamic Balance & Compliance** - Adaptive impedance control\n",
    "4. **Vision-Based Manipulation** - RealSense integration for object detection\n",
    "5. **Whole-Body Control** - Coordinated arm-leg motions\n",
    "6. **Human-Robot Interaction** - Gesture recognition and following\n",
    "7. **Teleoperation** - VR/Motion capture control interfaces\n",
    "\n",
    "---\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "This notebook assumes you have completed the basic tutorial (`unitree_g1_sdk_tutorial.ipynb`) and are familiar with:\n",
    "- High-level locomotion and arm control APIs\n",
    "- Low-level motor control with PD gains\n",
    "- Network setup and SDK initialization\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "1. [Setup & Dependencies](#setup)\n",
    "2. [Object Manipulation - Holding a Coffee Cup](#coffee-cup)\n",
    "3. [Reinforcement Learning with MimicKit](#reinforcement-learning)\n",
    "4. [Dynamic Balance & Impedance Control](#dynamic-balance)\n",
    "5. [Vision-Based Grasping](#vision-grasping)\n",
    "6. [Whole-Body Motion Planning](#whole-body)\n",
    "7. [Human-Robot Interaction](#hri)\n",
    "8. [Teleoperation Interfaces](#teleoperation)\n",
    "9. [Advanced Visualization](#visualization)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='setup'></a>\n",
    "## 1. Setup & Dependencies\n",
    "\n",
    "Install additional packages for advanced use cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install advanced dependencies\n",
    "!uv pip install torch torchvision  # For neural networks and RL\n",
    "!uv pip install opencv-python pyrealsense2  # Computer vision\n",
    "!uv pip install scipy scikit-learn  # Math and ML utilities\n",
    "!uv pip install gymnasium mujoco  # RL environments (alternative to IsaacGym)\n",
    "!uv pip install stable-baselines3  # RL algorithms\n",
    "!uv pip install open3d trimesh  # 3D visualization and processing\n",
    "!uv pip install mediapipe  # Hand/pose tracking for HRI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Tuple, Optional\n",
    "\n",
    "# Computer vision\n",
    "import cv2\n",
    "try:\n",
    "    import pyrealsense2 as rs\n",
    "except ImportError:\n",
    "    print(\"Warning: pyrealsense2 not available\")\n",
    "\n",
    "# Math and control\n",
    "from scipy.spatial.transform import Rotation\n",
    "from scipy.interpolate import interp1d\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "# Unitree SDK\n",
    "from unitree_sdk2py.core.channel import ChannelSubscriber, ChannelPublisher, ChannelFactoryInitialize\n",
    "from unitree_sdk2py.idl.unitree_hg.msg.dds_ import LowCmd_, LowState_\n",
    "from unitree_sdk2py.utils.crc import CRC\n",
    "from unitree_sdk2py.utils.thread import RecurrentThread\n",
    "from unitree_sdk2py.g1.loco.g1_loco_client import LocoClient\n",
    "from unitree_sdk2py.g1.arm.g1_arm_action_client import G1ArmActionClient\n",
    "\n",
    "print(\"‚úì All imports successful!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "NETWORK_INTERFACE = \"eth0\"  # Change to your network interface\n",
    "ROBOT_DOF = 29  # 23 or 29 DOF variant\n",
    "\n",
    "# Initialize DDS communication\n",
    "ChannelFactoryInitialize(0, NETWORK_INTERFACE)\n",
    "print(f\"‚úì Initialized communication on {NETWORK_INTERFACE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='coffee-cup'></a>\n",
    "## 2. Object Manipulation - Holding a Coffee Cup\n",
    "\n",
    "This section demonstrates how to grasp and hold a coffee cup using:\n",
    "- Forward/Inverse kinematics for arm positioning\n",
    "- Compliant grasp control with force feedback\n",
    "- Balance compensation during object manipulation\n",
    "\n",
    "### 2.1 Grasp Pose Planning\n",
    "\n",
    "We'll define target positions for the gripper (end-effector) and compute joint angles using inverse kinematics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class G1ArmKinematics:\n",
    "    \"\"\"Simplified kinematics for G1 7-DOF arm (approximation)\"\"\"\n",
    "    \n",
    "    def __init__(self, arm='left'):\n",
    "        self.arm = arm\n",
    "        # Approximate DH parameters (shoulder to wrist)\n",
    "        # Note: Replace with actual robot parameters from URDF\n",
    "        self.link_lengths = {\n",
    "            'shoulder_offset': 0.15,  # Shoulder width from body center\n",
    "            'upper_arm': 0.35,        # Shoulder to elbow\n",
    "            'forearm': 0.30,          # Elbow to wrist\n",
    "            'hand': 0.15              # Wrist to end-effector\n",
    "        }\n",
    "    \n",
    "    def inverse_kinematics_3dof(self, target_pos: np.ndarray) -> Optional[np.ndarray]:\n",
    "        \"\"\"\n",
    "        Simplified IK for shoulder pitch/roll + elbow (3-DOF planar)\n",
    "        \n",
    "        Args:\n",
    "            target_pos: [x, y, z] target position in robot base frame\n",
    "        \n",
    "        Returns:\n",
    "            [shoulder_pitch, shoulder_roll, elbow] joint angles in radians\n",
    "        \"\"\"\n",
    "        x, y, z = target_pos\n",
    "        \n",
    "        # Account for shoulder offset\n",
    "        shoulder_sign = 1 if self.arm == 'left' else -1\n",
    "        y_arm = y - shoulder_sign * self.link_lengths['shoulder_offset']\n",
    "        \n",
    "        # Distance to target in arm plane\n",
    "        r = np.sqrt(x**2 + y_arm**2 + z**2)\n",
    "        \n",
    "        L1 = self.link_lengths['upper_arm']\n",
    "        L2 = self.link_lengths['forearm'] + self.link_lengths['hand']\n",
    "        \n",
    "        # Check reachability\n",
    "        if r > (L1 + L2) or r < abs(L1 - L2):\n",
    "            print(f\"Warning: Target unreachable (distance: {r:.3f}m)\")\n",
    "            return None\n",
    "        \n",
    "        # Elbow angle (using law of cosines)\n",
    "        cos_elbow = (L1**2 + L2**2 - r**2) / (2 * L1 * L2)\n",
    "        cos_elbow = np.clip(cos_elbow, -1, 1)\n",
    "        elbow = np.pi - np.arccos(cos_elbow)  # Elbow down configuration\n",
    "        \n",
    "        # Shoulder pitch (vertical plane)\n",
    "        alpha = np.arctan2(z, np.sqrt(x**2 + y_arm**2))\n",
    "        beta = np.arctan2(L2 * np.sin(elbow), L1 + L2 * np.cos(elbow))\n",
    "        shoulder_pitch = alpha - beta\n",
    "        \n",
    "        # Shoulder roll (horizontal plane)\n",
    "        shoulder_roll = np.arctan2(y_arm, x)\n",
    "        \n",
    "        return np.array([shoulder_pitch, shoulder_roll, elbow])\n",
    "    \n",
    "    def forward_kinematics(self, joint_angles: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Compute end-effector position from joint angles\n",
    "        \n",
    "        Args:\n",
    "            joint_angles: [shoulder_pitch, shoulder_roll, shoulder_yaw, elbow, ...]\n",
    "        \n",
    "        Returns:\n",
    "            [x, y, z] end-effector position\n",
    "        \"\"\"\n",
    "        sp, sr = joint_angles[0], joint_angles[1]\n",
    "        elbow = joint_angles[3] if len(joint_angles) > 3 else 0\n",
    "        \n",
    "        L1 = self.link_lengths['upper_arm']\n",
    "        L2 = self.link_lengths['forearm'] + self.link_lengths['hand']\n",
    "        \n",
    "        # Simplified FK (2D plane approximation)\n",
    "        shoulder_sign = 1 if self.arm == 'left' else -1\n",
    "        \n",
    "        x = L1 * np.cos(sp) + L2 * np.cos(sp + elbow)\n",
    "        y = shoulder_sign * self.link_lengths['shoulder_offset'] + (L1 + L2) * np.sin(sr)\n",
    "        z = L1 * np.sin(sp) + L2 * np.sin(sp + elbow)\n",
    "        \n",
    "        return np.array([x, y, z])\n",
    "\n",
    "# Test kinematics\n",
    "left_arm_kin = G1ArmKinematics(arm='left')\n",
    "\n",
    "# Coffee cup position: 40cm forward, 20cm left, at chest height (80cm)\n",
    "cup_position = np.array([0.4, 0.2, 0.8])\n",
    "joint_solution = left_arm_kin.inverse_kinematics_3dof(cup_position)\n",
    "\n",
    "if joint_solution is not None:\n",
    "    print(f\"Target position: {cup_position}\")\n",
    "    print(f\"Joint solution (deg): {np.degrees(joint_solution)}\")\n",
    "    \n",
    "    # Verify with forward kinematics\n",
    "    fk_pos = left_arm_kin.forward_kinematics(np.concatenate([joint_solution, [0, 0, 0, 0]]))\n",
    "    print(f\"FK verification: {fk_pos}\")\n",
    "    print(f\"Error: {np.linalg.norm(fk_pos - cup_position):.4f}m\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Grasp Execution with Compliant Control\n",
    "\n",
    "Execute the grasp using low-level control with adaptive impedance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CoffeeGraspController:\n",
    "    \"\"\"Controller for grasping and holding a coffee cup\"\"\"\n",
    "    \n",
    "    def __init__(self, network_interface: str):\n",
    "        self.network_interface = network_interface\n",
    "        \n",
    "        # Create clients\n",
    "        self.sport_client = LocoClient()\n",
    "        self.sport_client.SetTimeout(10.0)\n",
    "        self.sport_client.Init()\n",
    "        \n",
    "        # Low-level control setup\n",
    "        self.lowcmd_publisher = ChannelPublisher(\"rt/lowcmd\", LowCmd_)\n",
    "        self.lowcmd_publisher.Init()\n",
    "        \n",
    "        self.lowstate_subscriber = ChannelSubscriber(\"rt/lowstate\", LowState_)\n",
    "        self.lowstate_subscriber.Init(self._state_callback, 10)\n",
    "        \n",
    "        self.current_state = None\n",
    "        self.cmd = LowCmd_()\n",
    "        \n",
    "        # Grasp parameters\n",
    "        self.grasp_phases = {\n",
    "            'approach': {'duration': 2.0, 'kp': 40, 'kd': 2.0},\n",
    "            'grasp': {'duration': 1.0, 'kp': 30, 'kd': 1.5},\n",
    "            'hold': {'duration': 5.0, 'kp': 25, 'kd': 1.0},\n",
    "            'release': {'duration': 1.5, 'kp': 40, 'kd': 2.0}\n",
    "        }\n",
    "    \n",
    "    def _state_callback(self, msg: LowState_):\n",
    "        \"\"\"Store current robot state\"\"\"\n",
    "        self.current_state = msg\n",
    "    \n",
    "    def grasp_cup(self, cup_position: np.ndarray, hand='left'):\n",
    "        \"\"\"\n",
    "        Execute coffee cup grasp sequence\n",
    "        \n",
    "        Args:\n",
    "            cup_position: [x, y, z] cup location in robot frame\n",
    "            hand: 'left' or 'right'\n",
    "        \"\"\"\n",
    "        print(\"ü§ñ Starting coffee cup grasp sequence...\")\n",
    "        \n",
    "        # Phase 1: Ensure stable standing posture\n",
    "        print(\"  [1/5] Stabilizing base...\")\n",
    "        self.sport_client.StandUp()\n",
    "        time.sleep(2)\n",
    "        \n",
    "        # Phase 2: Pre-grasp pose (arm extended toward cup)\n",
    "        print(\"  [2/5] Moving to pre-grasp position...\")\n",
    "        pregrasp_pos = cup_position + np.array([-0.1, 0, 0])  # 10cm before cup\n",
    "        self._move_arm_to_position(pregrasp_pos, hand, phase='approach')\n",
    "        \n",
    "        # Phase 3: Approach cup with compliant motion\n",
    "        print(\"  [3/5] Approaching cup...\")\n",
    "        self._move_arm_to_position(cup_position, hand, phase='grasp')\n",
    "        \n",
    "        # Phase 4: Close gripper (for dexterous hand) or maintain position\n",
    "        print(\"  [4/5] Grasping cup...\")\n",
    "        # Note: G1 may not have gripper - this is placeholder for future hardware\n",
    "        # For now, maintain wrist position with reduced stiffness\n",
    "        time.sleep(1)\n",
    "        \n",
    "        # Phase 5: Hold with compliant control\n",
    "        print(\"  [5/5] Holding cup (compliant mode)...\")\n",
    "        self._hold_position(cup_position, hand, duration=5.0)\n",
    "        \n",
    "        print(\"‚úì Grasp sequence complete!\")\n",
    "    \n",
    "    def _move_arm_to_position(self, target_pos: np.ndarray, hand: str, phase: str):\n",
    "        \"\"\"\n",
    "        Move arm to target position using low-level control\n",
    "        \n",
    "        Args:\n",
    "            target_pos: Target [x, y, z] position\n",
    "            hand: 'left' or 'right'\n",
    "            phase: Control phase name (for parameter lookup)\n",
    "        \"\"\"\n",
    "        # Compute IK\n",
    "        kin = G1ArmKinematics(arm=hand)\n",
    "        joint_angles = kin.inverse_kinematics_3dof(target_pos)\n",
    "        \n",
    "        if joint_angles is None:\n",
    "            print(f\"Warning: Target {target_pos} unreachable\")\n",
    "            return\n",
    "        \n",
    "        # Get phase parameters\n",
    "        params = self.grasp_phases[phase]\n",
    "        duration = params['duration']\n",
    "        kp, kd = params['kp'], params['kd']\n",
    "        \n",
    "        # Map to motor indices\n",
    "        if hand == 'left':\n",
    "            joint_indices = [15, 16, 18]  # LeftShoulderPitch, Roll, Elbow\n",
    "        else:\n",
    "            joint_indices = [22, 23, 25]  # RightShoulderPitch, Roll, Elbow\n",
    "        \n",
    "        # Execute motion with interpolation\n",
    "        start_time = time.time()\n",
    "        rate = 100  # Hz\n",
    "        \n",
    "        while time.time() - start_time < duration:\n",
    "            # Interpolation parameter (0 to 1)\n",
    "            alpha = (time.time() - start_time) / duration\n",
    "            \n",
    "            # Get current positions\n",
    "            if self.current_state is not None:\n",
    "                current_q = [self.current_state.motor_state[i].q for i in joint_indices]\n",
    "            else:\n",
    "                current_q = [0, 0, 0]\n",
    "            \n",
    "            # Smooth interpolation (cubic ease-in-out)\n",
    "            alpha_smooth = 3*alpha**2 - 2*alpha**3\n",
    "            \n",
    "            # Set target positions\n",
    "            for i, idx in enumerate(joint_indices):\n",
    "                target_angle = current_q[i] * (1 - alpha_smooth) + joint_angles[i] * alpha_smooth\n",
    "                self.cmd.motor_cmd[idx].q = target_angle\n",
    "                self.cmd.motor_cmd[idx].dq = 0\n",
    "                self.cmd.motor_cmd[idx].kp = kp\n",
    "                self.cmd.motor_cmd[idx].kd = kd\n",
    "                self.cmd.motor_cmd[idx].tau = 0\n",
    "            \n",
    "            # Publish command\n",
    "            self.cmd.crc = CRC.Crc(self.cmd)\n",
    "            self.lowcmd_publisher.Write(self.cmd)\n",
    "            \n",
    "            time.sleep(1.0 / rate)\n",
    "    \n",
    "    def _hold_position(self, position: np.ndarray, hand: str, duration: float):\n",
    "        \"\"\"\n",
    "        Hold current position with compliant control (low stiffness)\n",
    "        \n",
    "        Args:\n",
    "            position: Target position to maintain\n",
    "            hand: 'left' or 'right'\n",
    "            duration: How long to hold (seconds)\n",
    "        \"\"\"\n",
    "        params = self.grasp_phases['hold']\n",
    "        self._move_arm_to_position(position, hand, 'hold')\n",
    "        time.sleep(duration)\n",
    "\n",
    "# Example usage (CAUTION: This will move the robot!)\n",
    "print(\"Coffee grasp controller initialized\")\n",
    "print(\"To execute: controller.grasp_cup(np.array([0.4, 0.2, 0.8]), hand='left')\")\n",
    "\n",
    "# Uncomment to run:\n",
    "# controller = CoffeeGraspController(NETWORK_INTERFACE)\n",
    "# controller.grasp_cup(np.array([0.4, 0.2, 0.8]), hand='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Grasp Stability Analysis\n",
    "\n",
    "Monitor contact forces and adjust grasp based on feedback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraspMonitor:\n",
    "    \"\"\"Monitor and analyze grasp stability\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.force_history = []\n",
    "        self.torque_history = []\n",
    "        self.time_history = []\n",
    "    \n",
    "    def analyze_grasp_stability(self, motor_states: List, target_joints: List[int]):\n",
    "        \"\"\"\n",
    "        Analyze grasp stability from motor torque readings\n",
    "        \n",
    "        Args:\n",
    "            motor_states: Current motor state messages\n",
    "            target_joints: Joint indices involved in grasp\n",
    "        \n",
    "        Returns:\n",
    "            stability_score: 0-1 score (1 = very stable)\n",
    "        \"\"\"\n",
    "        torques = [abs(motor_states[i].tau_est) for i in target_joints]\n",
    "        \n",
    "        # Check if torques are within safe range\n",
    "        torque_variance = np.var(torques)\n",
    "        mean_torque = np.mean(torques)\n",
    "        \n",
    "        # Stability heuristic:\n",
    "        # - Low variance = stable\n",
    "        # - Moderate mean torque = good contact\n",
    "        # - Too high torque = excessive force (slip risk)\n",
    "        \n",
    "        variance_score = np.exp(-torque_variance * 10)  # Prefer low variance\n",
    "        torque_score = np.exp(-abs(mean_torque - 5.0) / 3.0)  # Prefer ~5 Nm\n",
    "        \n",
    "        stability = 0.6 * variance_score + 0.4 * torque_score\n",
    "        \n",
    "        return stability\n",
    "    \n",
    "    def visualize_grasp_forces(self):\n",
    "        \"\"\"Plot grasp force history\"\"\"\n",
    "        if len(self.force_history) == 0:\n",
    "            print(\"No force data collected yet\")\n",
    "            return\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 1, figsize=(12, 8))\n",
    "        \n",
    "        # Plot forces\n",
    "        axes[0].plot(self.time_history, self.force_history, 'b-', linewidth=2)\n",
    "        axes[0].set_ylabel('Grasp Force (N)', fontsize=11)\n",
    "        axes[0].set_title('Grasp Force During Hold Phase', fontsize=13)\n",
    "        axes[0].grid(True, alpha=0.3)\n",
    "        axes[0].axhline(y=10, color='g', linestyle='--', label='Target Force')\n",
    "        axes[0].legend()\n",
    "        \n",
    "        # Plot torques\n",
    "        axes[1].plot(self.time_history, self.torque_history, 'r-', linewidth=2)\n",
    "        axes[1].set_xlabel('Time (s)', fontsize=11)\n",
    "        axes[1].set_ylabel('Joint Torque (Nm)', fontsize=11)\n",
    "        axes[1].set_title('Wrist Torque', fontsize=13)\n",
    "        axes[1].grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Simulate grasp force data\n",
    "monitor = GraspMonitor()\n",
    "t = np.linspace(0, 5, 100)\n",
    "monitor.time_history = t.tolist()\n",
    "monitor.force_history = (10 + 2*np.sin(2*np.pi*0.5*t) + np.random.normal(0, 0.5, len(t))).tolist()\n",
    "monitor.torque_history = (5 + 1*np.cos(2*np.pi*0.3*t) + np.random.normal(0, 0.3, len(t))).tolist()\n",
    "\n",
    "monitor.visualize_grasp_forces()\n",
    "print(\"üìä Grasp force visualization (simulated data)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='reinforcement-learning'></a>\n",
    "## 3. Reinforcement Learning with MimicKit\n",
    "\n",
    "Train custom behaviors using imitation learning and reinforcement learning.\n",
    "\n",
    "### 3.1 Introduction to MimicKit\n",
    "\n",
    "**MimicKit** is a motion imitation framework for training physics-based controllers. It includes:\n",
    "\n",
    "- **DeepMimic**: Imitation learning from motion capture data\n",
    "- **AMP (Adversarial Motion Priors)**: Learning stylized locomotion\n",
    "- **ASE (Adversarial Skill Embeddings)**: Reusable skill learning\n",
    "- **ADD**: Physics-based imitation with differential discriminators\n",
    "\n",
    "**Key Features:**\n",
    "- Lightweight framework with minimal dependencies\n",
    "- Integration with NVIDIA IsaacGym for fast parallel simulation\n",
    "- Supports humanoid and quadruped robots\n",
    "- Pre-trained models available\n",
    "\n",
    "**GitHub**: https://github.com/xbpeng/MimicKit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Setting Up MimicKit for G1\n",
    "\n",
    "#### Installation Steps\n",
    "\n",
    "```bash\n",
    "# 1. Install IsaacGym (download from NVIDIA)\n",
    "# https://developer.nvidia.com/isaac-gym\n",
    "cd ~/Downloads\n",
    "tar -xf IsaacGym_Preview_*.tar.gz\n",
    "cd isaacgym/python\n",
    "pip install -e .\n",
    "\n",
    "# 2. Clone and install MimicKit\n",
    "git clone https://github.com/xbpeng/MimicKit.git\n",
    "cd MimicKit\n",
    "pip install -r requirements.txt\n",
    "\n",
    "# 3. Download assets (motion data, models)\n",
    "# Follow instructions in MimicKit README\n",
    "```\n",
    "\n",
    "#### Creating G1 URDF for IsaacGym\n",
    "\n",
    "To simulate G1 in IsaacGym, you need a URDF model. If not provided by Unitree:\n",
    "1. Use CAD files to generate URDF with SolidWorks/Fusion360 plugins\n",
    "2. Or create simplified URDF manually based on specifications\n",
    "3. Add collision geometries and inertial properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if IsaacGym is available\n",
    "try:\n",
    "    from isaacgym import gymapi\n",
    "    print(\"‚úì IsaacGym available\")\n",
    "    ISAACGYM_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è  IsaacGym not installed - using Gymnasium/MuJoCo instead\")\n",
    "    ISAACGYM_AVAILABLE = False\n",
    "    \n",
    "    # Fallback to Gymnasium\n",
    "    import gymnasium as gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Training a Custom Skill with DeepMimic\n",
    "\n",
    "Example: Training G1 to pick up and hand over an object using motion imitation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder for MimicKit integration\n",
    "# This requires IsaacGym and motion capture data\n",
    "\n",
    "class G1DeepMimicTrainer:\n",
    "    \"\"\"\n",
    "    Wrapper for training G1 behaviors with DeepMimic algorithm\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, motion_file: str, urdf_path: str):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            motion_file: Path to reference motion data (.npy or .txt)\n",
    "            urdf_path: Path to G1 URDF file\n",
    "        \"\"\"\n",
    "        self.motion_file = motion_file\n",
    "        self.urdf_path = urdf_path\n",
    "        \n",
    "        # Training hyperparameters (from MimicKit defaults)\n",
    "        self.config = {\n",
    "            'learning_rate': 3e-4,\n",
    "            'batch_size': 4096,\n",
    "            'num_envs': 4096,  # Parallel simulations\n",
    "            'horizon': 32,\n",
    "            'gamma': 0.95,\n",
    "            'lam': 0.95,\n",
    "            'motion_weight': 0.5,  # Weight for motion imitation reward\n",
    "            'task_weight': 0.5,    # Weight for task-specific reward\n",
    "        }\n",
    "    \n",
    "    def load_reference_motion(self):\n",
    "        \"\"\"\n",
    "        Load reference motion data from mocap or keyframes\n",
    "        \n",
    "        Motion format: [time, joint_positions, joint_velocities, root_pose]\n",
    "        \"\"\"\n",
    "        # Placeholder - actual implementation depends on data format\n",
    "        print(f\"Loading motion from {self.motion_file}\")\n",
    "        # motion_data = np.load(self.motion_file)\n",
    "        # return motion_data\n",
    "        pass\n",
    "    \n",
    "    def compute_imitation_reward(self, current_pose, reference_pose):\n",
    "        \"\"\"\n",
    "        Compute reward based on similarity to reference motion\n",
    "        \n",
    "        Args:\n",
    "            current_pose: Current robot joint configuration\n",
    "            reference_pose: Target pose from reference motion\n",
    "        \n",
    "        Returns:\n",
    "            reward: Scalar reward value\n",
    "        \"\"\"\n",
    "        # Joint position error\n",
    "        pos_error = np.linalg.norm(current_pose - reference_pose)\n",
    "        \n",
    "        # Exponential reward (from DeepMimic paper)\n",
    "        reward = np.exp(-2.0 * pos_error)\n",
    "        \n",
    "        return reward\n",
    "    \n",
    "    def train(self, num_iterations: int = 10000):\n",
    "        \"\"\"\n",
    "        Train policy using PPO + motion imitation\n",
    "        \n",
    "        Args:\n",
    "            num_iterations: Number of training iterations\n",
    "        \"\"\"\n",
    "        print(\"üéì Starting DeepMimic training...\")\n",
    "        print(f\"   Config: {self.config}\")\n",
    "        print(f\"   Iterations: {num_iterations}\")\n",
    "        \n",
    "        # Actual training loop would go here\n",
    "        # See MimicKit repository for full implementation\n",
    "        \n",
    "        print(\"‚ö†Ô∏è  Full training requires MimicKit + IsaacGym installation\")\n",
    "        print(\"   See: https://github.com/xbpeng/MimicKit\")\n",
    "\n",
    "# Example setup\n",
    "print(\"DeepMimic trainer configured\")\n",
    "print(\"\"\"\\nTo train a custom behavior:\n",
    "1. Record reference motion (mocap or manual keyframes)\n",
    "2. Convert to MimicKit format (.npy or .txt)\n",
    "3. Create G1 URDF with proper joint/link definitions\n",
    "4. Run: trainer = G1DeepMimicTrainer('motion.npy', 'g1.urdf')\n",
    "5. Execute: trainer.train(num_iterations=10000)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Alternative: Training with Stable-Baselines3 + Gymnasium\n",
    "\n",
    "If IsaacGym is not available, use Gymnasium with a custom G1 environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simplified RL training with Stable-Baselines3\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import torch\n",
    "\n",
    "try:\n",
    "    from stable_baselines3 import PPO\n",
    "    from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "    SB3_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"Install stable-baselines3: pip install stable-baselines3\")\n",
    "    SB3_AVAILABLE = False\n",
    "\n",
    "class G1ReachingEnv(gym.Env):\n",
    "    \"\"\"\n",
    "    Simple reaching task environment for G1 arm\n",
    "    \n",
    "    Goal: Train arm to reach random target positions\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Action space: 7-DOF arm joint velocities\n",
    "        self.action_space = spaces.Box(\n",
    "            low=-1.0, high=1.0, shape=(7,), dtype=np.float32\n",
    "        )\n",
    "        \n",
    "        # Observation space: joint positions (7) + target position (3) + distance (1)\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=-np.inf, high=np.inf, shape=(11,), dtype=np.float32\n",
    "        )\n",
    "        \n",
    "        self.kinematics = G1ArmKinematics('left')\n",
    "        self.reset()\n",
    "    \n",
    "    def reset(self, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "        \n",
    "        # Reset arm to neutral position\n",
    "        self.joint_positions = np.zeros(7)\n",
    "        \n",
    "        # Random target in reachable workspace\n",
    "        self.target_pos = np.array([\n",
    "            np.random.uniform(0.2, 0.6),  # x: forward\n",
    "            np.random.uniform(0.0, 0.4),  # y: left\n",
    "            np.random.uniform(0.6, 1.2)   # z: height\n",
    "        ])\n",
    "        \n",
    "        self.steps = 0\n",
    "        \n",
    "        return self._get_obs(), {}\n",
    "    \n",
    "    def _get_obs(self):\n",
    "        # Current end-effector position\n",
    "        ee_pos = self.kinematics.forward_kinematics(self.joint_positions)\n",
    "        distance = np.linalg.norm(ee_pos - self.target_pos)\n",
    "        \n",
    "        obs = np.concatenate([\n",
    "            self.joint_positions,  # 7\n",
    "            self.target_pos,       # 3\n",
    "            [distance]             # 1\n",
    "        ]).astype(np.float32)\n",
    "        \n",
    "        return obs\n",
    "    \n",
    "    def step(self, action):\n",
    "        # Apply action (joint velocity commands)\n",
    "        self.joint_positions += action * 0.1  # Scale velocities\n",
    "        \n",
    "        # Clip to joint limits (simplified)\n",
    "        self.joint_positions = np.clip(self.joint_positions, -np.pi, np.pi)\n",
    "        \n",
    "        # Compute reward\n",
    "        ee_pos = self.kinematics.forward_kinematics(self.joint_positions)\n",
    "        distance = np.linalg.norm(ee_pos - self.target_pos)\n",
    "        \n",
    "        # Reward: negative distance + bonus for reaching\n",
    "        reward = -distance\n",
    "        if distance < 0.05:  # Within 5cm\n",
    "            reward += 10.0\n",
    "            terminated = True\n",
    "        else:\n",
    "            terminated = False\n",
    "        \n",
    "        self.steps += 1\n",
    "        truncated = self.steps >= 100  # Max episode length\n",
    "        \n",
    "        return self._get_obs(), reward, terminated, truncated, {}\n",
    "    \n",
    "    def render(self):\n",
    "        pass\n",
    "\n",
    "if SB3_AVAILABLE:\n",
    "    # Create and train\n",
    "    env = G1ReachingEnv()\n",
    "    \n",
    "    # Train PPO agent\n",
    "    print(\"üéì Training PPO agent for reaching task...\")\n",
    "    model = PPO(\n",
    "        \"MlpPolicy\",\n",
    "        env,\n",
    "        verbose=1,\n",
    "        learning_rate=3e-4,\n",
    "        n_steps=2048,\n",
    "        batch_size=64,\n",
    "        device='cpu'  # Change to 'cuda' if GPU available\n",
    "    )\n",
    "    \n",
    "    # Train for a few steps (increase for real training)\n",
    "    print(\"Training for 10,000 timesteps (increase for better performance)...\")\n",
    "    model.learn(total_timesteps=10000)\n",
    "    \n",
    "    # Save model\n",
    "    model.save(\"g1_reaching_ppo\")\n",
    "    print(\"‚úì Model saved to g1_reaching_ppo.zip\")\n",
    "    \n",
    "    # Test trained policy\n",
    "    print(\"\\nTesting trained policy...\")\n",
    "    obs, _ = env.reset()\n",
    "    for i in range(100):\n",
    "        action, _ = model.predict(obs, deterministic=True)\n",
    "        obs, reward, terminated, truncated, _ = env.step(action)\n",
    "        if terminated or truncated:\n",
    "            print(f\"‚úì Reached target in {i+1} steps!\")\n",
    "            break\n",
    "else:\n",
    "    print(\"Install stable-baselines3 to run RL training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Deploying Trained Policy to Real Robot\n",
    "\n",
    "Transfer learned behavior from simulation to hardware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyDeployment:\n",
    "    \"\"\"\n",
    "    Deploy trained RL policy to real G1 robot\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_path: str, network_interface: str):\n",
    "        # Load trained model\n",
    "        if SB3_AVAILABLE:\n",
    "            self.model = PPO.load(model_path)\n",
    "            print(f\"‚úì Loaded model from {model_path}\")\n",
    "        \n",
    "        # Initialize robot interface\n",
    "        self.network_interface = network_interface\n",
    "        self.lowstate_subscriber = ChannelSubscriber(\"rt/lowstate\", LowState_)\n",
    "        self.lowstate_subscriber.Init(self._state_callback, 10)\n",
    "        \n",
    "        self.lowcmd_publisher = ChannelPublisher(\"rt/lowcmd\", LowCmd_)\n",
    "        self.lowcmd_publisher.Init()\n",
    "        \n",
    "        self.current_state = None\n",
    "    \n",
    "    def _state_callback(self, msg: LowState_):\n",
    "        self.current_state = msg\n",
    "    \n",
    "    def get_observation(self):\n",
    "        \"\"\"\n",
    "        Extract observation from robot state (must match training format)\n",
    "        \"\"\"\n",
    "        if self.current_state is None:\n",
    "            return None\n",
    "        \n",
    "        # Extract joint positions for left arm (indices 15-21)\n",
    "        joint_positions = np.array([\n",
    "            self.current_state.motor_state[i].q \n",
    "            for i in range(15, 22)\n",
    "        ])\n",
    "        \n",
    "        # Add task-specific observations (e.g., target position)\n",
    "        # This must match the observation space used during training\n",
    "        target_pos = np.array([0.4, 0.2, 0.8])  # Example target\n",
    "        \n",
    "        # Compute distance\n",
    "        kin = G1ArmKinematics('left')\n",
    "        ee_pos = kin.forward_kinematics(joint_positions)\n",
    "        distance = np.linalg.norm(ee_pos - target_pos)\n",
    "        \n",
    "        obs = np.concatenate([joint_positions, target_pos, [distance]])\n",
    "        return obs.astype(np.float32)\n",
    "    \n",
    "    def execute_policy(self, duration: float = 10.0):\n",
    "        \"\"\"\n",
    "        Run trained policy on real robot\n",
    "        \n",
    "        Args:\n",
    "            duration: How long to run (seconds)\n",
    "        \"\"\"\n",
    "        print(\"ü§ñ Executing trained policy on robot...\")\n",
    "        print(\"‚ö†Ô∏è  WARNING: Ensure robot is in safe area!\")\n",
    "        \n",
    "        start_time = time.time()\n",
    "        rate = 50  # Hz (control frequency)\n",
    "        \n",
    "        while time.time() - start_time < duration:\n",
    "            # Get observation\n",
    "            obs = self.get_observation()\n",
    "            if obs is None:\n",
    "                time.sleep(0.01)\n",
    "                continue\n",
    "            \n",
    "            # Get action from policy\n",
    "            if SB3_AVAILABLE:\n",
    "                action, _ = self.model.predict(obs, deterministic=True)\n",
    "            else:\n",
    "                action = np.zeros(7)  # Placeholder\n",
    "            \n",
    "            # Convert action to motor commands\n",
    "            cmd = LowCmd_()\n",
    "            for i, motor_idx in enumerate(range(15, 22)):\n",
    "                # Action is joint velocity - integrate to position\n",
    "                current_q = self.current_state.motor_state[motor_idx].q\n",
    "                target_q = current_q + action[i] * 0.1  # Scale action\n",
    "                \n",
    "                cmd.motor_cmd[motor_idx].q = target_q\n",
    "                cmd.motor_cmd[motor_idx].dq = 0\n",
    "                cmd.motor_cmd[motor_idx].kp = 40\n",
    "                cmd.motor_cmd[motor_idx].kd = 2\n",
    "                cmd.motor_cmd[motor_idx].tau = 0\n",
    "            \n",
    "            # Send command\n",
    "            cmd.crc = CRC.Crc(cmd)\n",
    "            self.lowcmd_publisher.Write(cmd)\n",
    "            \n",
    "            time.sleep(1.0 / rate)\n",
    "        \n",
    "        print(\"‚úì Policy execution complete\")\n",
    "\n",
    "print(\"Policy deployment ready\")\n",
    "print(\"To deploy: deployer = PolicyDeployment('g1_reaching_ppo.zip', NETWORK_INTERFACE)\")\n",
    "print(\"           deployer.execute_policy(duration=10.0)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='dynamic-balance'></a>\n",
    "## 4. Dynamic Balance & Impedance Control\n",
    "\n",
    "Implement adaptive compliance for robust manipulation during locomotion.\n",
    "\n",
    "### 4.1 Center of Mass (CoM) Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BalanceController:\n",
    "    \"\"\"\n",
    "    Monitor and maintain balance during manipulation tasks\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Robot parameters (approximate for G1)\n",
    "        self.total_mass = 47.0  # kg (approximate)\n",
    "        self.foot_separation = 0.20  # m\n",
    "        \n",
    "        # Safety thresholds\n",
    "        self.max_com_offset = 0.08  # meters from support polygon center\n",
    "        self.max_tilt_angle = 0.15  # radians (~8.5 degrees)\n",
    "    \n",
    "    def estimate_com_from_imu(self, imu_data: dict, joint_states: dict) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Estimate center of mass position from IMU and joint angles\n",
    "        \n",
    "        Args:\n",
    "            imu_data: {'rpy': [roll, pitch, yaw], 'accel': [ax, ay, az]}\n",
    "            joint_states: Joint positions for all motors\n",
    "        \n",
    "        Returns:\n",
    "            com_position: [x, y, z] in robot base frame\n",
    "        \"\"\"\n",
    "        # Simplified CoM estimation (full version requires link masses)\n",
    "        # This is a heuristic based on body tilt\n",
    "        \n",
    "        roll, pitch, yaw = imu_data['rpy']\n",
    "        \n",
    "        # Approximate CoM shift due to tilt\n",
    "        # Assumes CoM at hip height (~0.8m)\n",
    "        com_height = 0.8\n",
    "        \n",
    "        com_x = com_height * np.tan(pitch)\n",
    "        com_y = com_height * np.tan(roll)\n",
    "        com_z = com_height\n",
    "        \n",
    "        return np.array([com_x, com_y, com_z])\n",
    "    \n",
    "    def check_stability(self, com_pos: np.ndarray, support_polygon: np.ndarray) -> bool:\n",
    "        \"\"\"\n",
    "        Check if CoM is within support polygon\n",
    "        \n",
    "        Args:\n",
    "            com_pos: Center of mass [x, y, z]\n",
    "            support_polygon: Vertices of support polygon (e.g., foot positions)\n",
    "        \n",
    "        Returns:\n",
    "            is_stable: True if CoM is within polygon\n",
    "        \"\"\"\n",
    "        # Project CoM to ground plane\n",
    "        com_2d = com_pos[:2]\n",
    "        \n",
    "        # For standing with two feet, support polygon is rectangle between feet\n",
    "        # Simplified: check if CoM is near center (0, 0)\n",
    "        distance_from_center = np.linalg.norm(com_2d)\n",
    "        \n",
    "        is_stable = distance_from_center < self.max_com_offset\n",
    "        \n",
    "        return is_stable\n",
    "    \n",
    "    def compute_compensatory_motion(self, com_pos: np.ndarray, target_com: np.ndarray) -> dict:\n",
    "        \"\"\"\n",
    "        Compute waist/hip adjustments to shift CoM toward target\n",
    "        \n",
    "        Args:\n",
    "            com_pos: Current CoM position\n",
    "            target_com: Desired CoM position\n",
    "        \n",
    "        Returns:\n",
    "            joint_adjustments: Dictionary of joint angle corrections\n",
    "        \"\"\"\n",
    "        error = target_com - com_pos\n",
    "        \n",
    "        # Simple proportional control\n",
    "        # Shift waist to compensate for CoM error\n",
    "        waist_roll_correction = -error[1] * 0.5  # Lateral shift\n",
    "        waist_pitch_correction = -error[0] * 0.5  # Forward/back shift\n",
    "        \n",
    "        adjustments = {\n",
    "            'WaistRoll': waist_roll_correction,\n",
    "            'WaistPitch': waist_pitch_correction,\n",
    "        }\n",
    "        \n",
    "        return adjustments\n",
    "\n",
    "# Test balance controller\n",
    "balance_ctrl = BalanceController()\n",
    "\n",
    "# Simulate IMU data with slight tilt\n",
    "imu_sim = {\n",
    "    'rpy': [0.05, 0.03, 0.0],  # 5deg roll, 3deg pitch\n",
    "    'accel': [0, 0, 9.81]\n",
    "}\n",
    "\n",
    "com_est = balance_ctrl.estimate_com_from_imu(imu_sim, {})\n",
    "print(f\"Estimated CoM position: {com_est}\")\n",
    "\n",
    "is_stable = balance_ctrl.check_stability(com_est, None)\n",
    "print(f\"Robot stable: {is_stable}\")\n",
    "\n",
    "if not is_stable:\n",
    "    corrections = balance_ctrl.compute_compensatory_motion(com_est, np.array([0, 0, 0.8]))\n",
    "    print(f\"Balance corrections: {corrections}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Adaptive Impedance Control\n",
    "\n",
    "Adjust arm stiffness based on task requirements and contact forces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaptiveImpedanceController:\n",
    "    \"\"\"\n",
    "    Dynamically adjust PD gains based on contact forces and task phase\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Define impedance profiles for different tasks\n",
    "        self.impedance_profiles = {\n",
    "            'free_motion': {'kp': 60, 'kd': 3.0},     # Stiff for precise positioning\n",
    "            'contact': {'kp': 20, 'kd': 1.0},          # Compliant for safe interaction\n",
    "            'grasp': {'kp': 30, 'kd': 1.5},            # Medium for holding\n",
    "            'dynamic': {'kp': 40, 'kd': 2.5},          # Moderate for moving with object\n",
    "        }\n",
    "        \n",
    "        self.current_profile = 'free_motion'\n",
    "    \n",
    "    def detect_contact(self, torque_readings: np.ndarray, threshold: float = 3.0) -> bool:\n",
    "        \"\"\"\n",
    "        Detect contact based on unexpected torques\n",
    "        \n",
    "        Args:\n",
    "            torque_readings: Current motor torques\n",
    "            threshold: Contact detection threshold (Nm)\n",
    "        \n",
    "        Returns:\n",
    "            in_contact: True if contact detected\n",
    "        \"\"\"\n",
    "        # Check if any torque exceeds threshold\n",
    "        max_torque = np.max(np.abs(torque_readings))\n",
    "        return max_torque > threshold\n",
    "    \n",
    "    def update_impedance(self, task_phase: str, force_feedback: Optional[np.ndarray] = None) -> dict:\n",
    "        \"\"\"\n",
    "        Update impedance parameters based on task phase and forces\n",
    "        \n",
    "        Args:\n",
    "            task_phase: Current task phase ('free_motion', 'contact', etc.)\n",
    "            force_feedback: Measured contact forces (optional)\n",
    "        \n",
    "        Returns:\n",
    "            gains: {'kp': ..., 'kd': ...}\n",
    "        \"\"\"\n",
    "        if task_phase in self.impedance_profiles:\n",
    "            base_gains = self.impedance_profiles[task_phase]\n",
    "        else:\n",
    "            base_gains = self.impedance_profiles['free_motion']\n",
    "        \n",
    "        gains = base_gains.copy()\n",
    "        \n",
    "        # Adapt based on force feedback\n",
    "        if force_feedback is not None:\n",
    "            force_magnitude = np.linalg.norm(force_feedback)\n",
    "            \n",
    "            # Reduce stiffness if large forces detected (safety)\n",
    "            if force_magnitude > 10.0:  # N\n",
    "                gains['kp'] *= 0.7\n",
    "                gains['kd'] *= 0.8\n",
    "        \n",
    "        self.current_profile = task_phase\n",
    "        return gains\n",
    "    \n",
    "    def visualize_impedance_trajectory(self):\n",
    "        \"\"\"\n",
    "        Show how impedance changes during a manipulation task\n",
    "        \"\"\"\n",
    "        # Simulate task sequence\n",
    "        phases = ['free_motion', 'free_motion', 'contact', 'grasp', 'dynamic', 'dynamic', 'grasp', 'contact', 'free_motion']\n",
    "        time_per_phase = 1.0  # seconds\n",
    "        \n",
    "        times = []\n",
    "        kp_values = []\n",
    "        kd_values = []\n",
    "        phase_labels = []\n",
    "        \n",
    "        t = 0\n",
    "        for phase in phases:\n",
    "            gains = self.update_impedance(phase)\n",
    "            times.append(t)\n",
    "            kp_values.append(gains['kp'])\n",
    "            kd_values.append(gains['kd'])\n",
    "            phase_labels.append(phase)\n",
    "            t += time_per_phase\n",
    "        \n",
    "        fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 8))\n",
    "        \n",
    "        # Plot Kp\n",
    "        ax1.step(times, kp_values, 'b-', linewidth=2, where='post', label='Kp')\n",
    "        ax1.set_ylabel('Position Gain (Kp)', fontsize=11)\n",
    "        ax1.set_title('Adaptive Impedance During Manipulation Task', fontsize=13)\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        ax1.legend()\n",
    "        \n",
    "        # Plot Kd\n",
    "        ax2.step(times, kd_values, 'r-', linewidth=2, where='post', label='Kd')\n",
    "        ax2.set_xlabel('Time (s)', fontsize=11)\n",
    "        ax2.set_ylabel('Damping Gain (Kd)', fontsize=11)\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        ax2.legend()\n",
    "        \n",
    "        # Add phase annotations\n",
    "        for i, (t, phase) in enumerate(zip(times, phase_labels)):\n",
    "            if i == 0 or phase != phase_labels[i-1]:\n",
    "                ax1.axvline(x=t, color='gray', linestyle='--', alpha=0.5)\n",
    "                ax2.axvline(x=t, color='gray', linestyle='--', alpha=0.5)\n",
    "                ax1.text(t + 0.1, 55, phase, fontsize=8, rotation=90, alpha=0.7)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Visualize adaptive impedance\n",
    "impedance_ctrl = AdaptiveImpedanceController()\n",
    "impedance_ctrl.visualize_impedance_trajectory()\n",
    "print(\"üìä Adaptive impedance trajectory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='vision-grasping'></a>\n",
    "## 5. Vision-Based Grasping\n",
    "\n",
    "Integrate Intel RealSense depth camera for object detection and grasp planning.\n",
    "\n",
    "### 5.1 RealSense Camera Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RealSenseGraspDetector:\n",
    "    \"\"\"\n",
    "    Detect graspable objects using RealSense depth camera\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        try:\n",
    "            # Configure RealSense pipeline\n",
    "            self.pipeline = rs.pipeline()\n",
    "            config = rs.config()\n",
    "            config.enable_stream(rs.stream.color, 640, 480, rs.format.bgr8, 30)\n",
    "            config.enable_stream(rs.stream.depth, 640, 480, rs.format.z16, 30)\n",
    "            \n",
    "            self.pipeline.start(config)\n",
    "            print(\"‚úì RealSense camera initialized\")\n",
    "            self.camera_available = True\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è  RealSense not available: {e}\")\n",
    "            self.camera_available = False\n",
    "    \n",
    "    def capture_frame(self) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Capture RGB and depth frame\n",
    "        \n",
    "        Returns:\n",
    "            color_image: RGB image (H, W, 3)\n",
    "            depth_image: Depth map (H, W) in millimeters\n",
    "        \"\"\"\n",
    "        if not self.camera_available:\n",
    "            # Return dummy data\n",
    "            return np.zeros((480, 640, 3), dtype=np.uint8), np.zeros((480, 640), dtype=np.uint16)\n",
    "        \n",
    "        frames = self.pipeline.wait_for_frames()\n",
    "        color_frame = frames.get_color_frame()\n",
    "        depth_frame = frames.get_depth_frame()\n",
    "        \n",
    "        color_image = np.asanyarray(color_frame.get_data())\n",
    "        depth_image = np.asanyarray(depth_frame.get_data())\n",
    "        \n",
    "        return color_image, depth_image\n",
    "    \n",
    "    def detect_objects(self, color_image: np.ndarray, depth_image: np.ndarray) -> List[dict]:\n",
    "        \"\"\"\n",
    "        Detect objects in the scene\n",
    "        \n",
    "        Args:\n",
    "            color_image: RGB image\n",
    "            depth_image: Depth map\n",
    "        \n",
    "        Returns:\n",
    "            objects: List of detected objects with [x, y, z, width, height, depth]\n",
    "        \"\"\"\n",
    "        # Simple segmentation: find objects on table using depth\n",
    "        # Filter depth to table height range (e.g., 600-800mm from camera)\n",
    "        table_mask = (depth_image > 600) & (depth_image < 800)\n",
    "        \n",
    "        # Find connected components (objects)\n",
    "        contours, _ = cv2.findContours(\n",
    "            table_mask.astype(np.uint8), \n",
    "            cv2.RETR_EXTERNAL, \n",
    "            cv2.CHAIN_APPROX_SIMPLE\n",
    "        )\n",
    "        \n",
    "        objects = []\n",
    "        for contour in contours:\n",
    "            if cv2.contourArea(contour) < 500:  # Filter small noise\n",
    "                continue\n",
    "            \n",
    "            # Bounding box\n",
    "            x, y, w, h = cv2.boundingRect(contour)\n",
    "            \n",
    "            # Average depth in region\n",
    "            roi_depth = depth_image[y:y+h, x:x+w]\n",
    "            avg_depth = np.median(roi_depth[roi_depth > 0])\n",
    "            \n",
    "            objects.append({\n",
    "                'bbox': (x, y, w, h),\n",
    "                'depth': avg_depth,\n",
    "                'center_pixel': (x + w//2, y + h//2)\n",
    "            })\n",
    "        \n",
    "        return objects\n",
    "    \n",
    "    def pixel_to_3d(self, pixel_coords: Tuple[int, int], depth: float) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Convert pixel coordinates to 3D position in robot frame\n",
    "        \n",
    "        Args:\n",
    "            pixel_coords: (u, v) pixel coordinates\n",
    "            depth: Depth value in millimeters\n",
    "        \n",
    "        Returns:\n",
    "            position_3d: [x, y, z] in robot base frame (meters)\n",
    "        \"\"\"\n",
    "        # Camera intrinsics (example for RealSense D435)\n",
    "        fx, fy = 616.0, 616.0  # Focal lengths\n",
    "        cx, cy = 320.0, 240.0  # Principal point\n",
    "        \n",
    "        u, v = pixel_coords\n",
    "        z_cam = depth / 1000.0  # mm to meters\n",
    "        \n",
    "        # Camera frame coordinates\n",
    "        x_cam = (u - cx) * z_cam / fx\n",
    "        y_cam = (v - cy) * z_cam / fy\n",
    "        \n",
    "        # Transform to robot base frame\n",
    "        # Assuming camera is mounted on head looking forward\n",
    "        # This transform depends on camera mounting position\n",
    "        # Example: camera at [0.1, 0, 1.5] looking forward\n",
    "        camera_offset = np.array([0.1, 0, 1.5])  # Camera position on robot\n",
    "        \n",
    "        # Camera frame: x-right, y-down, z-forward\n",
    "        # Robot frame: x-forward, y-left, z-up\n",
    "        position_robot = camera_offset + np.array([z_cam, -x_cam, -y_cam])\n",
    "        \n",
    "        return position_robot\n",
    "    \n",
    "    def visualize_detections(self, color_image: np.ndarray, objects: List[dict]):\n",
    "        \"\"\"\n",
    "        Draw bounding boxes on detected objects\n",
    "        \"\"\"\n",
    "        vis_image = color_image.copy()\n",
    "        \n",
    "        for obj in objects:\n",
    "            x, y, w, h = obj['bbox']\n",
    "            cv2.rectangle(vis_image, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "            \n",
    "            # Label with depth\n",
    "            depth_mm = obj['depth']\n",
    "            label = f\"{depth_mm:.0f}mm\"\n",
    "            cv2.putText(vis_image, label, (x, y-10), \n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "        \n",
    "        # Display\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.imshow(cv2.cvtColor(vis_image, cv2.COLOR_BGR2RGB))\n",
    "        plt.title(f\"Detected {len(objects)} objects\")\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "# Example usage\n",
    "grasp_detector = RealSenseGraspDetector()\n",
    "\n",
    "if grasp_detector.camera_available:\n",
    "    color, depth = grasp_detector.capture_frame()\n",
    "    objects = grasp_detector.detect_objects(color, depth)\n",
    "    grasp_detector.visualize_detections(color, objects)\n",
    "    \n",
    "    # Compute 3D positions\n",
    "    for i, obj in enumerate(objects):\n",
    "        pos_3d = grasp_detector.pixel_to_3d(obj['center_pixel'], obj['depth'])\n",
    "        print(f\"Object {i}: {pos_3d}\")\n",
    "else:\n",
    "    print(\"RealSense camera not available - using simulated data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='whole-body'></a>\n",
    "## 6. Whole-Body Motion Planning\n",
    "\n",
    "Coordinate legs and arms for complex manipulation during locomotion.\n",
    "\n",
    "### 6.1 Walking While Carrying Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WholeBodyController:\n",
    "    \"\"\"\n",
    "    Coordinate locomotion and manipulation\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, network_interface: str):\n",
    "        self.sport_client = LocoClient()\n",
    "        self.sport_client.SetTimeout(10.0)\n",
    "        self.sport_client.Init()\n",
    "        \n",
    "        self.balance_ctrl = BalanceController()\n",
    "    \n",
    "    def walk_with_object(self, \n",
    "                         target_velocity: Tuple[float, float, float],\n",
    "                         object_mass: float,\n",
    "                         duration: float = 5.0):\n",
    "        \"\"\"\n",
    "        Walk while carrying an object, adjusting gait for balance\n",
    "        \n",
    "        Args:\n",
    "            target_velocity: (vx, vy, vyaw) desired velocity\n",
    "            object_mass: Mass of carried object (kg)\n",
    "            duration: Walk duration (seconds)\n",
    "        \"\"\"\n",
    "        print(f\"üö∂ Walking with {object_mass}kg object...\")\n",
    "        \n",
    "        vx, vy, vyaw = target_velocity\n",
    "        \n",
    "        # Adjust velocity based on object mass (conservative scaling)\n",
    "        mass_factor = max(0.5, 1.0 - object_mass / 10.0)  # Reduce speed if heavy\n",
    "        vx_adjusted = vx * mass_factor\n",
    "        vy_adjusted = vy * mass_factor\n",
    "        vyaw_adjusted = vyaw * mass_factor\n",
    "        \n",
    "        print(f\"   Adjusted velocity: ({vx_adjusted:.2f}, {vy_adjusted:.2f}, {vyaw_adjusted:.2f})\")\n",
    "        \n",
    "        # Execute walking motion\n",
    "        start_time = time.time()\n",
    "        while time.time() - start_time < duration:\n",
    "            self.sport_client.Move(vx_adjusted, vy_adjusted, vyaw_adjusted)\n",
    "            time.sleep(0.1)\n",
    "        \n",
    "        # Stop\n",
    "        self.sport_client.Move(0, 0, 0)\n",
    "        print(\"‚úì Walking complete\")\n",
    "\n",
    "print(\"Whole-body controller ready\")\n",
    "print(\"Example: controller.walk_with_object((0.2, 0, 0), object_mass=1.5, duration=3.0)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='hri'></a>\n",
    "## 7. Human-Robot Interaction\n",
    "\n",
    "Detect and respond to human gestures using MediaPipe.\n",
    "\n",
    "### 7.1 Hand Gesture Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import mediapipe as mp\n",
    "    MEDIAPIPE_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"Install MediaPipe: pip install mediapipe\")\n",
    "    MEDIAPIPE_AVAILABLE = False\n",
    "\n",
    "if MEDIAPIPE_AVAILABLE:\n",
    "    class GestureRecognizer:\n",
    "        \"\"\"\n",
    "        Recognize hand gestures for robot control\n",
    "        \"\"\"\n",
    "        \n",
    "        def __init__(self):\n",
    "            self.mp_hands = mp.solutions.hands\n",
    "            self.hands = self.mp_hands.Hands(\n",
    "                static_image_mode=False,\n",
    "                max_num_hands=2,\n",
    "                min_detection_confidence=0.5\n",
    "            )\n",
    "            self.mp_draw = mp.solutions.drawing_utils\n",
    "        \n",
    "        def detect_gesture(self, image: np.ndarray) -> str:\n",
    "            \"\"\"\n",
    "            Detect gesture from RGB image\n",
    "            \n",
    "            Returns:\n",
    "                gesture_name: 'wave', 'point', 'thumbs_up', 'open_palm', etc.\n",
    "            \"\"\"\n",
    "            image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "            results = self.hands.process(image_rgb)\n",
    "            \n",
    "            if not results.multi_hand_landmarks:\n",
    "                return 'none'\n",
    "            \n",
    "            # Analyze first hand\n",
    "            hand_landmarks = results.multi_hand_landmarks[0]\n",
    "            \n",
    "            # Simple gesture classification based on finger positions\n",
    "            gesture = self._classify_gesture(hand_landmarks)\n",
    "            \n",
    "            return gesture\n",
    "        \n",
    "        def _classify_gesture(self, landmarks) -> str:\n",
    "            \"\"\"\n",
    "            Classify gesture based on landmark positions\n",
    "            \"\"\"\n",
    "            # Get fingertip and base positions\n",
    "            thumb_tip = landmarks.landmark[4]\n",
    "            index_tip = landmarks.landmark[8]\n",
    "            middle_tip = landmarks.landmark[12]\n",
    "            ring_tip = landmarks.landmark[16]\n",
    "            pinky_tip = landmarks.landmark[20]\n",
    "            \n",
    "            wrist = landmarks.landmark[0]\n",
    "            \n",
    "            # Check if fingers are extended (simple heuristic)\n",
    "            index_extended = index_tip.y < wrist.y\n",
    "            middle_extended = middle_tip.y < wrist.y\n",
    "            ring_extended = ring_tip.y < wrist.y\n",
    "            pinky_extended = pinky_tip.y < wrist.y\n",
    "            \n",
    "            # Classify\n",
    "            if all([index_extended, middle_extended, ring_extended, pinky_extended]):\n",
    "                return 'open_palm'\n",
    "            elif index_extended and not any([middle_extended, ring_extended, pinky_extended]):\n",
    "                return 'point'\n",
    "            else:\n",
    "                return 'unknown'\n",
    "    \n",
    "    print(\"‚úì Gesture recognizer ready\")\n",
    "else:\n",
    "    print(\"MediaPipe not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='teleoperation'></a>\n",
    "## 8. Teleoperation Interfaces\n",
    "\n",
    "Control G1 using VR controllers or motion capture.\n",
    "\n",
    "### 8.1 Keyboard Teleoperation (Simple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"\\nüéÆ Teleoperation Interfaces\n",
    "\n",
    "Advanced teleoperation options:\n",
    "\n",
    "1. **Keyboard Control** (basic)\n",
    "   - W/A/S/D: Forward/Left/Back/Right\n",
    "   - Q/E: Rotate left/right\n",
    "   - Arrow keys: Arm control\n",
    "\n",
    "2. **VR Control** (requires VR headset + SteamVR)\n",
    "   - Use OpenVR/PyOpenVR to read controller poses\n",
    "   - Map controller positions to arm IK targets\n",
    "   - Trigger for grasp/release\n",
    "\n",
    "3. **Motion Capture** (requires mocap system)\n",
    "   - OptiTrack, Vicon, or marker-less (MediaPipe)\n",
    "   - Retarget human motions to robot joints\n",
    "   - Real-time imitation\n",
    "\n",
    "4. **Joystick/Gamepad** (requires pygame/inputs library)\n",
    "   - Left stick: Locomotion\n",
    "   - Right stick: Torso orientation\n",
    "   - Buttons: Arm gestures\n",
    "\n",
    "See MimicKit documentation for motion retargeting examples.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='visualization'></a>\n",
    "## 9. Advanced Visualization\n",
    "\n",
    "### 9.1 3D Robot Visualization with Open3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import open3d as o3d\n",
    "    OPEN3D_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"Install Open3D: pip install open3d\")\n",
    "    OPEN3D_AVAILABLE = False\n",
    "\n",
    "if OPEN3D_AVAILABLE:\n",
    "    print(\"\"\"\\nüé® 3D Visualization with Open3D\n",
    "    \n",
    "    To visualize robot state in 3D:\n",
    "    1. Load robot mesh (STL/OBJ files from CAD)\n",
    "    2. Update mesh poses based on joint angles (forward kinematics)\n",
    "    3. Render with Open3D visualizer\n",
    "    \n",
    "    Example:\n",
    "    ```python\n",
    "    import open3d as o3d\n",
    "    \n",
    "    # Load robot meshes\n",
    "    torso_mesh = o3d.io.read_triangle_mesh('meshes/torso.stl')\n",
    "    arm_mesh = o3d.io.read_triangle_mesh('meshes/arm.stl')\n",
    "    \n",
    "    # Apply transforms based on joint angles\n",
    "    # (Use forward kinematics)\n",
    "    \n",
    "    # Visualize\n",
    "    o3d.visualization.draw_geometries([torso_mesh, arm_mesh])\n",
    "    ```\n",
    "    \"\"\")\n",
    "else:\n",
    "    print(\"Open3D not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary & Next Steps\n",
    "\n",
    "This notebook covered advanced use cases for the Unitree G1:\n",
    "\n",
    "‚úÖ **Object Manipulation** - Grasping coffee cup with compliant control  \n",
    "‚úÖ **Reinforcement Learning** - MimicKit integration for custom behavior training  \n",
    "‚úÖ **Dynamic Balance** - CoM tracking and adaptive impedance  \n",
    "‚úÖ **Vision-Based Grasping** - RealSense depth camera for object detection  \n",
    "‚úÖ **Whole-Body Control** - Coordinated locomotion and manipulation  \n",
    "‚úÖ **Human-Robot Interaction** - Gesture recognition with MediaPipe  \n",
    "‚úÖ **Teleoperation** - VR and motion capture interfaces  \n",
    "‚úÖ **3D Visualization** - Open3D rendering  \n",
    "\n",
    "### Further Reading\n",
    "\n",
    "- **MimicKit**: https://github.com/xbpeng/MimicKit\n",
    "- **IsaacGym**: https://developer.nvidia.com/isaac-gym\n",
    "- **Stable-Baselines3**: https://stable-baselines3.readthedocs.io/\n",
    "- **RealSense SDK**: https://github.com/IntelRealSense/librealsense\n",
    "- **MediaPipe**: https://mediapipe.dev/\n",
    "\n",
    "---\n",
    "\n",
    "**‚ö†Ô∏è Safety Notes:**\n",
    "- Test all advanced behaviors in simulation first\n",
    "- Use low stiffness gains when testing new motions\n",
    "- Always have emergency stop ready\n",
    "- Ensure adequate workspace clearance\n",
    "- Monitor battery and motor temperatures\n",
    "\n",
    "---\n",
    "\n",
    "*Created for Unitree G1 Advanced Applications*  \n",
    "*Covers RL, vision, manipulation, and HRI*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# High-Level Reinforcement Learning for Unitree G1\n",
    "\n",
    "This notebook implements a **high-level RL framework** for the Unitree G1 humanoid robot, focusing on:\n",
    "\n",
    "1. **State Space:** Robot observations + USB controller commands\n",
    "2. **Action Space:** Abstract high-level actions (path planning, gestures, audio)\n",
    "3. **Reward Design:** Task-oriented rewards for real-world applications\n",
    "4. **Training Pipeline:** From simulation to real robot deployment\n",
    "\n",
    "## Key Concepts\n",
    "\n",
    "**High-Level RL** operates at the **task planning level**, abstracting away low-level motor control:\n",
    "\n",
    "```\n",
    "Traditional RL:        State ‚Üí Neural Network ‚Üí Joint Torques\n",
    "                       (Low-level, 29-DOF control)\n",
    "\n",
    "High-Level RL:         State ‚Üí Neural Network ‚Üí Abstract Actions\n",
    "                       (Task-level: \"walk to X\", \"wave hand\", \"say hello\")\n",
    "                       \n",
    "                       Abstract Actions ‚Üí Motion Primitives ‚Üí Joint Commands\n",
    "```\n",
    "\n",
    "**Benefits:**\n",
    "- Faster learning (smaller action space)\n",
    "- Better generalization\n",
    "- Safer (primitives are pre-validated)\n",
    "- Human-interpretable actions\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [State Space Design](#state-space)\n",
    "2. [USB Controller Integration](#controller)\n",
    "3. [High-Level Action Space](#actions)\n",
    "4. [Motion Primitives Library](#primitives)\n",
    "5. [Reward Function Design](#rewards)\n",
    "6. [Environment Implementation](#environment)\n",
    "7. [Training with PPO](#training)\n",
    "8. [Deployment to Real Robot](#deployment)\n",
    "9. [Example Applications](#applications)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup & Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install gymnasium stable-baselines3 torch numpy matplotlib\n",
    "!pip install pygame  # For USB controller support\n",
    "!pip install pynput  # Alternative: keyboard/mouse input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Dict, List, Tuple, Optional, Any\n",
    "from dataclasses import dataclass\n",
    "from enum import Enum\n",
    "import time\n",
    "import json\n",
    "\n",
    "# RL frameworks\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "try:\n",
    "    from stable_baselines3 import PPO\n",
    "    from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "    from stable_baselines3.common.callbacks import BaseCallback\n",
    "    SB3_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"Warning: stable-baselines3 not available\")\n",
    "    SB3_AVAILABLE = False\n",
    "\n",
    "# USB Controller\n",
    "try:\n",
    "    import pygame\n",
    "    PYGAME_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"Warning: pygame not available (USB controller input disabled)\")\n",
    "    PYGAME_AVAILABLE = False\n",
    "\n",
    "# Unitree SDK\n",
    "try:\n",
    "    from unitree_sdk2py.core.channel import ChannelSubscriber, ChannelPublisher, ChannelFactoryInitialize\n",
    "    from unitree_sdk2py.idl.unitree_hg.msg.dds_ import LowCmd_, LowState_\n",
    "    from unitree_sdk2py.g1.loco.g1_loco_client import LocoClient\n",
    "    from unitree_sdk2py.g1.audio.g1_audio_client import AudioClient\n",
    "    from unitree_sdk2py.g1.arm.g1_arm_action_client import G1ArmActionClient, action_map\n",
    "    UNITREE_SDK_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"Warning: Unitree SDK not available (simulation only)\")\n",
    "    UNITREE_SDK_AVAILABLE = False\n",
    "\n",
    "print(\"‚úì Imports complete\")\n",
    "print(f\"  - PyGame (Controller): {PYGAME_AVAILABLE}\")\n",
    "print(f\"  - Stable-Baselines3: {SB3_AVAILABLE}\")\n",
    "print(f\"  - Unitree SDK: {UNITREE_SDK_AVAILABLE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='state-space'></a>\n",
    "## 1. State Space Design\n",
    "\n",
    "The state space combines **robot observations** and **human commands** from a USB controller.\n",
    "\n",
    "### State Components\n",
    "\n",
    "#### A. Robot Observations (Proprioceptive)\n",
    "- **Base Orientation:** Roll, pitch, yaw (3D)\n",
    "- **Base Angular Velocity:** œâx, œây, œâz (3D)\n",
    "- **Base Linear Velocity:** vx, vy, vz (3D)\n",
    "- **Joint Positions:** 29 joint angles (29D)\n",
    "- **Joint Velocities:** 29 joint velocities (29D)\n",
    "- **Contact State:** Foot contact sensors (2D - left/right)\n",
    "\n",
    "#### B. Robot Observations (Exteroceptive)\n",
    "- **Goal Direction:** Relative vector to target (3D)\n",
    "- **Goal Distance:** Scalar distance to target (1D)\n",
    "- **Obstacle Proximity:** Nearest obstacle distance in 8 directions (8D)\n",
    "\n",
    "#### C. Human Commands (USB Controller)\n",
    "- **Velocity Command:** Desired (vx, vy, œâ) from joystick (3D)\n",
    "- **Mode Selector:** Discrete mode (walk/run/idle) (1D)\n",
    "- **Gesture Trigger:** Which gesture to perform (1D)\n",
    "\n",
    "**Total State Dimension:** 3 + 3 + 3 + 29 + 29 + 2 + 3 + 1 + 8 + 3 + 1 + 1 = **86 dimensions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class RobotState:\n",
    "    \"\"\"Complete state representation for high-level RL\"\"\"\n",
    "    \n",
    "    # Proprioceptive (internal sensors)\n",
    "    base_orientation: np.ndarray  # [roll, pitch, yaw] in radians\n",
    "    base_angular_velocity: np.ndarray  # [wx, wy, wz] in rad/s\n",
    "    base_linear_velocity: np.ndarray  # [vx, vy, vz] in m/s\n",
    "    joint_positions: np.ndarray  # 29 joint angles in radians\n",
    "    joint_velocities: np.ndarray  # 29 joint velocities in rad/s\n",
    "    foot_contacts: np.ndarray  # [left_contact, right_contact] (0 or 1)\n",
    "    \n",
    "    # Exteroceptive (external sensors)\n",
    "    goal_direction: np.ndarray  # Unit vector pointing to goal [x, y, z]\n",
    "    goal_distance: float  # Euclidean distance to goal in meters\n",
    "    obstacle_proximity: np.ndarray  # Distance to nearest obstacle in 8 directions\n",
    "    \n",
    "    # Human commands (from USB controller)\n",
    "    cmd_velocity: np.ndarray  # Desired [vx, vy, omega] from joystick\n",
    "    cmd_mode: int  # 0=idle, 1=walk, 2=run\n",
    "    cmd_gesture: int  # Gesture ID (0=none, 1=wave, 2=handshake, etc.)\n",
    "    \n",
    "    def to_array(self) -> np.ndarray:\n",
    "        \"\"\"Convert state to flat numpy array for RL\"\"\"\n",
    "        return np.concatenate([\n",
    "            self.base_orientation,\n",
    "            self.base_angular_velocity,\n",
    "            self.base_linear_velocity,\n",
    "            self.joint_positions,\n",
    "            self.joint_velocities,\n",
    "            self.foot_contacts,\n",
    "            self.goal_direction,\n",
    "            [self.goal_distance],\n",
    "            self.obstacle_proximity,\n",
    "            self.cmd_velocity,\n",
    "            [self.cmd_mode],\n",
    "            [self.cmd_gesture]\n",
    "        ])\n",
    "    \n",
    "    @property\n",
    "    def dimension(self) -> int:\n",
    "        \"\"\"Total state dimension\"\"\"\n",
    "        return len(self.to_array())\n",
    "\n",
    "# Example state initialization\n",
    "example_state = RobotState(\n",
    "    base_orientation=np.zeros(3),\n",
    "    base_angular_velocity=np.zeros(3),\n",
    "    base_linear_velocity=np.zeros(3),\n",
    "    joint_positions=np.zeros(29),\n",
    "    joint_velocities=np.zeros(29),\n",
    "    foot_contacts=np.array([1, 1]),  # Both feet on ground\n",
    "    goal_direction=np.array([1, 0, 0]),  # Goal straight ahead\n",
    "    goal_distance=5.0,  # 5 meters away\n",
    "    obstacle_proximity=np.ones(8) * 10.0,  # No nearby obstacles\n",
    "    cmd_velocity=np.zeros(3),\n",
    "    cmd_mode=1,  # Walk mode\n",
    "    cmd_gesture=0  # No gesture\n",
    ")\n",
    "\n",
    "print(f\"State dimension: {example_state.dimension}\")\n",
    "print(f\"State vector shape: {example_state.to_array().shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='controller'></a>\n",
    "## 2. USB Controller Integration\n",
    "\n",
    "We'll use a standard USB game controller (e.g., Xbox, PlayStation) to provide human commands.\n",
    "\n",
    "### Controller Mapping\n",
    "\n",
    "```\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ        USB Game Controller              ‚îÇ\n",
    "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
    "‚îÇ                                         ‚îÇ\n",
    "‚îÇ  Left Stick:  ‚Üí Forward/Backward (vx)  ‚îÇ\n",
    "‚îÇ               ‚Üí Strafe Left/Right (vy)  ‚îÇ\n",
    "‚îÇ                                         ‚îÇ\n",
    "‚îÇ  Right Stick: ‚Üí Rotate Left/Right (œâ)  ‚îÇ\n",
    "‚îÇ                                         ‚îÇ\n",
    "‚îÇ  Buttons:                               ‚îÇ\n",
    "‚îÇ    A/X     ‚Üí Walk Mode                  ‚îÇ\n",
    "‚îÇ    B/Circle ‚Üí Run Mode                   ‚îÇ\n",
    "‚îÇ    Y/Triangle ‚Üí Idle/Stop                ‚îÇ\n",
    "‚îÇ    LB      ‚Üí Wave Hand                  ‚îÇ\n",
    "‚îÇ    RB      ‚Üí Handshake                  ‚îÇ\n",
    "‚îÇ    LT      ‚Üí Clap                       ‚îÇ\n",
    "‚îÇ    Start   ‚Üí Emergency Stop             ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class USBControllerInput:\n",
    "    \"\"\"Read commands from USB game controller (Xbox/PlayStation compatible)\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.available = False\n",
    "        self.joystick = None\n",
    "        \n",
    "        if PYGAME_AVAILABLE:\n",
    "            try:\n",
    "                pygame.init()\n",
    "                pygame.joystick.init()\n",
    "                \n",
    "                if pygame.joystick.get_count() > 0:\n",
    "                    self.joystick = pygame.joystick.Joystick(0)\n",
    "                    self.joystick.init()\n",
    "                    self.available = True\n",
    "                    print(f\"‚úì Controller connected: {self.joystick.get_name()}\")\n",
    "                    print(f\"  Axes: {self.joystick.get_numaxes()}\")\n",
    "                    print(f\"  Buttons: {self.joystick.get_numbuttons()}\")\n",
    "                else:\n",
    "                    print(\"‚ö†Ô∏è  No USB controller detected\")\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è  Controller initialization failed: {e}\")\n",
    "        \n",
    "        # Default command values\n",
    "        self.cmd_velocity = np.zeros(3)\n",
    "        self.cmd_mode = 0\n",
    "        self.cmd_gesture = 0\n",
    "        self.emergency_stop = False\n",
    "    \n",
    "    def read_commands(self) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Read current controller state\n",
    "        \n",
    "        Returns:\n",
    "            commands: Dictionary with velocity, mode, gesture commands\n",
    "        \"\"\"\n",
    "        if not self.available:\n",
    "            # Return default commands if no controller\n",
    "            return {\n",
    "                'velocity': self.cmd_velocity,\n",
    "                'mode': self.cmd_mode,\n",
    "                'gesture': self.cmd_gesture,\n",
    "                'emergency_stop': False\n",
    "            }\n",
    "        \n",
    "        # Process pygame events\n",
    "        pygame.event.pump()\n",
    "        \n",
    "        # Read axes (joysticks)\n",
    "        # Left stick: axis 0 (left/right), axis 1 (up/down)\n",
    "        # Right stick: axis 3 (left/right)\n",
    "        left_x = self.joystick.get_axis(0) if self.joystick.get_numaxes() > 0 else 0.0\n",
    "        left_y = -self.joystick.get_axis(1) if self.joystick.get_numaxes() > 1 else 0.0  # Invert Y\n",
    "        right_x = self.joystick.get_axis(3) if self.joystick.get_numaxes() > 3 else 0.0\n",
    "        \n",
    "        # Apply deadzone (ignore small movements)\n",
    "        deadzone = 0.15\n",
    "        left_x = left_x if abs(left_x) > deadzone else 0.0\n",
    "        left_y = left_y if abs(left_y) > deadzone else 0.0\n",
    "        right_x = right_x if abs(right_x) > deadzone else 0.0\n",
    "        \n",
    "        # Map to velocity commands\n",
    "        # Scale: joystick [-1, 1] ‚Üí velocity [max_speed]\n",
    "        max_linear = 0.5  # m/s\n",
    "        max_angular = 0.8  # rad/s\n",
    "        \n",
    "        self.cmd_velocity = np.array([\n",
    "            left_y * max_linear,   # Forward/backward\n",
    "            left_x * max_linear,   # Strafe left/right\n",
    "            right_x * max_angular  # Rotate\n",
    "        ])\n",
    "        \n",
    "        # Read buttons\n",
    "        button_a = self.joystick.get_button(0) if self.joystick.get_numbuttons() > 0 else 0\n",
    "        button_b = self.joystick.get_button(1) if self.joystick.get_numbuttons() > 1 else 0\n",
    "        button_y = self.joystick.get_button(3) if self.joystick.get_numbuttons() > 3 else 0\n",
    "        button_lb = self.joystick.get_button(4) if self.joystick.get_numbuttons() > 4 else 0\n",
    "        button_rb = self.joystick.get_button(5) if self.joystick.get_numbuttons() > 5 else 0\n",
    "        button_lt = self.joystick.get_button(6) if self.joystick.get_numbuttons() > 6 else 0\n",
    "        button_start = self.joystick.get_button(7) if self.joystick.get_numbuttons() > 7 else 0\n",
    "        \n",
    "        # Mode selection\n",
    "        if button_a:\n",
    "            self.cmd_mode = 1  # Walk\n",
    "        elif button_b:\n",
    "            self.cmd_mode = 2  # Run\n",
    "        elif button_y:\n",
    "            self.cmd_mode = 0  # Idle\n",
    "        \n",
    "        # Gesture selection\n",
    "        if button_lb:\n",
    "            self.cmd_gesture = 1  # Wave\n",
    "        elif button_rb:\n",
    "            self.cmd_gesture = 2  # Handshake\n",
    "        elif button_lt:\n",
    "            self.cmd_gesture = 3  # Clap\n",
    "        else:\n",
    "            self.cmd_gesture = 0  # None\n",
    "        \n",
    "        # Emergency stop\n",
    "        self.emergency_stop = bool(button_start)\n",
    "        \n",
    "        return {\n",
    "            'velocity': self.cmd_velocity.copy(),\n",
    "            'mode': self.cmd_mode,\n",
    "            'gesture': self.cmd_gesture,\n",
    "            'emergency_stop': self.emergency_stop\n",
    "        }\n",
    "    \n",
    "    def get_command_string(self) -> str:\n",
    "        \"\"\"Human-readable command description\"\"\"\n",
    "        mode_names = ['IDLE', 'WALK', 'RUN']\n",
    "        gesture_names = ['None', 'Wave', 'Handshake', 'Clap']\n",
    "        \n",
    "        return (\n",
    "            f\"Mode: {mode_names[self.cmd_mode]} | \"\n",
    "            f\"Vel: [{self.cmd_velocity[0]:.2f}, {self.cmd_velocity[1]:.2f}, {self.cmd_velocity[2]:.2f}] | \"\n",
    "            f\"Gesture: {gesture_names[self.cmd_gesture]}\"\n",
    "        )\n",
    "\n",
    "# Test controller\n",
    "controller = USBControllerInput()\n",
    "\n",
    "if controller.available:\n",
    "    print(\"\\nTesting controller (move sticks and press buttons)...\")\n",
    "    for i in range(50):  # Read for 5 seconds\n",
    "        commands = controller.read_commands()\n",
    "        print(f\"\\r{controller.get_command_string()}\", end='', flush=True)\n",
    "        time.sleep(0.1)\n",
    "    print(\"\\n‚úì Controller test complete\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  Controller not available - will use simulated commands\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='actions'></a>\n",
    "## 3. High-Level Action Space\n",
    "\n",
    "Instead of directly controlling 29 joints, we define **high-level abstract actions**:\n",
    "\n",
    "### Action Space Design\n",
    "\n",
    "#### Discrete Actions (10 total)\n",
    "\n",
    "| Action ID | Name | Description |\n",
    "|-----------|------|-------------|\n",
    "| 0 | `IDLE` | Stand still, damping mode |\n",
    "| 1 | `WALK_FORWARD` | Walk forward at commanded velocity |\n",
    "| 2 | `WALK_BACKWARD` | Walk backward |\n",
    "| 3 | `STRAFE_LEFT` | Sidestep left |\n",
    "| 4 | `STRAFE_RIGHT` | Sidestep right |\n",
    "| 5 | `ROTATE_LEFT` | Turn left in place |\n",
    "| 6 | `ROTATE_RIGHT` | Turn right in place |\n",
    "| 7 | `GESTURE_WAVE` | Perform wave gesture |\n",
    "| 8 | `GESTURE_HANDSHAKE` | Perform handshake gesture |\n",
    "| 9 | `GESTURE_CLAP` | Perform clap gesture |\n",
    "\n",
    "#### Continuous Parameters (3D)\n",
    "\n",
    "- **Velocity Scale:** [0, 1] - How fast to execute action\n",
    "- **Direction Bias:** [-1, 1] - Fine-tune direction\n",
    "- **Audio Volume:** [0, 1] - Volume for audio feedback\n",
    "\n",
    "**Total Action Space:** Discrete(10) + Box(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HighLevelAction(Enum):\n",
    "    \"\"\"Enumeration of high-level actions\"\"\"\n",
    "    IDLE = 0\n",
    "    WALK_FORWARD = 1\n",
    "    WALK_BACKWARD = 2\n",
    "    STRAFE_LEFT = 3\n",
    "    STRAFE_RIGHT = 4\n",
    "    ROTATE_LEFT = 5\n",
    "    ROTATE_RIGHT = 6\n",
    "    GESTURE_WAVE = 7\n",
    "    GESTURE_HANDSHAKE = 8\n",
    "    GESTURE_CLAP = 9\n",
    "\n",
    "@dataclass\n",
    "class ActionCommand:\n",
    "    \"\"\"Complete action specification\"\"\"\n",
    "    action_type: HighLevelAction\n",
    "    velocity_scale: float  # [0, 1]\n",
    "    direction_bias: float  # [-1, 1]\n",
    "    audio_volume: float  # [0, 1]\n",
    "    \n",
    "    def to_motion_command(self) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Convert high-level action to low-level motion command\n",
    "        \n",
    "        Returns:\n",
    "            motion_cmd: Dictionary with target velocities and gesture flags\n",
    "        \"\"\"\n",
    "        # Base velocities for each action type\n",
    "        action_velocities = {\n",
    "            HighLevelAction.IDLE: (0.0, 0.0, 0.0),\n",
    "            HighLevelAction.WALK_FORWARD: (0.3, 0.0, 0.0),\n",
    "            HighLevelAction.WALK_BACKWARD: (-0.2, 0.0, 0.0),\n",
    "            HighLevelAction.STRAFE_LEFT: (0.0, 0.2, 0.0),\n",
    "            HighLevelAction.STRAFE_RIGHT: (0.0, -0.2, 0.0),\n",
    "            HighLevelAction.ROTATE_LEFT: (0.0, 0.0, 0.5),\n",
    "            HighLevelAction.ROTATE_RIGHT: (0.0, 0.0, -0.5),\n",
    "            HighLevelAction.GESTURE_WAVE: (0.0, 0.0, 0.0),\n",
    "            HighLevelAction.GESTURE_HANDSHAKE: (0.0, 0.0, 0.0),\n",
    "            HighLevelAction.GESTURE_CLAP: (0.0, 0.0, 0.0),\n",
    "        }\n",
    "        \n",
    "        vx, vy, vw = action_velocities[self.action_type]\n",
    "        \n",
    "        # Apply velocity scaling\n",
    "        vx *= self.velocity_scale\n",
    "        vy *= self.velocity_scale\n",
    "        vw *= self.velocity_scale\n",
    "        \n",
    "        # Apply direction bias (adds lateral component)\n",
    "        vy += self.direction_bias * 0.1\n",
    "        \n",
    "        # Determine if gesture should be executed\n",
    "        is_gesture = self.action_type.value >= 7\n",
    "        gesture_id = self.action_type.value - 7 if is_gesture else None\n",
    "        \n",
    "        return {\n",
    "            'velocity': np.array([vx, vy, vw]),\n",
    "            'is_gesture': is_gesture,\n",
    "            'gesture_id': gesture_id,\n",
    "            'audio_volume': self.audio_volume\n",
    "        }\n",
    "    \n",
    "    def get_description(self) -> str:\n",
    "        \"\"\"Human-readable action description\"\"\"\n",
    "        return (\n",
    "            f\"{self.action_type.name} \"\n",
    "            f\"(speed={self.velocity_scale:.2f}, \"\n",
    "            f\"bias={self.direction_bias:.2f}, \"\n",
    "            f\"volume={self.audio_volume:.2f})\"\n",
    "        )\n",
    "\n",
    "# Example actions\n",
    "example_actions = [\n",
    "    ActionCommand(HighLevelAction.WALK_FORWARD, 0.8, 0.0, 0.5),\n",
    "    ActionCommand(HighLevelAction.ROTATE_LEFT, 0.5, 0.0, 0.3),\n",
    "    ActionCommand(HighLevelAction.GESTURE_WAVE, 1.0, 0.0, 0.8),\n",
    "]\n",
    "\n",
    "print(\"Example high-level actions:\\n\")\n",
    "for action in example_actions:\n",
    "    print(f\"  {action.get_description()}\")\n",
    "    motion_cmd = action.to_motion_command()\n",
    "    print(f\"    ‚Üí Velocity: {motion_cmd['velocity']}\")\n",
    "    print(f\"    ‚Üí Gesture: {motion_cmd['is_gesture']}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='primitives'></a>\n",
    "## 4. Motion Primitives Library\n",
    "\n",
    "Motion primitives are **pre-validated movement sequences** that abstract low-level control."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MotionPrimitive:\n",
    "    \"\"\"Base class for motion primitives\"\"\"\n",
    "    \n",
    "    def __init__(self, name: str, duration: float):\n",
    "        self.name = name\n",
    "        self.duration = duration  # seconds\n",
    "        self.is_executing = False\n",
    "        self.start_time = None\n",
    "    \n",
    "    def execute(self, robot_interface) -> bool:\n",
    "        \"\"\"\n",
    "        Execute the motion primitive\n",
    "        \n",
    "        Args:\n",
    "            robot_interface: Connection to real or simulated robot\n",
    "        \n",
    "        Returns:\n",
    "            completed: True if primitive finished\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"Reset primitive state\"\"\"\n",
    "        self.is_executing = False\n",
    "        self.start_time = None\n",
    "\n",
    "class LocomotionPrimitive(MotionPrimitive):\n",
    "    \"\"\"Locomotion primitive: walk with specified velocity\"\"\"\n",
    "    \n",
    "    def __init__(self, name: str, target_velocity: np.ndarray, duration: float = 1.0):\n",
    "        super().__init__(name, duration)\n",
    "        self.target_velocity = target_velocity  # [vx, vy, vw]\n",
    "    \n",
    "    def execute(self, robot_interface) -> bool:\n",
    "        if not self.is_executing:\n",
    "            self.is_executing = True\n",
    "            self.start_time = time.time()\n",
    "        \n",
    "        elapsed = time.time() - self.start_time\n",
    "        \n",
    "        if elapsed < self.duration:\n",
    "            # Send velocity command\n",
    "            if hasattr(robot_interface, 'send_velocity'):\n",
    "                robot_interface.send_velocity(self.target_velocity)\n",
    "            return False  # Still executing\n",
    "        else:\n",
    "            # Completed\n",
    "            self.reset()\n",
    "            return True\n",
    "\n",
    "class GesturePrimitive(MotionPrimitive):\n",
    "    \"\"\"Gesture primitive: pre-defined arm motion\"\"\"\n",
    "    \n",
    "    def __init__(self, name: str, gesture_id: int, duration: float = 3.0):\n",
    "        super().__init__(name, duration)\n",
    "        self.gesture_id = gesture_id\n",
    "    \n",
    "    def execute(self, robot_interface) -> bool:\n",
    "        if not self.is_executing:\n",
    "            self.is_executing = True\n",
    "            self.start_time = time.time()\n",
    "            \n",
    "            # Trigger gesture\n",
    "            if hasattr(robot_interface, 'execute_gesture'):\n",
    "                robot_interface.execute_gesture(self.gesture_id)\n",
    "        \n",
    "        elapsed = time.time() - self.start_time\n",
    "        \n",
    "        if elapsed >= self.duration:\n",
    "            self.reset()\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "class AudioPrimitive(MotionPrimitive):\n",
    "    \"\"\"Audio primitive: play sound or TTS\"\"\"\n",
    "    \n",
    "    def __init__(self, name: str, message: str, volume: float = 0.5):\n",
    "        super().__init__(name, duration=2.0)\n",
    "        self.message = message\n",
    "        self.volume = volume\n",
    "    \n",
    "    def execute(self, robot_interface) -> bool:\n",
    "        if not self.is_executing:\n",
    "            self.is_executing = True\n",
    "            self.start_time = time.time()\n",
    "            \n",
    "            # Play audio\n",
    "            if hasattr(robot_interface, 'speak'):\n",
    "                robot_interface.speak(self.message, self.volume)\n",
    "        \n",
    "        elapsed = time.time() - self.start_time\n",
    "        \n",
    "        if elapsed >= self.duration:\n",
    "            self.reset()\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "class MotionPrimitiveLibrary:\n",
    "    \"\"\"Library of pre-defined motion primitives\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.primitives = {}\n",
    "        self._initialize_primitives()\n",
    "    \n",
    "    def _initialize_primitives(self):\n",
    "        \"\"\"Create standard motion primitives\"\"\"\n",
    "        # Locomotion primitives\n",
    "        self.primitives['walk_forward'] = LocomotionPrimitive(\n",
    "            'walk_forward', np.array([0.3, 0, 0]), duration=1.0\n",
    "        )\n",
    "        self.primitives['walk_backward'] = LocomotionPrimitive(\n",
    "            'walk_backward', np.array([-0.2, 0, 0]), duration=1.0\n",
    "        )\n",
    "        self.primitives['strafe_left'] = LocomotionPrimitive(\n",
    "            'strafe_left', np.array([0, 0.2, 0]), duration=1.0\n",
    "        )\n",
    "        self.primitives['strafe_right'] = LocomotionPrimitive(\n",
    "            'strafe_right', np.array([0, -0.2, 0]), duration=1.0\n",
    "        )\n",
    "        self.primitives['rotate_left'] = LocomotionPrimitive(\n",
    "            'rotate_left', np.array([0, 0, 0.5]), duration=1.0\n",
    "        )\n",
    "        self.primitives['rotate_right'] = LocomotionPrimitive(\n",
    "            'rotate_right', np.array([0, 0, -0.5]), duration=1.0\n",
    "        )\n",
    "        \n",
    "        # Gesture primitives\n",
    "        self.primitives['wave'] = GesturePrimitive('wave', gesture_id=0, duration=3.0)\n",
    "        self.primitives['handshake'] = GesturePrimitive('handshake', gesture_id=1, duration=4.0)\n",
    "        self.primitives['clap'] = GesturePrimitive('clap', gesture_id=2, duration=3.0)\n",
    "        \n",
    "        # Audio primitives\n",
    "        self.primitives['greet'] = AudioPrimitive('greet', \"Hello, nice to meet you!\")\n",
    "        self.primitives['acknowledge'] = AudioPrimitive('acknowledge', \"Understood\")\n",
    "    \n",
    "    def get_primitive(self, name: str) -> Optional[MotionPrimitive]:\n",
    "        \"\"\"Retrieve primitive by name\"\"\"\n",
    "        return self.primitives.get(name)\n",
    "    \n",
    "    def list_primitives(self) -> List[str]:\n",
    "        \"\"\"List all available primitives\"\"\"\n",
    "        return list(self.primitives.keys())\n",
    "\n",
    "# Initialize library\n",
    "motion_lib = MotionPrimitiveLibrary()\n",
    "print(\"Motion Primitive Library initialized\\n\")\n",
    "print(\"Available primitives:\")\n",
    "for name in motion_lib.list_primitives():\n",
    "    primitive = motion_lib.get_primitive(name)\n",
    "    print(f\"  - {name:20s} (duration: {primitive.duration:.1f}s)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='rewards'></a>\n",
    "## 5. Reward Function Design\n",
    "\n",
    "A well-designed reward function is critical for RL success. We'll use a **simple, sparse reward** approach.\n",
    "\n",
    "### Reward Components\n",
    "\n",
    "#### 1. Goal Reaching Reward (+100)\n",
    "- Large positive reward when robot reaches goal\n",
    "- Encourages task completion\n",
    "\n",
    "#### 2. Progress Reward (+0.1 per step)\n",
    "- Small reward for moving closer to goal\n",
    "- Provides learning signal before goal is reached\n",
    "\n",
    "#### 3. Command Following Reward (+0.5)\n",
    "- Reward for following human controller commands\n",
    "- Encourages responsive behavior\n",
    "\n",
    "#### 4. Stability Penalty (-1.0 per fall)\n",
    "- Negative reward if robot falls (large tilt)\n",
    "- Ensures safety\n",
    "\n",
    "#### 5. Collision Penalty (-0.5)\n",
    "- Penalty for hitting obstacles\n",
    "- Encourages obstacle avoidance\n",
    "\n",
    "#### 6. Energy Penalty (-0.001 per action)\n",
    "- Small penalty for movement\n",
    "- Encourages efficiency\n",
    "\n",
    "**Total Reward:**\n",
    "```\n",
    "r = r_goal + r_progress + r_command + r_stability + r_collision + r_energy\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RewardFunction:\n",
    "    \"\"\"Compute reward for high-level RL\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Reward weights\n",
    "        self.w_goal = 100.0\n",
    "        self.w_progress = 0.1\n",
    "        self.w_command = 0.5\n",
    "        self.w_stability = -1.0\n",
    "        self.w_collision = -0.5\n",
    "        self.w_energy = -0.001\n",
    "        \n",
    "        # Thresholds\n",
    "        self.goal_radius = 0.5  # meters\n",
    "        self.fall_angle = 0.5  # radians (~30 degrees)\n",
    "        self.collision_distance = 0.3  # meters\n",
    "        \n",
    "        # Tracking\n",
    "        self.previous_goal_distance = None\n",
    "    \n",
    "    def compute_reward(\n",
    "        self,\n",
    "        state: RobotState,\n",
    "        action: ActionCommand,\n",
    "        next_state: RobotState\n",
    "    ) -> Tuple[float, Dict[str, float]]:\n",
    "        \"\"\"\n",
    "        Compute reward for state-action-next_state transition\n",
    "        \n",
    "        Returns:\n",
    "            total_reward: Scalar reward\n",
    "            reward_breakdown: Dictionary with individual components\n",
    "        \"\"\"\n",
    "        rewards = {}\n",
    "        \n",
    "        # 1. Goal reaching\n",
    "        reached_goal = next_state.goal_distance < self.goal_radius\n",
    "        rewards['goal'] = self.w_goal if reached_goal else 0.0\n",
    "        \n",
    "        # 2. Progress toward goal\n",
    "        if self.previous_goal_distance is not None:\n",
    "            progress = self.previous_goal_distance - next_state.goal_distance\n",
    "            rewards['progress'] = self.w_progress * progress\n",
    "        else:\n",
    "            rewards['progress'] = 0.0\n",
    "        \n",
    "        self.previous_goal_distance = next_state.goal_distance\n",
    "        \n",
    "        # 3. Command following (alignment with human commands)\n",
    "        # Measure how well action aligns with commanded velocity\n",
    "        motion_cmd = action.to_motion_command()\n",
    "        commanded_vel = state.cmd_velocity\n",
    "        actual_vel = motion_cmd['velocity']\n",
    "        \n",
    "        # Cosine similarity (if non-zero commands)\n",
    "        if np.linalg.norm(commanded_vel) > 0.01:\n",
    "            alignment = np.dot(commanded_vel, actual_vel) / (\n",
    "                np.linalg.norm(commanded_vel) * (np.linalg.norm(actual_vel) + 1e-6)\n",
    "            )\n",
    "            rewards['command'] = self.w_command * max(0, alignment)\n",
    "        else:\n",
    "            rewards['command'] = 0.0\n",
    "        \n",
    "        # 4. Stability penalty\n",
    "        roll, pitch, _ = next_state.base_orientation\n",
    "        fell = abs(roll) > self.fall_angle or abs(pitch) > self.fall_angle\n",
    "        rewards['stability'] = self.w_stability if fell else 0.0\n",
    "        \n",
    "        # 5. Collision penalty\n",
    "        min_obstacle_dist = np.min(next_state.obstacle_proximity)\n",
    "        collided = min_obstacle_dist < self.collision_distance\n",
    "        rewards['collision'] = self.w_collision if collided else 0.0\n",
    "        \n",
    "        # 6. Energy penalty (encourage efficiency)\n",
    "        action_magnitude = np.linalg.norm(motion_cmd['velocity'])\n",
    "        rewards['energy'] = self.w_energy * action_magnitude\n",
    "        \n",
    "        # Total reward\n",
    "        total_reward = sum(rewards.values())\n",
    "        \n",
    "        return total_reward, rewards\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"Reset tracking variables\"\"\"\n",
    "        self.previous_goal_distance = None\n",
    "\n",
    "# Example reward computation\n",
    "reward_fn = RewardFunction()\n",
    "\n",
    "# Simulate scenario: robot moving toward goal\n",
    "state = RobotState(\n",
    "    base_orientation=np.array([0.05, 0.02, 0.0]),\n",
    "    base_angular_velocity=np.zeros(3),\n",
    "    base_linear_velocity=np.array([0.3, 0, 0]),\n",
    "    joint_positions=np.zeros(29),\n",
    "    joint_velocities=np.zeros(29),\n",
    "    foot_contacts=np.array([1, 1]),\n",
    "    goal_direction=np.array([1, 0, 0]),\n",
    "    goal_distance=5.0,\n",
    "    obstacle_proximity=np.ones(8) * 10.0,\n",
    "    cmd_velocity=np.array([0.3, 0, 0]),\n",
    "    cmd_mode=1,\n",
    "    cmd_gesture=0\n",
    ")\n",
    "\n",
    "action = ActionCommand(HighLevelAction.WALK_FORWARD, 0.8, 0.0, 0.5)\n",
    "\n",
    "next_state = RobotState(\n",
    "    base_orientation=np.array([0.03, 0.01, 0.0]),\n",
    "    base_angular_velocity=np.zeros(3),\n",
    "    base_linear_velocity=np.array([0.3, 0, 0]),\n",
    "    joint_positions=np.zeros(29),\n",
    "    joint_velocities=np.zeros(29),\n",
    "    foot_contacts=np.array([1, 1]),\n",
    "    goal_direction=np.array([1, 0, 0]),\n",
    "    goal_distance=4.7,  # Moved 0.3m closer\n",
    "    obstacle_proximity=np.ones(8) * 10.0,\n",
    "    cmd_velocity=np.array([0.3, 0, 0]),\n",
    "    cmd_mode=1,\n",
    "    cmd_gesture=0\n",
    ")\n",
    "\n",
    "reward, breakdown = reward_fn.compute_reward(state, action, next_state)\n",
    "\n",
    "print(\"Reward Computation Example:\\n\")\n",
    "print(f\"Action: {action.get_description()}\")\n",
    "print(f\"\\nReward Breakdown:\")\n",
    "for component, value in breakdown.items():\n",
    "    print(f\"  {component:15s}: {value:+7.4f}\")\n",
    "print(f\"\\nTotal Reward: {reward:+7.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='environment'></a>\n",
    "## 6. Gymnasium Environment Implementation\n",
    "\n",
    "We'll implement a Gymnasium environment that wraps everything together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class G1HighLevelEnv(gym.Env):\n",
    "    \"\"\"\n",
    "    High-level RL environment for Unitree G1\n",
    "    \n",
    "    State: 86D (robot obs + controller commands)\n",
    "    Action: Discrete(10) + Box(3) (high-level actions + parameters)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, use_controller: bool = True, simulation: bool = True):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.use_controller = use_controller\n",
    "        self.simulation = simulation\n",
    "        \n",
    "        # Initialize components\n",
    "        self.controller = USBControllerInput() if use_controller else None\n",
    "        self.motion_lib = MotionPrimitiveLibrary()\n",
    "        self.reward_fn = RewardFunction()\n",
    "        \n",
    "        # Define spaces\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=-np.inf,\n",
    "            high=np.inf,\n",
    "            shape=(86,),\n",
    "            dtype=np.float32\n",
    "        )\n",
    "        \n",
    "        # Action space: discrete action + continuous parameters\n",
    "        self.action_space = spaces.Dict({\n",
    "            'action_type': spaces.Discrete(10),\n",
    "            'parameters': spaces.Box(\n",
    "                low=np.array([0.0, -1.0, 0.0]),\n",
    "                high=np.array([1.0, 1.0, 1.0]),\n",
    "                dtype=np.float32\n",
    "            )\n",
    "        })\n",
    "        \n",
    "        # Episode parameters\n",
    "        self.max_steps = 1000\n",
    "        self.current_step = 0\n",
    "        \n",
    "        # Robot state (initialized in reset)\n",
    "        self.state = None\n",
    "        self.goal_position = np.array([5.0, 0.0, 0.0])  # 5m ahead\n",
    "    \n",
    "    def reset(self, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "        \n",
    "        # Reset step counter\n",
    "        self.current_step = 0\n",
    "        \n",
    "        # Reset reward function\n",
    "        self.reward_fn.reset()\n",
    "        \n",
    "        # Initialize robot state\n",
    "        self.state = RobotState(\n",
    "            base_orientation=np.zeros(3),\n",
    "            base_angular_velocity=np.zeros(3),\n",
    "            base_linear_velocity=np.zeros(3),\n",
    "            joint_positions=np.zeros(29),\n",
    "            joint_velocities=np.zeros(29),\n",
    "            foot_contacts=np.array([1, 1]),\n",
    "            goal_direction=self.goal_position / np.linalg.norm(self.goal_position),\n",
    "            goal_distance=np.linalg.norm(self.goal_position),\n",
    "            obstacle_proximity=np.ones(8) * 10.0,\n",
    "            cmd_velocity=np.zeros(3),\n",
    "            cmd_mode=1,\n",
    "            cmd_gesture=0\n",
    "        )\n",
    "        \n",
    "        # Read controller if available\n",
    "        if self.controller and self.controller.available:\n",
    "            commands = self.controller.read_commands()\n",
    "            self.state.cmd_velocity = commands['velocity']\n",
    "            self.state.cmd_mode = commands['mode']\n",
    "            self.state.cmd_gesture = commands['gesture']\n",
    "        \n",
    "        return self.state.to_array(), {}\n",
    "    \n",
    "    def step(self, action):\n",
    "        # Parse action\n",
    "        action_type = HighLevelAction(action['action_type'])\n",
    "        velocity_scale, direction_bias, audio_volume = action['parameters']\n",
    "        \n",
    "        action_cmd = ActionCommand(\n",
    "            action_type=action_type,\n",
    "            velocity_scale=float(velocity_scale),\n",
    "            direction_bias=float(direction_bias),\n",
    "            audio_volume=float(audio_volume)\n",
    "        )\n",
    "        \n",
    "        # Execute action (simulate robot dynamics)\n",
    "        next_state = self._simulate_step(action_cmd)\n",
    "        \n",
    "        # Read new controller commands\n",
    "        if self.controller and self.controller.available:\n",
    "            commands = self.controller.read_commands()\n",
    "            next_state.cmd_velocity = commands['velocity']\n",
    "            next_state.cmd_mode = commands['mode']\n",
    "            next_state.cmd_gesture = commands['gesture']\n",
    "        \n",
    "        # Compute reward\n",
    "        reward, reward_breakdown = self.reward_fn.compute_reward(\n",
    "            self.state, action_cmd, next_state\n",
    "        )\n",
    "        \n",
    "        # Check termination\n",
    "        self.current_step += 1\n",
    "        \n",
    "        goal_reached = next_state.goal_distance < self.reward_fn.goal_radius\n",
    "        fell = abs(next_state.base_orientation[0]) > self.reward_fn.fall_angle or \\\n",
    "               abs(next_state.base_orientation[1]) > self.reward_fn.fall_angle\n",
    "        \n",
    "        terminated = goal_reached or fell\n",
    "        truncated = self.current_step >= self.max_steps\n",
    "        \n",
    "        # Update state\n",
    "        self.state = next_state\n",
    "        \n",
    "        info = {\n",
    "            'goal_reached': goal_reached,\n",
    "            'fell': fell,\n",
    "            'goal_distance': next_state.goal_distance,\n",
    "            'reward_breakdown': reward_breakdown\n",
    "        }\n",
    "        \n",
    "        return next_state.to_array(), reward, terminated, truncated, info\n",
    "    \n",
    "    def _simulate_step(self, action: ActionCommand) -> RobotState:\n",
    "        \"\"\"\n",
    "        Simulate robot dynamics for one timestep\n",
    "        \n",
    "        In real deployment, this would be replaced with actual robot interface\n",
    "        \"\"\"\n",
    "        dt = 0.1  # 10 Hz control\n",
    "        \n",
    "        # Get motion command\n",
    "        motion_cmd = action.to_motion_command()\n",
    "        velocity = motion_cmd['velocity']\n",
    "        \n",
    "        # Simple kinematic update (placeholder physics)\n",
    "        current_pos = self.goal_position - self.state.goal_direction * self.state.goal_distance\n",
    "        \n",
    "        # Update position based on velocity\n",
    "        new_pos = current_pos + velocity[:2] * dt  # Only x, y (2D navigation)\n",
    "        \n",
    "        # Update goal distance\n",
    "        new_goal_vector = self.goal_position[:2] - new_pos\n",
    "        new_goal_distance = np.linalg.norm(new_goal_vector)\n",
    "        new_goal_direction = np.append(\n",
    "            new_goal_vector / (new_goal_distance + 1e-6), [0]\n",
    "        )  # Normalize + add z=0\n",
    "        \n",
    "        # Add small noise to orientation (simulate walking dynamics)\n",
    "        orientation_noise = np.random.normal(0, 0.02, size=3)\n",
    "        new_orientation = self.state.base_orientation + orientation_noise\n",
    "        \n",
    "        # Create next state\n",
    "        next_state = RobotState(\n",
    "            base_orientation=new_orientation,\n",
    "            base_angular_velocity=self.state.base_angular_velocity * 0.9,  # Damping\n",
    "            base_linear_velocity=velocity,\n",
    "            joint_positions=self.state.joint_positions,  # Simplified\n",
    "            joint_velocities=self.state.joint_velocities * 0.9,\n",
    "            foot_contacts=self.state.foot_contacts,\n",
    "            goal_direction=new_goal_direction,\n",
    "            goal_distance=new_goal_distance,\n",
    "            obstacle_proximity=self.state.obstacle_proximity,\n",
    "            cmd_velocity=self.state.cmd_velocity,\n",
    "            cmd_mode=self.state.cmd_mode,\n",
    "            cmd_gesture=self.state.cmd_gesture\n",
    "        )\n",
    "        \n",
    "        return next_state\n",
    "    \n",
    "    def render(self):\n",
    "        \"\"\"Optional rendering (not implemented)\"\"\"\n",
    "        pass\n",
    "    \n",
    "    def close(self):\n",
    "        \"\"\"Cleanup\"\"\"\n",
    "        if self.controller and PYGAME_AVAILABLE:\n",
    "            pygame.quit()\n",
    "\n",
    "# Test environment\n",
    "print(\"Creating high-level RL environment...\")\n",
    "env = G1HighLevelEnv(use_controller=False, simulation=True)\n",
    "\n",
    "print(f\"\\nObservation space: {env.observation_space}\")\n",
    "print(f\"Action space: {env.action_space}\")\n",
    "\n",
    "# Test episode\n",
    "obs, info = env.reset()\n",
    "print(f\"\\nInitial observation shape: {obs.shape}\")\n",
    "\n",
    "for i in range(5):\n",
    "    action = {\n",
    "        'action_type': 1,  # WALK_FORWARD\n",
    "        'parameters': np.array([0.8, 0.0, 0.5])\n",
    "    }\n",
    "    obs, reward, terminated, truncated, info = env.step(action)\n",
    "    print(f\"Step {i+1}: reward={reward:.4f}, goal_dist={info['goal_distance']:.2f}m\")\n",
    "    \n",
    "    if terminated or truncated:\n",
    "        break\n",
    "\n",
    "print(\"\\n‚úì Environment test complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='training'></a>\n",
    "## 7. Training with PPO\n",
    "\n",
    "Train the policy using Proximal Policy Optimization (PPO)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom wrapper to flatten Dict action space for SB3\n",
    "class FlattenedActionWrapper(gym.Wrapper):\n",
    "    \"\"\"Flatten Dict action space to Box for compatibility\"\"\"\n",
    "    \n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        \n",
    "        # Flattened action: [action_type (1), parameters (3)] = 4D\n",
    "        self.action_space = spaces.Box(\n",
    "            low=np.array([0.0, 0.0, -1.0, 0.0]),\n",
    "            high=np.array([9.0, 1.0, 1.0, 1.0]),\n",
    "            dtype=np.float32\n",
    "        )\n",
    "    \n",
    "    def step(self, action):\n",
    "        # Convert flattened action back to dict\n",
    "        action_type = int(np.clip(action[0], 0, 9))\n",
    "        parameters = action[1:]\n",
    "        \n",
    "        dict_action = {\n",
    "            'action_type': action_type,\n",
    "            'parameters': parameters\n",
    "        }\n",
    "        \n",
    "        return self.env.step(dict_action)\n",
    "\n",
    "# Training callback to log progress\n",
    "class TrainingCallback(BaseCallback):\n",
    "    def __init__(self, verbose=0):\n",
    "        super().__init__(verbose)\n",
    "        self.episode_rewards = []\n",
    "        self.episode_lengths = []\n",
    "        self.current_episode_reward = 0\n",
    "        self.current_episode_length = 0\n",
    "    \n",
    "    def _on_step(self) -> bool:\n",
    "        self.current_episode_reward += self.locals['rewards'][0]\n",
    "        self.current_episode_length += 1\n",
    "        \n",
    "        if self.locals['dones'][0]:\n",
    "            self.episode_rewards.append(self.current_episode_reward)\n",
    "            self.episode_lengths.append(self.current_episode_length)\n",
    "            \n",
    "            if len(self.episode_rewards) % 10 == 0:\n",
    "                avg_reward = np.mean(self.episode_rewards[-10:])\n",
    "                avg_length = np.mean(self.episode_lengths[-10:])\n",
    "                print(f\"Episode {len(self.episode_rewards)}: \"\n",
    "                      f\"avg_reward={avg_reward:.2f}, avg_length={avg_length:.1f}\")\n",
    "            \n",
    "            self.current_episode_reward = 0\n",
    "            self.current_episode_length = 0\n",
    "        \n",
    "        return True\n",
    "\n",
    "if SB3_AVAILABLE:\n",
    "    print(\"üéì Starting PPO training...\\n\")\n",
    "    \n",
    "    # Create environment with wrapper\n",
    "    train_env = FlattenedActionWrapper(G1HighLevelEnv(use_controller=False, simulation=True))\n",
    "    \n",
    "    # Create PPO model\n",
    "    model = PPO(\n",
    "        policy=\"MlpPolicy\",\n",
    "        env=train_env,\n",
    "        learning_rate=3e-4,\n",
    "        n_steps=2048,\n",
    "        batch_size=64,\n",
    "        n_epochs=10,\n",
    "        gamma=0.99,\n",
    "        gae_lambda=0.95,\n",
    "        clip_range=0.2,\n",
    "        verbose=1,\n",
    "        device='cpu'  # Change to 'cuda' if GPU available\n",
    "    )\n",
    "    \n",
    "    # Train\n",
    "    callback = TrainingCallback()\n",
    "    \n",
    "    print(\"Training for 50,000 timesteps (increase for better performance)...\\n\")\n",
    "    model.learn(\n",
    "        total_timesteps=50000,\n",
    "        callback=callback,\n",
    "        progress_bar=True\n",
    "    )\n",
    "    \n",
    "    # Save model\n",
    "    model.save(\"g1_high_level_ppo\")\n",
    "    print(\"\\n‚úì Model saved to g1_high_level_ppo.zip\")\n",
    "    \n",
    "    # Plot training progress\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Episode rewards\n",
    "    ax1.plot(callback.episode_rewards, alpha=0.3, label='Episode Reward')\n",
    "    window = 10\n",
    "    if len(callback.episode_rewards) > window:\n",
    "        moving_avg = np.convolve(\n",
    "            callback.episode_rewards, \n",
    "            np.ones(window)/window, \n",
    "            mode='valid'\n",
    "        )\n",
    "        ax1.plot(range(window-1, len(callback.episode_rewards)), moving_avg, \n",
    "                'r-', linewidth=2, label=f'{window}-Episode Moving Average')\n",
    "    ax1.set_xlabel('Episode')\n",
    "    ax1.set_ylabel('Total Reward')\n",
    "    ax1.set_title('Training Progress: Rewards')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Episode lengths\n",
    "    ax2.plot(callback.episode_lengths, alpha=0.3, label='Episode Length')\n",
    "    if len(callback.episode_lengths) > window:\n",
    "        moving_avg = np.convolve(\n",
    "            callback.episode_lengths, \n",
    "            np.ones(window)/window, \n",
    "            mode='valid'\n",
    "        )\n",
    "        ax2.plot(range(window-1, len(callback.episode_lengths)), moving_avg, \n",
    "                'g-', linewidth=2, label=f'{window}-Episode Moving Average')\n",
    "    ax2.set_xlabel('Episode')\n",
    "    ax2.set_ylabel('Steps')\n",
    "    ax2.set_title('Training Progress: Episode Length')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('training_progress.png', dpi=150)\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nüìä Training visualization saved to training_progress.png\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Stable-Baselines3 not available - skipping training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='deployment'></a>\n",
    "## 8. Deployment to Real Robot\n",
    "\n",
    "Deploy the trained policy to the physical Unitree G1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class G1RobotInterface:\n",
    "    \"\"\"Interface to real Unitree G1 hardware\"\"\"\n",
    "    \n",
    "    def __init__(self, network_interface: str = \"eth0\"):\n",
    "        self.network_interface = network_interface\n",
    "        self.initialized = False\n",
    "        \n",
    "        if UNITREE_SDK_AVAILABLE:\n",
    "            try:\n",
    "                # Initialize DDS\n",
    "                ChannelFactoryInitialize(0, network_interface)\n",
    "                \n",
    "                # Create clients\n",
    "                self.loco_client = LocoClient()\n",
    "                self.loco_client.SetTimeout(10.0)\n",
    "                self.loco_client.Init()\n",
    "                \n",
    "                self.audio_client = AudioClient()\n",
    "                self.audio_client.SetTimeout(10.0)\n",
    "                self.audio_client.Init()\n",
    "                \n",
    "                self.arm_client = G1ArmActionClient()\n",
    "                self.arm_client.SetTimeout(10.0)\n",
    "                self.arm_client.Init()\n",
    "                \n",
    "                # Subscribe to state\n",
    "                self.state_subscriber = ChannelSubscriber(\"rt/lowstate\", LowState_)\n",
    "                self.state_subscriber.Init(self._state_callback, 10)\n",
    "                \n",
    "                self.current_state = None\n",
    "                self.initialized = True\n",
    "                \n",
    "                print(f\"‚úì Connected to G1 on {network_interface}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è  Failed to connect to robot: {e}\")\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è  Unitree SDK not available\")\n",
    "    \n",
    "    def _state_callback(self, msg: LowState_):\n",
    "        \"\"\"Store latest robot state\"\"\"\n",
    "        self.current_state = msg\n",
    "    \n",
    "    def get_state(self) -> Optional[RobotState]:\n",
    "        \"\"\"\n",
    "        Read current robot state\n",
    "        \n",
    "        Returns:\n",
    "            state: RobotState object or None if unavailable\n",
    "        \"\"\"\n",
    "        if not self.initialized or self.current_state is None:\n",
    "            return None\n",
    "        \n",
    "        # Extract data from low-level state\n",
    "        imu = self.current_state.imu_state\n",
    "        motors = self.current_state.motor_state\n",
    "        \n",
    "        # Build RobotState\n",
    "        state = RobotState(\n",
    "            base_orientation=np.array(imu.rpy),\n",
    "            base_angular_velocity=np.array(imu.gyroscope),\n",
    "            base_linear_velocity=np.array([0, 0, 0]),  # Not directly available\n",
    "            joint_positions=np.array([m.q for m in motors]),\n",
    "            joint_velocities=np.array([m.dq for m in motors]),\n",
    "            foot_contacts=np.array([1, 1]),  # Placeholder (need force sensors)\n",
    "            goal_direction=np.array([1, 0, 0]),  # Set externally\n",
    "            goal_distance=5.0,  # Set externally\n",
    "            obstacle_proximity=np.ones(8) * 10.0,  # From depth camera\n",
    "            cmd_velocity=np.zeros(3),  # From controller\n",
    "            cmd_mode=1,\n",
    "            cmd_gesture=0\n",
    "        )\n",
    "        \n",
    "        return state\n",
    "    \n",
    "    def send_velocity(self, velocity: np.ndarray):\n",
    "        \"\"\"Send velocity command to robot\"\"\"\n",
    "        if not self.initialized:\n",
    "            return\n",
    "        \n",
    "        vx, vy, vw = velocity\n",
    "        self.loco_client.Move(float(vx), float(vy), float(vw))\n",
    "    \n",
    "    def execute_gesture(self, gesture_id: int):\n",
    "        \"\"\"Execute pre-defined gesture\"\"\"\n",
    "        if not self.initialized:\n",
    "            return\n",
    "        \n",
    "        gesture_map = {\n",
    "            0: \"high wave\",\n",
    "            1: \"shake hand\",\n",
    "            2: \"clap\"\n",
    "        }\n",
    "        \n",
    "        gesture_name = gesture_map.get(gesture_id)\n",
    "        if gesture_name and gesture_name in action_map:\n",
    "            self.arm_client.ExecuteAction(action_map[gesture_name])\n",
    "            time.sleep(0.1)\n",
    "            self.arm_client.ExecuteAction(action_map[\"release arm\"])\n",
    "    \n",
    "    def speak(self, message: str, volume: float = 0.5):\n",
    "        \"\"\"Text-to-speech output\"\"\"\n",
    "        if not self.initialized:\n",
    "            return\n",
    "        \n",
    "        # Set volume (0-100)\n",
    "        self.audio_client.SetVolume(int(volume * 100))\n",
    "        \n",
    "        # Chinese TTS (G1 default)\n",
    "        self.audio_client.TtsMaker(message, 0)\n",
    "    \n",
    "    def emergency_stop(self):\n",
    "        \"\"\"Emergency stop\"\"\"\n",
    "        if not self.initialized:\n",
    "            return\n",
    "        \n",
    "        self.loco_client.Damp()\n",
    "        print(\"‚ö†Ô∏è  EMERGENCY STOP ACTIVATED\")\n",
    "\n",
    "class DeployedPolicy:\n",
    "    \"\"\"Deployed RL policy running on real robot\"\"\"\n",
    "    \n",
    "    def __init__(self, model_path: str, network_interface: str = \"eth0\"):\n",
    "        # Load trained model\n",
    "        if SB3_AVAILABLE:\n",
    "            self.model = PPO.load(model_path)\n",
    "            print(f\"‚úì Loaded policy from {model_path}\")\n",
    "        else:\n",
    "            self.model = None\n",
    "            print(\"‚ö†Ô∏è  SB3 not available\")\n",
    "        \n",
    "        # Connect to robot\n",
    "        self.robot = G1RobotInterface(network_interface)\n",
    "        \n",
    "        # Controller for human input\n",
    "        self.controller = USBControllerInput()\n",
    "        \n",
    "        # Goal tracking\n",
    "        self.goal_position = np.array([5.0, 0.0, 0.0])\n",
    "    \n",
    "    def run(self, duration: float = 60.0):\n",
    "        \"\"\"\n",
    "        Run policy on real robot\n",
    "        \n",
    "        Args:\n",
    "            duration: How long to run (seconds)\n",
    "        \"\"\"\n",
    "        if not self.robot.initialized or self.model is None:\n",
    "            print(\"‚ö†Ô∏è  Cannot run: robot or model not initialized\")\n",
    "            return\n",
    "        \n",
    "        print(f\"ü§ñ Running policy for {duration:.0f} seconds...\")\n",
    "        print(\"   Press START button on controller for emergency stop\\n\")\n",
    "        \n",
    "        # Stand up\n",
    "        self.robot.loco_client.Squat2StandUp()\n",
    "        time.sleep(2)\n",
    "        \n",
    "        start_time = time.time()\n",
    "        step_count = 0\n",
    "        \n",
    "        while time.time() - start_time < duration:\n",
    "            # Get robot state\n",
    "            state = self.robot.get_state()\n",
    "            if state is None:\n",
    "                time.sleep(0.1)\n",
    "                continue\n",
    "            \n",
    "            # Get controller commands\n",
    "            if self.controller.available:\n",
    "                commands = self.controller.read_commands()\n",
    "                state.cmd_velocity = commands['velocity']\n",
    "                state.cmd_mode = commands['mode']\n",
    "                state.cmd_gesture = commands['gesture']\n",
    "                \n",
    "                # Check emergency stop\n",
    "                if commands['emergency_stop']:\n",
    "                    self.robot.emergency_stop()\n",
    "                    break\n",
    "            \n",
    "            # Update goal tracking (would come from navigation system)\n",
    "            current_pos = np.array([0, 0, 0])  # Placeholder\n",
    "            goal_vec = self.goal_position - current_pos\n",
    "            state.goal_distance = np.linalg.norm(goal_vec)\n",
    "            state.goal_direction = goal_vec / (state.goal_distance + 1e-6)\n",
    "            \n",
    "            # Get action from policy\n",
    "            obs = state.to_array()\n",
    "            action_flat, _ = self.model.predict(obs, deterministic=True)\n",
    "            \n",
    "            # Parse action\n",
    "            action_type = HighLevelAction(int(np.clip(action_flat[0], 0, 9)))\n",
    "            velocity_scale, direction_bias, audio_volume = action_flat[1:]\n",
    "            \n",
    "            action_cmd = ActionCommand(\n",
    "                action_type=action_type,\n",
    "                velocity_scale=float(velocity_scale),\n",
    "                direction_bias=float(direction_bias),\n",
    "                audio_volume=float(audio_volume)\n",
    "            )\n",
    "            \n",
    "            # Execute action\n",
    "            motion_cmd = action_cmd.to_motion_command()\n",
    "            \n",
    "            if motion_cmd['is_gesture']:\n",
    "                self.robot.execute_gesture(motion_cmd['gesture_id'])\n",
    "            else:\n",
    "                self.robot.send_velocity(motion_cmd['velocity'])\n",
    "            \n",
    "            # Log progress\n",
    "            if step_count % 10 == 0:\n",
    "                print(f\"\\rStep {step_count}: {action_cmd.get_description()}\", \n",
    "                      end='', flush=True)\n",
    "            \n",
    "            step_count += 1\n",
    "            time.sleep(0.1)  # 10 Hz control loop\n",
    "        \n",
    "        # Stop robot\n",
    "        self.robot.loco_client.Move(0, 0, 0)\n",
    "        print(\"\\n\\n‚úì Policy execution complete\")\n",
    "\n",
    "# Example deployment (requires real robot)\n",
    "print(\"\\nDeployment Example:\\n\")\n",
    "print(\"To deploy trained policy to real G1:\")\n",
    "print(\"\"\"\\n```python\n",
    "# Load and run policy\n",
    "policy = DeployedPolicy(\n",
    "    model_path='g1_high_level_ppo.zip',\n",
    "    network_interface='eth0'\n",
    ")\n",
    "\n",
    "# Run for 60 seconds\n",
    "policy.run(duration=60.0)\n",
    "```\"\"\")\n",
    "print(\"\\n‚ö†Ô∏è  Ensure robot is in safe environment before running!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='applications'></a>\n",
    "## 9. Example Applications\n",
    "\n",
    "### Application 1: Interactive Tour Guide\n",
    "\n",
    "Robot follows human operator and responds to gesture commands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TourGuideApplication:\n",
    "    \"\"\"\n",
    "    Tour guide robot that follows human and performs gestures\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, policy_path: str):\n",
    "        self.deployed_policy = DeployedPolicy(policy_path)\n",
    "        self.waypoints = [\n",
    "            (\"Entrance\", np.array([0, 0, 0])),\n",
    "            (\"Exhibit A\", np.array([5, 0, 0])),\n",
    "            (\"Exhibit B\", np.array([5, 5, 0])),\n",
    "            (\"Exit\", np.array([0, 5, 0]))\n",
    "        ]\n",
    "        self.current_waypoint = 0\n",
    "    \n",
    "    def start_tour(self):\n",
    "        \"\"\"\n",
    "        Execute guided tour with narration\n",
    "        \"\"\"\n",
    "        print(\"üé≠ Starting tour guide mode...\")\n",
    "        \n",
    "        for name, position in self.waypoints:\n",
    "            print(f\"\\nNavigating to: {name}\")\n",
    "            \n",
    "            # Update goal\n",
    "            self.deployed_policy.goal_position = position\n",
    "            \n",
    "            # Wave when arriving\n",
    "            self.deployed_policy.robot.execute_gesture(0)  # Wave\n",
    "            \n",
    "            # Narration (would be location-specific)\n",
    "            self.deployed_policy.robot.speak(f\"Welcome to {name}\")\n",
    "            \n",
    "            # Wait at location\n",
    "            time.sleep(5)\n",
    "        \n",
    "        print(\"\\n‚úì Tour complete!\")\n",
    "\n",
    "print(\"\\nüìù Example: Tour Guide Application\")\n",
    "print(\"   - Robot follows pre-defined waypoints\")\n",
    "print(\"   - Waves and provides audio narration at each location\")\n",
    "print(\"   - Responds to human controller for manual override\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Application 2: Warehouse Assistant\n",
    "\n",
    "Robot navigates warehouse and performs pick-and-place tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WarehouseAssistant:\n",
    "    \"\"\"\n",
    "    Warehouse robot for logistics tasks\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, policy_path: str):\n",
    "        self.deployed_policy = DeployedPolicy(policy_path)\n",
    "        self.inventory = {\n",
    "            'A1': 'Computer Monitor',\n",
    "            'A2': 'Keyboard',\n",
    "            'B1': 'Mouse',\n",
    "            'B2': 'Headphones'\n",
    "        }\n",
    "    \n",
    "    def pick_item(self, location: str) -> bool:\n",
    "        \"\"\"\n",
    "        Navigate to location and pick item\n",
    "        \n",
    "        Returns:\n",
    "            success: True if item retrieved\n",
    "        \"\"\"\n",
    "        item_name = self.inventory.get(location, \"Unknown\")\n",
    "        print(f\"\\nüì¶ Retrieving: {item_name} from {location}\")\n",
    "        \n",
    "        # Navigate to location (would use real coordinates)\n",
    "        # For demo, use placeholder\n",
    "        self.deployed_policy.goal_position = np.array([5, 0, 0])\n",
    "        \n",
    "        # Audio feedback\n",
    "        self.deployed_policy.robot.speak(f\"Retrieving {item_name}\")\n",
    "        \n",
    "        # Simulate pick (would use manipulation skills)\n",
    "        time.sleep(3)\n",
    "        \n",
    "        print(f\"‚úì Item retrieved: {item_name}\")\n",
    "        return True\n",
    "    \n",
    "    def deliver_item(self, destination: str) -> bool:\n",
    "        \"\"\"\n",
    "        Deliver held item to destination\n",
    "        \"\"\"\n",
    "        print(f\"\\nüöö Delivering to: {destination}\")\n",
    "        \n",
    "        # Navigate to destination\n",
    "        # ...\n",
    "        \n",
    "        # Place item\n",
    "        self.deployed_policy.robot.speak(\"Delivery complete\")\n",
    "        \n",
    "        return True\n",
    "\n",
    "print(\"\\nüìù Example: Warehouse Assistant\")\n",
    "print(\"   - Autonomous navigation to storage locations\")\n",
    "print(\"   - Audio confirmation of tasks\")\n",
    "print(\"   - Human operator can override via controller\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary & Next Steps\n",
    "\n",
    "This notebook demonstrated a **complete high-level RL framework** for the Unitree G1:\n",
    "\n",
    "‚úÖ **State Space:** Combined robot sensors + human controller input (86D)  \n",
    "‚úÖ **Action Space:** Abstract high-level actions (locomotion, gestures, audio)  \n",
    "‚úÖ **Motion Primitives:** Pre-validated movement sequences  \n",
    "‚úÖ **Reward Function:** Simple sparse rewards for goal reaching and command following  \n",
    "‚úÖ **Training:** PPO implementation with Stable-Baselines3  \n",
    "‚úÖ **Deployment:** Real robot interface with safety features  \n",
    "‚úÖ **Applications:** Tour guide and warehouse assistant examples\n",
    "\n",
    "### Key Advantages of High-Level RL\n",
    "\n",
    "1. **Faster Learning:** Smaller action space than low-level joint control\n",
    "2. **Better Generalization:** Abstractions transfer across tasks\n",
    "3. **Safer:** Primitives are pre-tested and bounded\n",
    "4. **Human-Interpretable:** Actions have semantic meaning\n",
    "5. **Controller Integration:** Human can guide learning and provide real-time input\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. **Expand Motion Library:** Add more primitives (stairs, doors, obstacles)\n",
    "2. **Vision Integration:** Add camera observations to state space\n",
    "3. **Multi-Task Learning:** Train single policy for multiple applications\n",
    "4. **Hierarchical RL:** Add meta-controller for task planning\n",
    "5. **Human Preference Learning:** Fine-tune policy using human feedback\n",
    "\n",
    "### References\n",
    "\n",
    "- **Hierarchical RL:** \"Options Framework\" (Sutton, Precup, Singh)\n",
    "- **Human-in-the-Loop:** \"Deep TAMER\" (Warnell et al.)\n",
    "- **PPO Algorithm:** \"Proximal Policy Optimization\" (Schulman et al.)\n",
    "- **Motion Primitives:** \"Dynamic Movement Primitives\" (Schaal et al.)\n",
    "\n",
    "---\n",
    "\n",
    "*High-Level RL Framework for Unitree G1*  \n",
    "*Enables rapid task learning with human guidance*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

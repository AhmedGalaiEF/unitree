{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# High-Level Reinforcement Learning for Unitree G1\n",
    "\n",
    "This notebook implements a **high-level RL framework** for the Unitree G1 humanoid robot, focusing on:\n",
    "\n",
    "1. **State Space:** Robot observations + USB controller commands\n",
    "2. **Action Space:** Abstract high-level actions (path planning, gestures, audio)\n",
    "3. **Reward Design:** Task-oriented rewards for real-world applications\n",
    "4. **Training Pipeline:** From simulation to real robot deployment\n",
    "\n",
    "## Key Concepts\n",
    "\n",
    "**High-Level RL** operates at the **task planning level**, abstracting away low-level motor control:\n",
    "\n",
    "```\n",
    "Traditional RL:        State ‚Üí Neural Network ‚Üí Joint Torques\n",
    "                       (Low-level, 29-DOF control)\n",
    "\n",
    "High-Level RL:         State ‚Üí Neural Network ‚Üí Abstract Actions\n",
    "                       (Task-level: \"walk to X\", \"wave hand\", \"say hello\")\n",
    "                       \n",
    "                       Abstract Actions ‚Üí Motion Primitives ‚Üí Joint Commands\n",
    "```\n",
    "\n",
    "**Benefits:**\n",
    "- Faster learning (smaller action space)\n",
    "- Better generalization\n",
    "- Safer (primitives are pre-validated)\n",
    "- Human-interpretable actions\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [State Space Design](#state-space)\n",
    "2. [USB Controller Integration](#controller)\n",
    "3. [High-Level Action Space](#actions)\n",
    "4. [Motion Primitives Library](#primitives)\n",
    "5. [Reward Function Design](#rewards)\n",
    "6. [Environment Implementation](#environment)\n",
    "7. [Training with PPO](#training)\n",
    "8. [Deployment to Real Robot](#deployment)\n",
    "9. [Example Applications](#applications)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup & Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install gymnasium stable-baselines3 torch numpy matplotlib\n",
    "!pip install pygame  # For USB controller support\n",
    "!pip install pynput  # Alternative: keyboard/mouse input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Dict, List, Tuple, Optional, Any\n",
    "from dataclasses import dataclass\n",
    "from enum import Enum\n",
    "import time\n",
    "import json\n",
    "\n",
    "# RL frameworks\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "try:\n",
    "    from stable_baselines3 import PPO\n",
    "    from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "    from stable_baselines3.common.callbacks import BaseCallback\n",
    "    SB3_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"Warning: stable-baselines3 not available\")\n",
    "    SB3_AVAILABLE = False\n",
    "\n",
    "# USB Controller\n",
    "try:\n",
    "    import pygame\n",
    "    PYGAME_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"Warning: pygame not available (USB controller input disabled)\")\n",
    "    PYGAME_AVAILABLE = False\n",
    "\n",
    "# Unitree SDK\n",
    "try:\n",
    "    from unitree_sdk2py.core.channel import ChannelSubscriber, ChannelPublisher, ChannelFactoryInitialize\n",
    "    from unitree_sdk2py.idl.unitree_hg.msg.dds_ import LowCmd_, LowState_\n",
    "    from unitree_sdk2py.g1.loco.g1_loco_client import LocoClient\n",
    "    from unitree_sdk2py.g1.audio.g1_audio_client import AudioClient\n",
    "    from unitree_sdk2py.g1.arm.g1_arm_action_client import G1ArmActionClient, action_map\n",
    "    UNITREE_SDK_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"Warning: Unitree SDK not available (simulation only)\")\n",
    "    UNITREE_SDK_AVAILABLE = False\n",
    "\n",
    "print(\"‚úì Imports complete\")\n",
    "print(f\"  - PyGame (Controller): {PYGAME_AVAILABLE}\")\n",
    "print(f\"  - Stable-Baselines3: {SB3_AVAILABLE}\")\n",
    "print(f\"  - Unitree SDK: {UNITREE_SDK_AVAILABLE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='state-space'></a>\n",
    "## 1. State Space Design\n",
    "\n",
    "The state space combines **robot observations** and **human commands** from a USB controller.\n",
    "\n",
    "### State Components\n",
    "\n",
    "#### A. Robot Observations (Proprioceptive)\n",
    "- **Base Orientation:** Roll, pitch, yaw (3D)\n",
    "- **Base Angular Velocity:** œâx, œây, œâz (3D)\n",
    "- **Base Linear Velocity:** vx, vy, vz (3D)\n",
    "- **Joint Positions:** 29 joint angles (29D)\n",
    "- **Joint Velocities:** 29 joint velocities (29D)\n",
    "- **Contact State:** Foot contact sensors (2D - left/right)\n",
    "\n",
    "#### B. Robot Observations (Exteroceptive)\n",
    "- **Goal Direction:** Relative vector to target (3D)\n",
    "- **Goal Distance:** Scalar distance to target (1D)\n",
    "- **Obstacle Proximity:** Nearest obstacle distance in 8 directions (8D)\n",
    "\n",
    "#### C. Human Commands (USB Controller)\n",
    "- **Velocity Command:** Desired (vx, vy, œâ) from joystick (3D)\n",
    "- **Mode Selector:** Discrete mode (walk/run/idle) (1D)\n",
    "- **Gesture Trigger:** Which gesture to perform (1D)\n",
    "\n",
    "**Total State Dimension:** 3 + 3 + 3 + 29 + 29 + 2 + 3 + 1 + 8 + 3 + 1 + 1 = **86 dimensions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class RobotState:\n",
    "    \"\"\"Complete state representation for high-level RL\"\"\"\n",
    "    \n",
    "    # Proprioceptive (internal sensors)\n",
    "    base_orientation: np.ndarray  # [roll, pitch, yaw] in radians\n",
    "    base_angular_velocity: np.ndarray  # [wx, wy, wz] in rad/s\n",
    "    base_linear_velocity: np.ndarray  # [vx, vy, vz] in m/s\n",
    "    joint_positions: np.ndarray  # 29 joint angles in radians\n",
    "    joint_velocities: np.ndarray  # 29 joint velocities in rad/s\n",
    "    foot_contacts: np.ndarray  # [left_contact, right_contact] (0 or 1)\n",
    "    \n",
    "    # Exteroceptive (external sensors)\n",
    "    goal_direction: np.ndarray  # Unit vector pointing to goal [x, y, z]\n",
    "    goal_distance: float  # Euclidean distance to goal in meters\n",
    "    obstacle_proximity: np.ndarray  # Distance to nearest obstacle in 8 directions\n",
    "    \n",
    "    # Human commands (from USB controller)\n",
    "    cmd_velocity: np.ndarray  # Desired [vx, vy, omega] from joystick\n",
    "    cmd_mode: int  # 0=idle, 1=walk, 2=run\n",
    "    cmd_gesture: int  # Gesture ID (0=none, 1=wave, 2=handshake, etc.)\n",
    "    \n",
    "    def to_array(self) -> np.ndarray:\n",
    "        \"\"\"Convert state to flat numpy array for RL\"\"\"\n",
    "        return np.concatenate([\n",
    "            self.base_orientation,\n",
    "            self.base_angular_velocity,\n",
    "            self.base_linear_velocity,\n",
    "            self.joint_positions,\n",
    "            self.joint_velocities,\n",
    "            self.foot_contacts,\n",
    "            self.goal_direction,\n",
    "            [self.goal_distance],\n",
    "            self.obstacle_proximity,\n",
    "            self.cmd_velocity,\n",
    "            [self.cmd_mode],\n",
    "            [self.cmd_gesture]\n",
    "        ])\n",
    "    \n",
    "    @property\n",
    "    def dimension(self) -> int:\n",
    "        \"\"\"Total state dimension\"\"\"\n",
    "        return len(self.to_array())\n",
    "\n",
    "# Example state initialization\n",
    "example_state = RobotState(\n",
    "    base_orientation=np.zeros(3),\n",
    "    base_angular_velocity=np.zeros(3),\n",
    "    base_linear_velocity=np.zeros(3),\n",
    "    joint_positions=np.zeros(29),\n",
    "    joint_velocities=np.zeros(29),\n",
    "    foot_contacts=np.array([1, 1]),  # Both feet on ground\n",
    "    goal_direction=np.array([1, 0, 0]),  # Goal straight ahead\n",
    "    goal_distance=5.0,  # 5 meters away\n",
    "    obstacle_proximity=np.ones(8) * 10.0,  # No nearby obstacles\n",
    "    cmd_velocity=np.zeros(3),\n",
    "    cmd_mode=1,  # Walk mode\n",
    "    cmd_gesture=0  # No gesture\n",
    ")\n",
    "\n",
    "print(f\"State dimension: {example_state.dimension}\")\n",
    "print(f\"State vector shape: {example_state.to_array().shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='controller'></a>\n",
    "## 2. USB Controller Integration\n",
    "\n",
    "We'll use a standard USB game controller (e.g., Xbox, PlayStation) to provide human commands.\n",
    "\n",
    "### Controller Mapping\n",
    "\n",
    "```\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ        USB Game Controller              ‚îÇ\n",
    "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
    "‚îÇ                                         ‚îÇ\n",
    "‚îÇ  Left Stick:  ‚Üí Forward/Backward (vx)  ‚îÇ\n",
    "‚îÇ               ‚Üí Strafe Left/Right (vy)  ‚îÇ\n",
    "‚îÇ                                         ‚îÇ\n",
    "‚îÇ  Right Stick: ‚Üí Rotate Left/Right (œâ)  ‚îÇ\n",
    "‚îÇ                                         ‚îÇ\n",
    "‚îÇ  Buttons:                               ‚îÇ\n",
    "‚îÇ    A/X     ‚Üí Walk Mode                  ‚îÇ\n",
    "‚îÇ    B/Circle ‚Üí Run Mode                   ‚îÇ\n",
    "‚îÇ    Y/Triangle ‚Üí Idle/Stop                ‚îÇ\n",
    "‚îÇ    LB      ‚Üí Wave Hand                  ‚îÇ\n",
    "‚îÇ    RB      ‚Üí Handshake                  ‚îÇ\n",
    "‚îÇ    LT      ‚Üí Clap                       ‚îÇ\n",
    "‚îÇ    Start   ‚Üí Emergency Stop             ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class USBControllerInput:\n",
    "    \"\"\"Read commands from USB game controller (Xbox/PlayStation compatible)\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.available = False\n",
    "        self.joystick = None\n",
    "        \n",
    "        if PYGAME_AVAILABLE:\n",
    "            try:\n",
    "                pygame.init()\n",
    "                pygame.joystick.init()\n",
    "                \n",
    "                if pygame.joystick.get_count() > 0:\n",
    "                    self.joystick = pygame.joystick.Joystick(0)\n",
    "                    self.joystick.init()\n",
    "                    self.available = True\n",
    "                    print(f\"‚úì Controller connected: {self.joystick.get_name()}\")\n",
    "                    print(f\"  Axes: {self.joystick.get_numaxes()}\")\n",
    "                    print(f\"  Buttons: {self.joystick.get_numbuttons()}\")\n",
    "                else:\n",
    "                    print(\"‚ö†Ô∏è  No USB controller detected\")\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è  Controller initialization failed: {e}\")\n",
    "        \n",
    "        # Default command values\n",
    "        self.cmd_velocity = np.zeros(3)\n",
    "        self.cmd_mode = 0\n",
    "        self.cmd_gesture = 0\n",
    "        self.emergency_stop = False\n",
    "    \n",
    "    def read_commands(self) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Read current controller state\n",
    "        \n",
    "        Returns:\n",
    "            commands: Dictionary with velocity, mode, gesture commands\n",
    "        \"\"\"\n",
    "        if not self.available:\n",
    "            # Return default commands if no controller\n",
    "            return {\n",
    "                'velocity': self.cmd_velocity,\n",
    "                'mode': self.cmd_mode,\n",
    "                'gesture': self.cmd_gesture,\n",
    "                'emergency_stop': False\n",
    "            }\n",
    "        \n",
    "        # Process pygame events\n",
    "        pygame.event.pump()\n",
    "        \n",
    "        # Read axes (joysticks)\n",
    "        # Left stick: axis 0 (left/right), axis 1 (up/down)\n",
    "        # Right stick: axis 3 (left/right)\n",
    "        left_x = self.joystick.get_axis(0) if self.joystick.get_numaxes() > 0 else 0.0\n",
    "        left_y = -self.joystick.get_axis(1) if self.joystick.get_numaxes() > 1 else 0.0  # Invert Y\n",
    "        right_x = self.joystick.get_axis(3) if self.joystick.get_numaxes() > 3 else 0.0\n",
    "        \n",
    "        # Apply deadzone (ignore small movements)\n",
    "        deadzone = 0.15\n",
    "        left_x = left_x if abs(left_x) > deadzone else 0.0\n",
    "        left_y = left_y if abs(left_y) > deadzone else 0.0\n",
    "        right_x = right_x if abs(right_x) > deadzone else 0.0\n",
    "        \n",
    "        # Map to velocity commands\n",
    "        # Scale: joystick [-1, 1] ‚Üí velocity [max_speed]\n",
    "        max_linear = 0.5  # m/s\n",
    "        max_angular = 0.8  # rad/s\n",
    "        \n",
    "        self.cmd_velocity = np.array([\n",
    "            left_y * max_linear,   # Forward/backward\n",
    "            left_x * max_linear,   # Strafe left/right\n",
    "            right_x * max_angular  # Rotate\n",
    "        ])\n",
    "        \n",
    "        # Read buttons\n",
    "        button_a = self.joystick.get_button(0) if self.joystick.get_numbuttons() > 0 else 0\n",
    "        button_b = self.joystick.get_button(1) if self.joystick.get_numbuttons() > 1 else 0\n",
    "        button_y = self.joystick.get_button(3) if self.joystick.get_numbuttons() > 3 else 0\n",
    "        button_lb = self.joystick.get_button(4) if self.joystick.get_numbuttons() > 4 else 0\n",
    "        button_rb = self.joystick.get_button(5) if self.joystick.get_numbuttons() > 5 else 0\n",
    "        button_lt = self.joystick.get_button(6) if self.joystick.get_numbuttons() > 6 else 0\n",
    "        button_start = self.joystick.get_button(7) if self.joystick.get_numbuttons() > 7 else 0\n",
    "        \n",
    "        # Mode selection\n",
    "        if button_a:\n",
    "            self.cmd_mode = 1  # Walk\n",
    "        elif button_b:\n",
    "            self.cmd_mode = 2  # Run\n",
    "        elif button_y:\n",
    "            self.cmd_mode = 0  # Idle\n",
    "        \n",
    "        # Gesture selection\n",
    "        if button_lb:\n",
    "            self.cmd_gesture = 1  # Wave\n",
    "        elif button_rb:\n",
    "            self.cmd_gesture = 2  # Handshake\n",
    "        elif button_lt:\n",
    "            self.cmd_gesture = 3  # Clap\n",
    "        else:\n",
    "            self.cmd_gesture = 0  # None\n",
    "        \n",
    "        # Emergency stop\n",
    "        self.emergency_stop = bool(button_start)\n",
    "        \n",
    "        return {\n",
    "            'velocity': self.cmd_velocity.copy(),\n",
    "            'mode': self.cmd_mode,\n",
    "            'gesture': self.cmd_gesture,\n",
    "            'emergency_stop': self.emergency_stop\n",
    "        }\n",
    "    \n",
    "    def get_command_string(self) -> str:\n",
    "        \"\"\"Human-readable command description\"\"\"\n",
    "        mode_names = ['IDLE', 'WALK', 'RUN']\n",
    "        gesture_names = ['None', 'Wave', 'Handshake', 'Clap']\n",
    "        \n",
    "        return (\n",
    "            f\"Mode: {mode_names[self.cmd_mode]} | \"\n",
    "            f\"Vel: [{self.cmd_velocity[0]:.2f}, {self.cmd_velocity[1]:.2f}, {self.cmd_velocity[2]:.2f}] | \"\n",
    "            f\"Gesture: {gesture_names[self.cmd_gesture]}\"\n",
    "        )\n",
    "\n",
    "# Test controller\n",
    "controller = USBControllerInput()\n",
    "\n",
    "if controller.available:\n",
    "    print(\"\\nTesting controller (move sticks and press buttons)...\")\n",
    "    for i in range(50):  # Read for 5 seconds\n",
    "        commands = controller.read_commands()\n",
    "        print(f\"\\r{controller.get_command_string()}\", end='', flush=True)\n",
    "        time.sleep(0.1)\n",
    "    print(\"\\n‚úì Controller test complete\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  Controller not available - will use simulated commands\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='actions'></a>\n",
    "## 3. High-Level Action Space\n",
    "\n",
    "Instead of directly controlling 29 joints, we define **high-level abstract actions**:\n",
    "\n",
    "### Action Space Design\n",
    "\n",
    "#### Discrete Actions (10 total)\n",
    "\n",
    "| Action ID | Name | Description |\n",
    "|-----------|------|-------------|\n",
    "| 0 | `IDLE` | Stand still, damping mode |\n",
    "| 1 | `WALK_FORWARD` | Walk forward at commanded velocity |\n",
    "| 2 | `WALK_BACKWARD` | Walk backward |\n",
    "| 3 | `STRAFE_LEFT` | Sidestep left |\n",
    "| 4 | `STRAFE_RIGHT` | Sidestep right |\n",
    "| 5 | `ROTATE_LEFT` | Turn left in place |\n",
    "| 6 | `ROTATE_RIGHT` | Turn right in place |\n",
    "| 7 | `GESTURE_WAVE` | Perform wave gesture |\n",
    "| 8 | `GESTURE_HANDSHAKE` | Perform handshake gesture |\n",
    "| 9 | `GESTURE_CLAP` | Perform clap gesture |\n",
    "\n",
    "#### Continuous Parameters (3D)\n",
    "\n",
    "- **Velocity Scale:** [0, 1] - How fast to execute action\n",
    "- **Direction Bias:** [-1, 1] - Fine-tune direction\n",
    "- **Audio Volume:** [0, 1] - Volume for audio feedback\n",
    "\n",
    "**Total Action Space:** Discrete(10) + Box(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HighLevelAction(Enum):\n",
    "    \"\"\"Enumeration of high-level actions\"\"\"\n",
    "    IDLE = 0\n",
    "    WALK_FORWARD = 1\n",
    "    WALK_BACKWARD = 2\n",
    "    STRAFE_LEFT = 3\n",
    "    STRAFE_RIGHT = 4\n",
    "    ROTATE_LEFT = 5\n",
    "    ROTATE_RIGHT = 6\n",
    "    GESTURE_WAVE = 7\n",
    "    GESTURE_HANDSHAKE = 8\n",
    "    GESTURE_CLAP = 9\n",
    "\n",
    "@dataclass\n",
    "class ActionCommand:\n",
    "    \"\"\"Complete action specification\"\"\"\n",
    "    action_type: HighLevelAction\n",
    "    velocity_scale: float  # [0, 1]\n",
    "    direction_bias: float  # [-1, 1]\n",
    "    audio_volume: float  # [0, 1]\n",
    "    \n",
    "    def to_motion_command(self) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Convert high-level action to low-level motion command\n",
    "        \n",
    "        Returns:\n",
    "            motion_cmd: Dictionary with target velocities and gesture flags\n",
    "        \"\"\"\n",
    "        # Base velocities for each action type\n",
    "        action_velocities = {\n",
    "            HighLevelAction.IDLE: (0.0, 0.0, 0.0),\n",
    "            HighLevelAction.WALK_FORWARD: (0.3, 0.0, 0.0),\n",
    "            HighLevelAction.WALK_BACKWARD: (-0.2, 0.0, 0.0),\n",
    "            HighLevelAction.STRAFE_LEFT: (0.0, 0.2, 0.0),\n",
    "            HighLevelAction.STRAFE_RIGHT: (0.0, -0.2, 0.0),\n",
    "            HighLevelAction.ROTATE_LEFT: (0.0, 0.0, 0.5),\n",
    "            HighLevelAction.ROTATE_RIGHT: (0.0, 0.0, -0.5),\n",
    "            HighLevelAction.GESTURE_WAVE: (0.0, 0.0, 0.0),\n",
    "            HighLevelAction.GESTURE_HANDSHAKE: (0.0, 0.0, 0.0),\n",
    "            HighLevelAction.GESTURE_CLAP: (0.0, 0.0, 0.0),\n",
    "        }\n",
    "        \n",
    "        vx, vy, vw = action_velocities[self.action_type]\n",
    "        \n",
    "        # Apply velocity scaling\n",
    "        vx *= self.velocity_scale\n",
    "        vy *= self.velocity_scale\n",
    "        vw *= self.velocity_scale\n",
    "        \n",
    "        # Apply direction bias (adds lateral component)\n",
    "        vy += self.direction_bias * 0.1\n",
    "        \n",
    "        # Determine if gesture should be executed\n",
    "        is_gesture = self.action_type.value >= 7\n",
    "        gesture_id = self.action_type.value - 7 if is_gesture else None\n",
    "        \n",
    "        return {\n",
    "            'velocity': np.array([vx, vy, vw]),\n",
    "            'is_gesture': is_gesture,\n",
    "            'gesture_id': gesture_id,\n",
    "            'audio_volume': self.audio_volume\n",
    "        }\n",
    "    \n",
    "    def get_description(self) -> str:\n",
    "        \"\"\"Human-readable action description\"\"\"\n",
    "        return (\n",
    "            f\"{self.action_type.name} \"\n",
    "            f\"(speed={self.velocity_scale:.2f}, \"\n",
    "            f\"bias={self.direction_bias:.2f}, \"\n",
    "            f\"volume={self.audio_volume:.2f})\"\n",
    "        )\n",
    "\n",
    "# Example actions\n",
    "example_actions = [\n",
    "    ActionCommand(HighLevelAction.WALK_FORWARD, 0.8, 0.0, 0.5),\n",
    "    ActionCommand(HighLevelAction.ROTATE_LEFT, 0.5, 0.0, 0.3),\n",
    "    ActionCommand(HighLevelAction.GESTURE_WAVE, 1.0, 0.0, 0.8),\n",
    "]\n",
    "\n",
    "print(\"Example high-level actions:\\n\")\n",
    "for action in example_actions:\n",
    "    print(f\"  {action.get_description()}\")\n",
    "    motion_cmd = action.to_motion_command()\n",
    "    print(f\"    ‚Üí Velocity: {motion_cmd['velocity']}\")\n",
    "    print(f\"    ‚Üí Gesture: {motion_cmd['is_gesture']}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='primitives'></a>\n",
    "## 4. Motion Primitives Library\n",
    "\n",
    "Motion primitives are **pre-validated movement sequences** that abstract low-level control."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MotionPrimitive:\n",
    "    \"\"\"Base class for motion primitives\"\"\"\n",
    "    \n",
    "    def __init__(self, name: str, duration: float):\n",
    "        self.name = name\n",
    "        self.duration = duration  # seconds\n",
    "        self.is_executing = False\n",
    "        self.start_time = None\n",
    "    \n",
    "    def execute(self, robot_interface) -> bool:\n",
    "        \"\"\"\n",
    "        Execute the motion primitive\n",
    "        \n",
    "        Args:\n",
    "            robot_interface: Connection to real or simulated robot\n",
    "        \n",
    "        Returns:\n",
    "            completed: True if primitive finished\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"Reset primitive state\"\"\"\n",
    "        self.is_executing = False\n",
    "        self.start_time = None\n",
    "\n",
    "class LocomotionPrimitive(MotionPrimitive):\n",
    "    \"\"\"Locomotion primitive: walk with specified velocity\"\"\"\n",
    "    \n",
    "    def __init__(self, name: str, target_velocity: np.ndarray, duration: float = 1.0):\n",
    "        super().__init__(name, duration)\n",
    "        self.target_velocity = target_velocity  # [vx, vy, vw]\n",
    "    \n",
    "    def execute(self, robot_interface) -> bool:\n",
    "        if not self.is_executing:\n",
    "            self.is_executing = True\n",
    "            self.start_time = time.time()\n",
    "        \n",
    "        elapsed = time.time() - self.start_time\n",
    "        \n",
    "        if elapsed < self.duration:\n",
    "            # Send velocity command\n",
    "            if hasattr(robot_interface, 'send_velocity'):\n",
    "                robot_interface.send_velocity(self.target_velocity)\n",
    "            return False  # Still executing\n",
    "        else:\n",
    "            # Completed\n",
    "            self.reset()\n",
    "            return True\n",
    "\n",
    "class GesturePrimitive(MotionPrimitive):\n",
    "    \"\"\"Gesture primitive: pre-defined arm motion\"\"\"\n",
    "    \n",
    "    def __init__(self, name: str, gesture_id: int, duration: float = 3.0):\n",
    "        super().__init__(name, duration)\n",
    "        self.gesture_id = gesture_id\n",
    "    \n",
    "    def execute(self, robot_interface) -> bool:\n",
    "        if not self.is_executing:\n",
    "            self.is_executing = True\n",
    "            self.start_time = time.time()\n",
    "            \n",
    "            # Trigger gesture\n",
    "            if hasattr(robot_interface, 'execute_gesture'):\n",
    "                robot_interface.execute_gesture(self.gesture_id)\n",
    "        \n",
    "        elapsed = time.time() - self.start_time\n",
    "        \n",
    "        if elapsed >= self.duration:\n",
    "            self.reset()\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "class AudioPrimitive(MotionPrimitive):\n",
    "    \"\"\"Audio primitive: play sound or TTS\"\"\"\n",
    "    \n",
    "    def __init__(self, name: str, message: str, volume: float = 0.5):\n",
    "        super().__init__(name, duration=2.0)\n",
    "        self.message = message\n",
    "        self.volume = volume\n",
    "    \n",
    "    def execute(self, robot_interface) -> bool:\n",
    "        if not self.is_executing:\n",
    "            self.is_executing = True\n",
    "            self.start_time = time.time()\n",
    "            \n",
    "            # Play audio\n",
    "            if hasattr(robot_interface, 'speak'):\n",
    "                robot_interface.speak(self.message, self.volume)\n",
    "        \n",
    "        elapsed = time.time() - self.start_time\n",
    "        \n",
    "        if elapsed >= self.duration:\n",
    "            self.reset()\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "class MotionPrimitiveLibrary:\n",
    "    \"\"\"Library of pre-defined motion primitives\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.primitives = {}\n",
    "        self._initialize_primitives()\n",
    "    \n",
    "    def _initialize_primitives(self):\n",
    "        \"\"\"Create standard motion primitives\"\"\"\n",
    "        # Locomotion primitives\n",
    "        self.primitives['walk_forward'] = LocomotionPrimitive(\n",
    "            'walk_forward', np.array([0.3, 0, 0]), duration=1.0\n",
    "        )\n",
    "        self.primitives['walk_backward'] = LocomotionPrimitive(\n",
    "            'walk_backward', np.array([-0.2, 0, 0]), duration=1.0\n",
    "        )\n",
    "        self.primitives['strafe_left'] = LocomotionPrimitive(\n",
    "            'strafe_left', np.array([0, 0.2, 0]), duration=1.0\n",
    "        )\n",
    "        self.primitives['strafe_right'] = LocomotionPrimitive(\n",
    "            'strafe_right', np.array([0, -0.2, 0]), duration=1.0\n",
    "        )\n",
    "        self.primitives['rotate_left'] = LocomotionPrimitive(\n",
    "            'rotate_left', np.array([0, 0, 0.5]), duration=1.0\n",
    "        )\n",
    "        self.primitives['rotate_right'] = LocomotionPrimitive(\n",
    "            'rotate_right', np.array([0, 0, -0.5]), duration=1.0\n",
    "        )\n",
    "        \n",
    "        # Gesture primitives\n",
    "        self.primitives['wave'] = GesturePrimitive('wave', gesture_id=0, duration=3.0)\n",
    "        self.primitives['handshake'] = GesturePrimitive('handshake', gesture_id=1, duration=4.0)\n",
    "        self.primitives['clap'] = GesturePrimitive('clap', gesture_id=2, duration=3.0)\n",
    "        \n",
    "        # Audio primitives\n",
    "        self.primitives['greet'] = AudioPrimitive('greet', \"Hello, nice to meet you!\")\n",
    "        self.primitives['acknowledge'] = AudioPrimitive('acknowledge', \"Understood\")\n",
    "    \n",
    "    def get_primitive(self, name: str) -> Optional[MotionPrimitive]:\n",
    "        \"\"\"Retrieve primitive by name\"\"\"\n",
    "        return self.primitives.get(name)\n",
    "    \n",
    "    def list_primitives(self) -> List[str]:\n",
    "        \"\"\"List all available primitives\"\"\"\n",
    "        return list(self.primitives.keys())\n",
    "\n",
    "# Initialize library\n",
    "motion_lib = MotionPrimitiveLibrary()\n",
    "print(\"Motion Primitive Library initialized\\n\")\n",
    "print(\"Available primitives:\")\n",
    "for name in motion_lib.list_primitives():\n",
    "    primitive = motion_lib.get_primitive(name)\n",
    "    print(f\"  - {name:20s} (duration: {primitive.duration:.1f}s)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='rewards'></a>\n",
    "## 5. Reward Function Design\n",
    "\n",
    "A well-designed reward function is critical for RL success. We'll use a **simple, sparse reward** approach.\n",
    "\n",
    "### Reward Components\n",
    "\n",
    "#### 1. Goal Reaching Reward (+100)\n",
    "- Large positive reward when robot reaches goal\n",
    "- Encourages task completion\n",
    "\n",
    "#### 2. Progress Reward (+0.1 per step)\n",
    "- Small reward for moving closer to goal\n",
    "- Provides learning signal before goal is reached\n",
    "\n",
    "#### 3. Command Following Reward (+0.5)\n",
    "- Reward for following human controller commands\n",
    "- Encourages responsive behavior\n",
    "\n",
    "#### 4. Stability Penalty (-1.0 per fall)\n",
    "- Negative reward if robot falls (large tilt)\n",
    "- Ensures safety\n",
    "\n",
    "#### 5. Collision Penalty (-0.5)\n",
    "- Penalty for hitting obstacles\n",
    "- Encourages obstacle avoidance\n",
    "\n",
    "#### 6. Energy Penalty (-0.001 per action)\n",
    "- Small penalty for movement\n",
    "- Encourages efficiency\n",
    "\n",
    "**Total Reward:**\n",
    "```\n",
    "r = r_goal + r_progress + r_command + r_stability + r_collision + r_energy\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RewardFunction:\n",
    "    \"\"\"Compute reward for high-level RL\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Reward weights\n",
    "        self.w_goal = 100.0\n",
    "        self.w_progress = 0.1\n",
    "        self.w_command = 0.5\n",
    "        self.w_stability = -1.0\n",
    "        self.w_collision = -0.5\n",
    "        self.w_energy = -0.001\n",
    "        \n",
    "        # Thresholds\n",
    "        self.goal_radius = 0.5  # meters\n",
    "        self.fall_angle = 0.5  # radians (~30 degrees)\n",
    "        self.collision_distance = 0.3  # meters\n",
    "        \n",
    "        # Tracking\n",
    "        self.previous_goal_distance = None\n",
    "    \n",
    "    def compute_reward(\n",
    "        self,\n",
    "        state: RobotState,\n",
    "        action: ActionCommand,\n",
    "        next_state: RobotState\n",
    "    ) -> Tuple[float, Dict[str, float]]:\n",
    "        \"\"\"\n",
    "        Compute reward for state-action-next_state transition\n",
    "        \n",
    "        Returns:\n",
    "            total_reward: Scalar reward\n",
    "            reward_breakdown: Dictionary with individual components\n",
    "        \"\"\"\n",
    "        rewards = {}\n",
    "        \n",
    "        # 1. Goal reaching\n",
    "        reached_goal = next_state.goal_distance < self.goal_radius\n",
    "        rewards['goal'] = self.w_goal if reached_goal else 0.0\n",
    "        \n",
    "        # 2. Progress toward goal\n",
    "        if self.previous_goal_distance is not None:\n",
    "            progress = self.previous_goal_distance - next_state.goal_distance\n",
    "            rewards['progress'] = self.w_progress * progress\n",
    "        else:\n",
    "            rewards['progress'] = 0.0\n",
    "        \n",
    "        self.previous_goal_distance = next_state.goal_distance\n",
    "        \n",
    "        # 3. Command following (alignment with human commands)\n",
    "        # Measure how well action aligns with commanded velocity\n",
    "        motion_cmd = action.to_motion_command()\n",
    "        commanded_vel = state.cmd_velocity\n",
    "        actual_vel = motion_cmd['velocity']\n",
    "        \n",
    "        # Cosine similarity (if non-zero commands)\n",
    "        if np.linalg.norm(commanded_vel) > 0.01:\n",
    "            alignment = np.dot(commanded_vel, actual_vel) / (\n",
    "                np.linalg.norm(commanded_vel) * (np.linalg.norm(actual_vel) + 1e-6)\n",
    "            )\n",
    "            rewards['command'] = self.w_command * max(0, alignment)\n",
    "        else:\n",
    "            rewards['command'] = 0.0\n",
    "        \n",
    "        # 4. Stability penalty\n",
    "        roll, pitch, _ = next_state.base_orientation\n",
    "        fell = abs(roll) > self.fall_angle or abs(pitch) > self.fall_angle\n",
    "        rewards['stability'] = self.w_stability if fell else 0.0\n",
    "        \n",
    "        # 5. Collision penalty\n",
    "        min_obstacle_dist = np.min(next_state.obstacle_proximity)\n",
    "        collided = min_obstacle_dist < self.collision_distance\n",
    "        rewards['collision'] = self.w_collision if collided else 0.0\n",
    "        \n",
    "        # 6. Energy penalty (encourage efficiency)\n",
    "        action_magnitude = np.linalg.norm(motion_cmd['velocity'])\n",
    "        rewards['energy'] = self.w_energy * action_magnitude\n",
    "        \n",
    "        # Total reward\n",
    "        total_reward = sum(rewards.values())\n",
    "        \n",
    "        return total_reward, rewards\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"Reset tracking variables\"\"\"\n",
    "        self.previous_goal_distance = None\n",
    "\n",
    "# Example reward computation\n",
    "reward_fn = RewardFunction()\n",
    "\n",
    "# Simulate scenario: robot moving toward goal\n",
    "state = RobotState(\n",
    "    base_orientation=np.array([0.05, 0.02, 0.0]),\n",
    "    base_angular_velocity=np.zeros(3),\n",
    "    base_linear_velocity=np.array([0.3, 0, 0]),\n",
    "    joint_positions=np.zeros(29),\n",
    "    joint_velocities=np.zeros(29),\n",
    "    foot_contacts=np.array([1, 1]),\n",
    "    goal_direction=np.array([1, 0, 0]),\n",
    "    goal_distance=5.0,\n",
    "    obstacle_proximity=np.ones(8) * 10.0,\n",
    "    cmd_velocity=np.array([0.3, 0, 0]),\n",
    "    cmd_mode=1,\n",
    "    cmd_gesture=0\n",
    ")\n",
    "\n",
    "action = ActionCommand(HighLevelAction.WALK_FORWARD, 0.8, 0.0, 0.5)\n",
    "\n",
    "next_state = RobotState(\n",
    "    base_orientation=np.array([0.03, 0.01, 0.0]),\n",
    "    base_angular_velocity=np.zeros(3),\n",
    "    base_linear_velocity=np.array([0.3, 0, 0]),\n",
    "    joint_positions=np.zeros(29),\n",
    "    joint_velocities=np.zeros(29),\n",
    "    foot_contacts=np.array([1, 1]),\n",
    "    goal_direction=np.array([1, 0, 0]),\n",
    "    goal_distance=4.7,  # Moved 0.3m closer\n",
    "    obstacle_proximity=np.ones(8) * 10.0,\n",
    "    cmd_velocity=np.array([0.3, 0, 0]),\n",
    "    cmd_mode=1,\n",
    "    cmd_gesture=0\n",
    ")\n",
    "\n",
    "reward, breakdown = reward_fn.compute_reward(state, action, next_state)\n",
    "\n",
    "print(\"Reward Computation Example:\\n\")\n",
    "print(f\"Action: {action.get_description()}\")\n",
    "print(f\"\\nReward Breakdown:\")\n",
    "for component, value in breakdown.items():\n",
    "    print(f\"  {component:15s}: {value:+7.4f}\")\n",
    "print(f\"\\nTotal Reward: {reward:+7.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='environment'></a>\n",
    "## 6. Gymnasium Environment Implementation\n",
    "\n",
    "We'll implement a Gymnasium environment that wraps everything together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class G1HighLevelEnv(gym.Env):\n",
    "    \"\"\"\n",
    "    High-level RL environment for Unitree G1\n",
    "    \n",
    "    State: 86D (robot obs + controller commands)\n",
    "    Action: Discrete(10) + Box(3) (high-level actions + parameters)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, use_controller: bool = True, simulation: bool = True):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.use_controller = use_controller\n",
    "        self.simulation = simulation\n",
    "        \n",
    "        # Initialize components\n",
    "        self.controller = USBControllerInput() if use_controller else None\n",
    "        self.motion_lib = MotionPrimitiveLibrary()\n",
    "        self.reward_fn = RewardFunction()\n",
    "        \n",
    "        # Define spaces\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=-np.inf,\n",
    "            high=np.inf,\n",
    "            shape=(86,),\n",
    "            dtype=np.float32\n",
    "        )\n",
    "        \n",
    "        # Action space: discrete action + continuous parameters\n",
    "        self.action_space = spaces.Dict({\n",
    "            'action_type': spaces.Discrete(10),\n",
    "            'parameters': spaces.Box(\n",
    "                low=np.array([0.0, -1.0, 0.0]),\n",
    "                high=np.array([1.0, 1.0, 1.0]),\n",
    "                dtype=np.float32\n",
    "            )\n",
    "        })\n",
    "        \n",
    "        # Episode parameters\n",
    "        self.max_steps = 1000\n",
    "        self.current_step = 0\n",
    "        \n",
    "        # Robot state (initialized in reset)\n",
    "        self.state = None\n",
    "        self.goal_position = np.array([5.0, 0.0, 0.0])  # 5m ahead\n",
    "    \n",
    "    def reset(self, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "        \n",
    "        # Reset step counter\n",
    "        self.current_step = 0\n",
    "        \n",
    "        # Reset reward function\n",
    "        self.reward_fn.reset()\n",
    "        \n",
    "        # Initialize robot state\n",
    "        self.state = RobotState(\n",
    "            base_orientation=np.zeros(3),\n",
    "            base_angular_velocity=np.zeros(3),\n",
    "            base_linear_velocity=np.zeros(3),\n",
    "            joint_positions=np.zeros(29),\n",
    "            joint_velocities=np.zeros(29),\n",
    "            foot_contacts=np.array([1, 1]),\n",
    "            goal_direction=self.goal_position / np.linalg.norm(self.goal_position),\n",
    "            goal_distance=np.linalg.norm(self.goal_position),\n",
    "            obstacle_proximity=np.ones(8) * 10.0,\n",
    "            cmd_velocity=np.zeros(3),\n",
    "            cmd_mode=1,\n",
    "            cmd_gesture=0\n",
    "        )\n",
    "        \n",
    "        # Read controller if available\n",
    "        if self.controller and self.controller.available:\n",
    "            commands = self.controller.read_commands()\n",
    "            self.state.cmd_velocity = commands['velocity']\n",
    "            self.state.cmd_mode = commands['mode']\n",
    "            self.state.cmd_gesture = commands['gesture']\n",
    "        \n",
    "        return self.state.to_array(), {}\n",
    "    \n",
    "    def step(self, action):\n",
    "        # Parse action\n",
    "        action_type = HighLevelAction(action['action_type'])\n",
    "        velocity_scale, direction_bias, audio_volume = action['parameters']\n",
    "        \n",
    "        action_cmd = ActionCommand(\n",
    "            action_type=action_type,\n",
    "            velocity_scale=float(velocity_scale),\n",
    "            direction_bias=float(direction_bias),\n",
    "            audio_volume=float(audio_volume)\n",
    "        )\n",
    "        \n",
    "        # Execute action (simulate robot dynamics)\n",
    "        next_state = self._simulate_step(action_cmd)\n",
    "        \n",
    "        # Read new controller commands\n",
    "        if self.controller and self.controller.available:\n",
    "            commands = self.controller.read_commands()\n",
    "            next_state.cmd_velocity = commands['velocity']\n",
    "            next_state.cmd_mode = commands['mode']\n",
    "            next_state.cmd_gesture = commands['gesture']\n",
    "        \n",
    "        # Compute reward\n",
    "        reward, reward_breakdown = self.reward_fn.compute_reward(\n",
    "            self.state, action_cmd, next_state\n",
    "        )\n",
    "        \n",
    "        # Check termination\n",
    "        self.current_step += 1\n",
    "        \n",
    "        goal_reached = next_state.goal_distance < self.reward_fn.goal_radius\n",
    "        fell = abs(next_state.base_orientation[0]) > self.reward_fn.fall_angle or \\\n",
    "               abs(next_state.base_orientation[1]) > self.reward_fn.fall_angle\n",
    "        \n",
    "        terminated = goal_reached or fell\n",
    "        truncated = self.current_step >= self.max_steps\n",
    "        \n",
    "        # Update state\n",
    "        self.state = next_state\n",
    "        \n",
    "        info = {\n",
    "            'goal_reached': goal_reached,\n",
    "            'fell': fell,\n",
    "            'goal_distance': next_state.goal_distance,\n",
    "            'reward_breakdown': reward_breakdown\n",
    "        }\n",
    "        \n",
    "        return next_state.to_array(), reward, terminated, truncated, info\n",
    "    \n",
    "    def _simulate_step(self, action: ActionCommand) -> RobotState:\n",
    "        \"\"\"\n",
    "        Simulate robot dynamics for one timestep\n",
    "        \n",
    "        In real deployment, this would be replaced with actual robot interface\n",
    "        \"\"\"\n",
    "        dt = 0.1  # 10 Hz control\n",
    "        \n",
    "        # Get motion command\n",
    "        motion_cmd = action.to_motion_command()\n",
    "        velocity = motion_cmd['velocity']\n",
    "        \n",
    "        # Simple kinematic update (placeholder physics)\n",
    "        current_pos = self.goal_position - self.state.goal_direction * self.state.goal_distance\n",
    "        \n",
    "        # Update position based on velocity\n",
    "        new_pos = current_pos + velocity[:2] * dt  # Only x, y (2D navigation)\n",
    "        \n",
    "        # Update goal distance\n",
    "        new_goal_vector = self.goal_position[:2] - new_pos\n",
    "        new_goal_distance = np.linalg.norm(new_goal_vector)\n",
    "        new_goal_direction = np.append(\n",
    "            new_goal_vector / (new_goal_distance + 1e-6), [0]\n",
    "        )  # Normalize + add z=0\n",
    "        \n",
    "        # Add small noise to orientation (simulate walking dynamics)\n",
    "        orientation_noise = np.random.normal(0, 0.02, size=3)\n",
    "        new_orientation = self.state.base_orientation + orientation_noise\n",
    "        \n",
    "        # Create next state\n",
    "        next_state = RobotState(\n",
    "            base_orientation=new_orientation,\n",
    "            base_angular_velocity=self.state.base_angular_velocity * 0.9,  # Damping\n",
    "            base_linear_velocity=velocity,\n",
    "            joint_positions=self.state.joint_positions,  # Simplified\n",
    "            joint_velocities=self.state.joint_velocities * 0.9,\n",
    "            foot_contacts=self.state.foot_contacts,\n",
    "            goal_direction=new_goal_direction,\n",
    "            goal_distance=new_goal_distance,\n",
    "            obstacle_proximity=self.state.obstacle_proximity,\n",
    "            cmd_velocity=self.state.cmd_velocity,\n",
    "            cmd_mode=self.state.cmd_mode,\n",
    "            cmd_gesture=self.state.cmd_gesture\n",
    "        )\n",
    "        \n",
    "        return next_state\n",
    "    \n",
    "    def render(self):\n",
    "        \"\"\"Optional rendering (not implemented)\"\"\"\n",
    "        pass\n",
    "    \n",
    "    def close(self):\n",
    "        \"\"\"Cleanup\"\"\"\n",
    "        if self.controller and PYGAME_AVAILABLE:\n",
    "            pygame.quit()\n",
    "\n",
    "# Test environment\n",
    "print(\"Creating high-level RL environment...\")\n",
    "env = G1HighLevelEnv(use_controller=False, simulation=True)\n",
    "\n",
    "print(f\"\\nObservation space: {env.observation_space}\")\n",
    "print(f\"Action space: {env.action_space}\")\n",
    "\n",
    "# Test episode\n",
    "obs, info = env.reset()\n",
    "print(f\"\\nInitial observation shape: {obs.shape}\")\n",
    "\n",
    "for i in range(5):\n",
    "    action = {\n",
    "        'action_type': 1,  # WALK_FORWARD\n",
    "        'parameters': np.array([0.8, 0.0, 0.5])\n",
    "    }\n",
    "    obs, reward, terminated, truncated, info = env.step(action)\n",
    "    print(f\"Step {i+1}: reward={reward:.4f}, goal_dist={info['goal_distance']:.2f}m\")\n",
    "    \n",
    "    if terminated or truncated:\n",
    "        break\n",
    "\n",
    "print(\"\\n‚úì Environment test complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='training'></a>\n",
    "## 7. Training with PPO\n",
    "\n",
    "Train the policy using Proximal Policy Optimization (PPO)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom wrapper to flatten Dict action space for SB3\n",
    "class FlattenedActionWrapper(gym.Wrapper):\n",
    "    \"\"\"Flatten Dict action space to Box for compatibility\"\"\"\n",
    "    \n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        \n",
    "        # Flattened action: [action_type (1), parameters (3)] = 4D\n",
    "        self.action_space = spaces.Box(\n",
    "            low=np.array([0.0, 0.0, -1.0, 0.0]),\n",
    "            high=np.array([9.0, 1.0, 1.0, 1.0]),\n",
    "            dtype=np.float32\n",
    "        )\n",
    "    \n",
    "    def step(self, action):\n",
    "        # Convert flattened action back to dict\n",
    "        action_type = int(np.clip(action[0], 0, 9))\n",
    "        parameters = action[1:]\n",
    "        \n",
    "        dict_action = {\n",
    "            'action_type': action_type,\n",
    "            'parameters': parameters\n",
    "        }\n",
    "        \n",
    "        return self.env.step(dict_action)\n",
    "\n",
    "# Training callback to log progress\n",
    "class TrainingCallback(BaseCallback):\n",
    "    def __init__(self, verbose=0):\n",
    "        super().__init__(verbose)\n",
    "        self.episode_rewards = []\n",
    "        self.episode_lengths = []\n",
    "        self.current_episode_reward = 0\n",
    "        self.current_episode_length = 0\n",
    "    \n",
    "    def _on_step(self) -> bool:\n",
    "        self.current_episode_reward += self.locals['rewards'][0]\n",
    "        self.current_episode_length += 1\n",
    "        \n",
    "        if self.locals['dones'][0]:\n",
    "            self.episode_rewards.append(self.current_episode_reward)\n",
    "            self.episode_lengths.append(self.current_episode_length)\n",
    "            \n",
    "            if len(self.episode_rewards) % 10 == 0:\n",
    "                avg_reward = np.mean(self.episode_rewards[-10:])\n",
    "                avg_length = np.mean(self.episode_lengths[-10:])\n",
    "                print(f\"Episode {len(self.episode_rewards)}: \"\n",
    "                      f\"avg_reward={avg_reward:.2f}, avg_length={avg_length:.1f}\")\n",
    "            \n",
    "            self.current_episode_reward = 0\n",
    "            self.current_episode_length = 0\n",
    "        \n",
    "        return True\n",
    "\n",
    "if SB3_AVAILABLE:\n",
    "    print(\"üéì Starting PPO training...\\n\")\n",
    "    \n",
    "    # Create environment with wrapper\n",
    "    train_env = FlattenedActionWrapper(G1HighLevelEnv(use_controller=False, simulation=True))\n",
    "    \n",
    "    # Create PPO model\n",
    "    model = PPO(\n",
    "        policy=\"MlpPolicy\",\n",
    "        env=train_env,\n",
    "        learning_rate=3e-4,\n",
    "        n_steps=2048,\n",
    "        batch_size=64,\n",
    "        n_epochs=10,\n",
    "        gamma=0.99,\n",
    "        gae_lambda=0.95,\n",
    "        clip_range=0.2,\n",
    "        verbose=1,\n",
    "        device='cpu'  # Change to 'cuda' if GPU available\n",
    "    )\n",
    "    \n",
    "    # Train\n",
    "    callback = TrainingCallback()\n",
    "    \n",
    "    print(\"Training for 50,000 timesteps (increase for better performance)...\\n\")\n",
    "    model.learn(\n",
    "        total_timesteps=50000,\n",
    "        callback=callback,\n",
    "        progress_bar=True\n",
    "    )\n",
    "    \n",
    "    # Save model\n",
    "    model.save(\"g1_high_level_ppo\")\n",
    "    print(\"\\n‚úì Model saved to g1_high_level_ppo.zip\")\n",
    "    \n",
    "    # Plot training progress\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Episode rewards\n",
    "    ax1.plot(callback.episode_rewards, alpha=0.3, label='Episode Reward')\n",
    "    window = 10\n",
    "    if len(callback.episode_rewards) > window:\n",
    "        moving_avg = np.convolve(\n",
    "            callback.episode_rewards, \n",
    "            np.ones(window)/window, \n",
    "            mode='valid'\n",
    "        )\n",
    "        ax1.plot(range(window-1, len(callback.episode_rewards)), moving_avg, \n",
    "                'r-', linewidth=2, label=f'{window}-Episode Moving Average')\n",
    "    ax1.set_xlabel('Episode')\n",
    "    ax1.set_ylabel('Total Reward')\n",
    "    ax1.set_title('Training Progress: Rewards')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Episode lengths\n",
    "    ax2.plot(callback.episode_lengths, alpha=0.3, label='Episode Length')\n",
    "    if len(callback.episode_lengths) > window:\n",
    "        moving_avg = np.convolve(\n",
    "            callback.episode_lengths, \n",
    "            np.ones(window)/window, \n",
    "            mode='valid'\n",
    "        )\n",
    "        ax2.plot(range(window-1, len(callback.episode_lengths)), moving_avg, \n",
    "                'g-', linewidth=2, label=f'{window}-Episode Moving Average')\n",
    "    ax2.set_xlabel('Episode')\n",
    "    ax2.set_ylabel('Steps')\n",
    "    ax2.set_title('Training Progress: Episode Length')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('training_progress.png', dpi=150)\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nüìä Training visualization saved to training_progress.png\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Stable-Baselines3 not available - skipping training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='deployment'></a>\n",
    "## 8. Deployment to Real Robot\n",
    "\n",
    "Deploy the trained policy to the physical Unitree G1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class G1RobotInterface:\n",
    "    \"\"\"Interface to real Unitree G1 hardware\"\"\"\n",
    "    \n",
    "    def __init__(self, network_interface: str = \"eth0\"):\n",
    "        self.network_interface = network_interface\n",
    "        self.initialized = False\n",
    "        \n",
    "        if UNITREE_SDK_AVAILABLE:\n",
    "            try:\n",
    "                # Initialize DDS\n",
    "                ChannelFactoryInitialize(0, network_interface)\n",
    "                \n",
    "                # Create clients\n",
    "                self.loco_client = LocoClient()\n",
    "                self.loco_client.SetTimeout(10.0)\n",
    "                self.loco_client.Init()\n",
    "                \n",
    "                self.audio_client = AudioClient()\n",
    "                self.audio_client.SetTimeout(10.0)\n",
    "                self.audio_client.Init()\n",
    "                \n",
    "                self.arm_client = G1ArmActionClient()\n",
    "                self.arm_client.SetTimeout(10.0)\n",
    "                self.arm_client.Init()\n",
    "                \n",
    "                # Subscribe to state\n",
    "                self.state_subscriber = ChannelSubscriber(\"rt/lowstate\", LowState_)\n",
    "                self.state_subscriber.Init(self._state_callback, 10)\n",
    "                \n",
    "                self.current_state = None\n",
    "                self.initialized = True\n",
    "                \n",
    "                print(f\"‚úì Connected to G1 on {network_interface}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è  Failed to connect to robot: {e}\")\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è  Unitree SDK not available\")\n",
    "    \n",
    "    def _state_callback(self, msg: LowState_):\n",
    "        \"\"\"Store latest robot state\"\"\"\n",
    "        self.current_state = msg\n",
    "    \n",
    "    def get_state(self) -> Optional[RobotState]:\n",
    "        \"\"\"\n",
    "        Read current robot state\n",
    "        \n",
    "        Returns:\n",
    "            state: RobotState object or None if unavailable\n",
    "        \"\"\"\n",
    "        if not self.initialized or self.current_state is None:\n",
    "            return None\n",
    "        \n",
    "        # Extract data from low-level state\n",
    "        imu = self.current_state.imu_state\n",
    "        motors = self.current_state.motor_state\n",
    "        \n",
    "        # Build RobotState\n",
    "        state = RobotState(\n",
    "            base_orientation=np.array(imu.rpy),\n",
    "            base_angular_velocity=np.array(imu.gyroscope),\n",
    "            base_linear_velocity=np.array([0, 0, 0]),  # Not directly available\n",
    "            joint_positions=np.array([m.q for m in motors]),\n",
    "            joint_velocities=np.array([m.dq for m in motors]),\n",
    "            foot_contacts=np.array([1, 1]),  # Placeholder (need force sensors)\n",
    "            goal_direction=np.array([1, 0, 0]),  # Set externally\n",
    "            goal_distance=5.0,  # Set externally\n",
    "            obstacle_proximity=np.ones(8) * 10.0,  # From depth camera\n",
    "            cmd_velocity=np.zeros(3),  # From controller\n",
    "            cmd_mode=1,\n",
    "            cmd_gesture=0\n",
    "        )\n",
    "        \n",
    "        return state\n",
    "    \n",
    "    def send_velocity(self, velocity: np.ndarray):\n",
    "        \"\"\"Send velocity command to robot\"\"\"\n",
    "        if not self.initialized:\n",
    "            return\n",
    "        \n",
    "        vx, vy, vw = velocity\n",
    "        self.loco_client.Move(float(vx), float(vy), float(vw))\n",
    "    \n",
    "    def execute_gesture(self, gesture_id: int):\n",
    "        \"\"\"Execute pre-defined gesture\"\"\"\n",
    "        if not self.initialized:\n",
    "            return\n",
    "        \n",
    "        gesture_map = {\n",
    "            0: \"high wave\",\n",
    "            1: \"shake hand\",\n",
    "            2: \"clap\"\n",
    "        }\n",
    "        \n",
    "        gesture_name = gesture_map.get(gesture_id)\n",
    "        if gesture_name and gesture_name in action_map:\n",
    "            self.arm_client.ExecuteAction(action_map[gesture_name])\n",
    "            time.sleep(0.1)\n",
    "            self.arm_client.ExecuteAction(action_map[\"release arm\"])\n",
    "    \n",
    "    def speak(self, message: str, volume: float = 0.5):\n",
    "        \"\"\"Text-to-speech output\"\"\"\n",
    "        if not self.initialized:\n",
    "            return\n",
    "        \n",
    "        # Set volume (0-100)\n",
    "        self.audio_client.SetVolume(int(volume * 100))\n",
    "        \n",
    "        # Chinese TTS (G1 default)\n",
    "        self.audio_client.TtsMaker(message, 0)\n",
    "    \n",
    "    def emergency_stop(self):\n",
    "        \"\"\"Emergency stop\"\"\"\n",
    "        if not self.initialized:\n",
    "            return\n",
    "        \n",
    "        self.loco_client.Damp()\n",
    "        print(\"‚ö†Ô∏è  EMERGENCY STOP ACTIVATED\")\n",
    "\n",
    "class DeployedPolicy:\n",
    "    \"\"\"Deployed RL policy running on real robot\"\"\"\n",
    "    \n",
    "    def __init__(self, model_path: str, network_interface: str = \"eth0\"):\n",
    "        # Load trained model\n",
    "        if SB3_AVAILABLE:\n",
    "            self.model = PPO.load(model_path)\n",
    "            print(f\"‚úì Loaded policy from {model_path}\")\n",
    "        else:\n",
    "            self.model = None\n",
    "            print(\"‚ö†Ô∏è  SB3 not available\")\n",
    "        \n",
    "        # Connect to robot\n",
    "        self.robot = G1RobotInterface(network_interface)\n",
    "        \n",
    "        # Controller for human input\n",
    "        self.controller = USBControllerInput()\n",
    "        \n",
    "        # Goal tracking\n",
    "        self.goal_position = np.array([5.0, 0.0, 0.0])\n",
    "    \n",
    "    def run(self, duration: float = 60.0):\n",
    "        \"\"\"\n",
    "        Run policy on real robot\n",
    "        \n",
    "        Args:\n",
    "            duration: How long to run (seconds)\n",
    "        \"\"\"\n",
    "        if not self.robot.initialized or self.model is None:\n",
    "            print(\"‚ö†Ô∏è  Cannot run: robot or model not initialized\")\n",
    "            return\n",
    "        \n",
    "        print(f\"ü§ñ Running policy for {duration:.0f} seconds...\")\n",
    "        print(\"   Press START button on controller for emergency stop\\n\")\n",
    "        \n",
    "        # Stand up\n",
    "        self.robot.loco_client.Squat2StandUp()\n",
    "        time.sleep(2)\n",
    "        \n",
    "        start_time = time.time()\n",
    "        step_count = 0\n",
    "        \n",
    "        while time.time() - start_time < duration:\n",
    "            # Get robot state\n",
    "            state = self.robot.get_state()\n",
    "            if state is None:\n",
    "                time.sleep(0.1)\n",
    "                continue\n",
    "            \n",
    "            # Get controller commands\n",
    "            if self.controller.available:\n",
    "                commands = self.controller.read_commands()\n",
    "                state.cmd_velocity = commands['velocity']\n",
    "                state.cmd_mode = commands['mode']\n",
    "                state.cmd_gesture = commands['gesture']\n",
    "                \n",
    "                # Check emergency stop\n",
    "                if commands['emergency_stop']:\n",
    "                    self.robot.emergency_stop()\n",
    "                    break\n",
    "            \n",
    "            # Update goal tracking (would come from navigation system)\n",
    "            current_pos = np.array([0, 0, 0])  # Placeholder\n",
    "            goal_vec = self.goal_position - current_pos\n",
    "            state.goal_distance = np.linalg.norm(goal_vec)\n",
    "            state.goal_direction = goal_vec / (state.goal_distance + 1e-6)\n",
    "            \n",
    "            # Get action from policy\n",
    "            obs = state.to_array()\n",
    "            action_flat, _ = self.model.predict(obs, deterministic=True)\n",
    "            \n",
    "            # Parse action\n",
    "            action_type = HighLevelAction(int(np.clip(action_flat[0], 0, 9)))\n",
    "            velocity_scale, direction_bias, audio_volume = action_flat[1:]\n",
    "            \n",
    "            action_cmd = ActionCommand(\n",
    "                action_type=action_type,\n",
    "                velocity_scale=float(velocity_scale),\n",
    "                direction_bias=float(direction_bias),\n",
    "                audio_volume=float(audio_volume)\n",
    "            )\n",
    "            \n",
    "            # Execute action\n",
    "            motion_cmd = action_cmd.to_motion_command()\n",
    "            \n",
    "            if motion_cmd['is_gesture']:\n",
    "                self.robot.execute_gesture(motion_cmd['gesture_id'])\n",
    "            else:\n",
    "                self.robot.send_velocity(motion_cmd['velocity'])\n",
    "            \n",
    "            # Log progress\n",
    "            if step_count % 10 == 0:\n",
    "                print(f\"\\rStep {step_count}: {action_cmd.get_description()}\", \n",
    "                      end='', flush=True)\n",
    "            \n",
    "            step_count += 1\n",
    "            time.sleep(0.1)  # 10 Hz control loop\n",
    "        \n",
    "        # Stop robot\n",
    "        self.robot.loco_client.Move(0, 0, 0)\n",
    "        print(\"\\n\\n‚úì Policy execution complete\")\n",
    "\n",
    "# Example deployment (requires real robot)\n",
    "print(\"\\nDeployment Example:\\n\")\n",
    "print(\"To deploy trained policy to real G1:\")\n",
    "print(\"\"\"\\n```python\n",
    "# Load and run policy\n",
    "policy = DeployedPolicy(\n",
    "    model_path='g1_high_level_ppo.zip',\n",
    "    network_interface='eth0'\n",
    ")\n",
    "\n",
    "# Run for 60 seconds\n",
    "policy.run(duration=60.0)\n",
    "```\"\"\")\n",
    "print(\"\\n‚ö†Ô∏è  Ensure robot is in safe environment before running!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='applications'></a>\n",
    "## 9. Example Applications\n",
    "\n",
    "### Application 1: Interactive Tour Guide\n",
    "\n",
    "Robot follows human operator and responds to gesture commands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TourGuideApplication:\n",
    "    \"\"\"\n",
    "    Tour guide robot that follows human and performs gestures\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, policy_path: str):\n",
    "        self.deployed_policy = DeployedPolicy(policy_path)\n",
    "        self.waypoints = [\n",
    "            (\"Entrance\", np.array([0, 0, 0])),\n",
    "            (\"Exhibit A\", np.array([5, 0, 0])),\n",
    "            (\"Exhibit B\", np.array([5, 5, 0])),\n",
    "            (\"Exit\", np.array([0, 5, 0]))\n",
    "        ]\n",
    "        self.current_waypoint = 0\n",
    "    \n",
    "    def start_tour(self):\n",
    "        \"\"\"\n",
    "        Execute guided tour with narration\n",
    "        \"\"\"\n",
    "        print(\"üé≠ Starting tour guide mode...\")\n",
    "        \n",
    "        for name, position in self.waypoints:\n",
    "            print(f\"\\nNavigating to: {name}\")\n",
    "            \n",
    "            # Update goal\n",
    "            self.deployed_policy.goal_position = position\n",
    "            \n",
    "            # Wave when arriving\n",
    "            self.deployed_policy.robot.execute_gesture(0)  # Wave\n",
    "            \n",
    "            # Narration (would be location-specific)\n",
    "            self.deployed_policy.robot.speak(f\"Welcome to {name}\")\n",
    "            \n",
    "            # Wait at location\n",
    "            time.sleep(5)\n",
    "        \n",
    "        print(\"\\n‚úì Tour complete!\")\n",
    "\n",
    "print(\"\\nüìù Example: Tour Guide Application\")\n",
    "print(\"   - Robot follows pre-defined waypoints\")\n",
    "print(\"   - Waves and provides audio narration at each location\")\n",
    "print(\"   - Responds to human controller for manual override\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Application 2: Warehouse Assistant\n",
    "\n",
    "Robot navigates warehouse and performs pick-and-place tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WarehouseAssistant:\n",
    "    \"\"\"\n",
    "    Warehouse robot for logistics tasks\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, policy_path: str):\n",
    "        self.deployed_policy = DeployedPolicy(policy_path)\n",
    "        self.inventory = {\n",
    "            'A1': 'Computer Monitor',\n",
    "            'A2': 'Keyboard',\n",
    "            'B1': 'Mouse',\n",
    "            'B2': 'Headphones'\n",
    "        }\n",
    "    \n",
    "    def pick_item(self, location: str) -> bool:\n",
    "        \"\"\"\n",
    "        Navigate to location and pick item\n",
    "        \n",
    "        Returns:\n",
    "            success: True if item retrieved\n",
    "        \"\"\"\n",
    "        item_name = self.inventory.get(location, \"Unknown\")\n",
    "        print(f\"\\nüì¶ Retrieving: {item_name} from {location}\")\n",
    "        \n",
    "        # Navigate to location (would use real coordinates)\n",
    "        # For demo, use placeholder\n",
    "        self.deployed_policy.goal_position = np.array([5, 0, 0])\n",
    "        \n",
    "        # Audio feedback\n",
    "        self.deployed_policy.robot.speak(f\"Retrieving {item_name}\")\n",
    "        \n",
    "        # Simulate pick (would use manipulation skills)\n",
    "        time.sleep(3)\n",
    "        \n",
    "        print(f\"‚úì Item retrieved: {item_name}\")\n",
    "        return True\n",
    "    \n",
    "    def deliver_item(self, destination: str) -> bool:\n",
    "        \"\"\"\n",
    "        Deliver held item to destination\n",
    "        \"\"\"\n",
    "        print(f\"\\nüöö Delivering to: {destination}\")\n",
    "        \n",
    "        # Navigate to destination\n",
    "        # ...\n",
    "        \n",
    "        # Place item\n",
    "        self.deployed_policy.robot.speak(\"Delivery complete\")\n",
    "        \n",
    "        return True\n",
    "\n",
    "print(\"\\nüìù Example: Warehouse Assistant\")\n",
    "print(\"   - Autonomous navigation to storage locations\")\n",
    "print(\"   - Audio confirmation of tasks\")\n",
    "print(\"   - Human operator can override via controller\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## Summary & Next Steps\n\nThis notebook demonstrated a **complete high-level RL framework** for the Unitree G1:\n\n‚úÖ **State Space:** Combined robot sensors + human controller input (86D)  \n‚úÖ **Action Space:** Abstract high-level actions (locomotion, gestures, audio)  \n‚úÖ **Motion Primitives:** Pre-validated movement sequences  \n‚úÖ **Reward Function:** Simple sparse rewards for goal reaching and command following  \n‚úÖ **Training:** PPO implementation with Stable-Baselines3  \n‚úÖ **Deployment:** Real robot interface with safety features  \n‚úÖ **Applications:** Tour guide and warehouse assistant examples\n\n### Key Advantages of High-Level RL\n\n1. **Faster Learning:** Smaller action space than low-level joint control\n2. **Better Generalization:** Abstractions transfer across tasks\n3. **Safer:** Primitives are pre-tested and bounded\n4. **Human-Interpretable:** Actions have semantic meaning\n5. **Controller Integration:** Human can guide learning and provide real-time input\n\n### Next Steps\n\n1. **Expand Motion Library:** Add more primitives (stairs, doors, obstacles)\n2. **Vision Integration:** Add camera observations to state space\n3. **Multi-Task Learning:** Train single policy for multiple applications\n4. **Hierarchical RL:** Add meta-controller for task planning\n5. **Human Preference Learning:** Fine-tune policy using human feedback\n6. **üÜï LLM-Based Agentic Control:** High-level task planning with language models (see next section!)\n\n### References\n\n- **Hierarchical RL:** \"Options Framework\" (Sutton, Precup, Singh)\n- **Human-in-the-Loop:** \"Deep TAMER\" (Warnell et al.)\n- **PPO Algorithm:** \"Proximal Policy Optimization\" (Schulman et al.)\n- **Motion Primitives:** \"Dynamic Movement Primitives\" (Schaal et al.)\n\n---\n\n*High-Level RL Framework for Unitree G1*  \n*Enables rapid task learning with human guidance*"
  },
  {
   "cell_type": "markdown",
   "source": "---\n## LLM-Based Control: Summary & Advanced Topics\n\n### What We Built\n\nThis section implemented a **complete LLM-based agentic control system** that combines:\n\n‚úÖ **Natural Language Interface:** Control robot with plain English commands  \n‚úÖ **RAG Knowledge Base:** Context from manuals, code repos, API docs, and past executions  \n‚úÖ **LLM Agent Planner:** HuggingFace models for intelligent action planning  \n‚úÖ **RLHF Reward Model:** Secondary LLM evaluates and scores performance  \n‚úÖ **Integrated Execution:** Seamless connection to existing RL policies  \n‚úÖ **Continuous Learning:** Feedback collection for model fine-tuning  \n\n### System Architecture\n\n```\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ                    User Command (NL)                         ‚îÇ\n‚îÇ                 \"Walk to door and wave\"                      ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n                     ‚îÇ\n                     ‚ñº\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ              RAG System Retrieval                            ‚îÇ\n‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê           ‚îÇ\n‚îÇ  ‚îÇ  Manuals   ‚îÇ  ‚îÇ   GitHub   ‚îÇ  ‚îÇ  Past Logs  ‚îÇ           ‚îÇ\n‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò           ‚îÇ\n‚îÇ         Relevant context for planning (vector search)        ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n                     ‚îÇ\n                     ‚ñº\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ          LLM Agent Controller (Planning)                     ‚îÇ\n‚îÇ  Model: Llama-3.1-70B / Mixtral-8x7B / Phi-3               ‚îÇ\n‚îÇ  Output: [\"walk_forward\", \"idle\", \"wave\"]                   ‚îÇ\n‚îÇ          + reasoning + safety validation                     ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n                     ‚îÇ\n                     ‚ñº\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ         Motion Primitive Execution                           ‚îÇ\n‚îÇ  Uses existing RL policy + Unitree SDK                      ‚îÇ\n‚îÇ  Monitors state, detects failures                           ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n                     ‚îÇ\n                     ‚ñº\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ          RLHF Reward Model (Evaluation)                      ‚îÇ\n‚îÇ  Scores: Safety, Efficiency, Goal Progress, Naturalness    ‚îÇ\n‚îÇ  Provides: Reward signal + textual feedback                 ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n                     ‚îÇ\n                     ‚ñº\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ         Feedback Loop (Learning)                             ‚îÇ\n‚îÇ  Stores execution logs in RAG for future queries           ‚îÇ\n‚îÇ  Exports training data for LLM fine-tuning                  ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n```\n\n### Key Advantages Over Pure RL\n\n| Aspect | Traditional RL | LLM-Based Control |\n|--------|---------------|-------------------|\n| **Interface** | Numeric state/actions | Natural language |\n| **Learning Speed** | Requires 100K+ episodes | Few-shot from documentation |\n| **Generalization** | Task-specific | Zero-shot to new tasks |\n| **Explainability** | Black box policy | Reasoning provided |\n| **Human Feedback** | Requires reward engineering | Direct evaluation via RLHF |\n| **Knowledge Integration** | Limited to training data | RAG from unlimited sources |\n\n### Setup Instructions\n\n#### 1. Get HuggingFace API Token\n\n```python\n# Get free token from: https://huggingface.co/settings/tokens\nimport os\nos.environ[\"HUGGINGFACE_TOKEN\"] = \"hf_...\"  # Your token here\n```\n\n#### 2. Choose LLM Model\n\nAvailable models (via HuggingFace Inference API):\n\n- **Llama-3.1-70B-Instruct** - Best quality, slower (recommended)\n- **Mixtral-8x7B-Instruct-v0.1** - Fast, good balance\n- **Phi-3-medium-4k-instruct** - Lightweight, fastest\n- **Qwen/Qwen2.5-72B-Instruct** - Strong multilingual support\n\n```python\nllm_controller = LLMAgentController(\n    model_name=\"meta-llama/Llama-3.1-70B-Instruct\",\n    knowledge_base=knowledge_base,\n    use_rag=True\n)\n```\n\n#### 3. Populate Knowledge Base\n\n```python\n# Add your robot documentation\nknowledge_base.add_manual_documentation(\n    text=open(\"G1_Manual.pdf\").read(),\n    source=\"Official_Manual\"\n)\n\n# Add code from GitHub\nknowledge_base.add_github_code(\n    code=open(\"examples/navigation.py\").read(),\n    repo=\"unitree/unitree_sdk2\",\n    file_path=\"examples/navigation.py\"\n)\n\n# Add API docs\nknowledge_base.add_api_documentation({\n    \"function_name\": \"description...\"\n})\n```\n\n#### 4. Deploy to Real Robot\n\n```python\n# Connect to real G1\nrobot = G1RobotInterface(network_interface=\"eth0\")\n\n# Create controller\ncontroller = LLMBasedRobotController(\n    knowledge_base=knowledge_base,\n    llm_agent=llm_controller,\n    rlhf_model=rlhf_reward_model,\n    robot_interface=robot  # Use real robot instead of sim\n)\n\n# Execute commands\ncontroller.execute_command(\"Walk to the meeting room and greet visitors\")\n```\n\n### Advanced Topics\n\n#### Fine-Tuning the LLM Agent\n\nAfter collecting execution data, fine-tune for robot-specific control:\n\n```python\n# Export training data\ncontroller.export_training_data(\"robot_training.json\")\n\n# Use HuggingFace AutoTrain or custom training\n# Format: instruction ‚Üí actions + reasoning pairs\n# Example entry:\n{\n    \"instruction\": \"Walk forward and wave\",\n    \"output\": {\n        \"actions\": [\"walk_forward\", \"idle\", \"wave\"],\n        \"reasoning\": \"Break into locomotion then gesture for safety\"\n    }\n}\n```\n\n#### Training a Custom Reward Model\n\nFine-tune a smaller model on collected feedback:\n\n```python\n# Export RLHF feedback\nrlhf_model.export_feedback_dataset(\"rlhf_training.json\")\n\n# Train reward model to predict scores\n# Input: (command, actions, state_delta)\n# Output: (safety, efficiency, progress, naturalness scores)\n```\n\n#### Multimodal Integration\n\nAdd vision for enhanced understanding:\n\n```python\n# Add camera observations to state\nstate.camera_image = robot.get_camera_image()\n\n# Use multimodal LLM (e.g., LLaVA, GPT-4V)\n# Provides visual grounding for commands like:\n# \"Pick up the red box on the table\"\n```\n\n#### Hierarchical LLM Control\n\nUse multiple LLMs at different abstraction levels:\n\n```python\n# High-level task planner\ntask_plan = meta_llm.plan(\"Clean the office\")\n# ‚Üí [\"go_to_desk\", \"pick_trash\", \"go_to_bin\", \"dispose\"]\n\n# Low-level action planner (our LLM controller)\nfor task in task_plan:\n    actions = llm_controller.plan_actions(task)\n    execute(actions)\n```\n\n### Limitations & Future Work\n\n**Current Limitations:**\n- LLM latency (1-3 seconds per query)\n- Requires internet connection for HF API\n- Token costs for extensive use\n- Limited to predefined action primitives\n\n**Future Improvements:**\n1. **Local LLM Deployment:** Use quantized models (GGUF format) with llama.cpp\n2. **Action Space Learning:** Let LLM discover new primitives\n3. **Multi-Robot Coordination:** Extend to robot teams\n4. **Real-Time Adaptation:** Online learning from execution feedback\n5. **Safety Guarantees:** Formal verification of LLM plans\n\n### References & Resources\n\n**LLM for Robotics:**\n- \"Do As I Can, Not As I Say\" (Google, 2022) - SayCan paper\n- \"Code as Policies\" (Google, 2023) - LLMs generate robot code\n- \"RT-2: Vision-Language-Action Models\" (Google, 2023)\n- \"VoxPoser\" (Stanford, 2023) - LLM spatial reasoning\n\n**RLHF:**\n- \"Learning to Summarize from Human Feedback\" (OpenAI, 2020)\n- \"Training Language Models with Human Preferences\" (Anthropic, 2022)\n- \"Constitutional AI\" (Anthropic, 2022)\n\n**RAG Systems:**\n- \"Retrieval-Augmented Generation for Knowledge-Intensive Tasks\" (Facebook, 2020)\n- LangChain documentation: https://python.langchain.com\n- ChromaDB: https://www.trychroma.com\n\n**HuggingFace Resources:**\n- Inference API: https://huggingface.co/docs/api-inference\n- Model Hub: https://huggingface.co/models\n- Transformers: https://huggingface.co/docs/transformers\n\n---\n\n*LLM-Based Agentic Control for Unitree G1*  \n*Natural language interface with continuous learning from feedback*",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Example commands to test the LLM-based control system\nexample_commands = [\n    \"Walk forward for a few steps\",\n    \"Turn left and then wave hello\",\n    \"Move backward carefully and stop\",\n    \"Greet the person with a handshake gesture\",\n    \"Perform a welcoming gesture\"\n]\n\nprint(\"Testing LLM-Based Robot Control System\")\nprint(\"Note: Using fallback planner if HuggingFace token not configured\\n\")\n\n# Execute each command\nfor i, cmd in enumerate(example_commands[:3]):  # Test first 3 commands\n    result = llm_robot_controller.execute_command(cmd, verbose=True)\n    time.sleep(0.5)  # Brief pause between commands\n\n# Performance summary\nprint(\"\\n\" + \"=\"*70)\nprint(\"Performance Summary\")\nprint(\"=\"*70)\n\nsummary = llm_robot_controller.get_performance_summary()\nprint(f\"\\nTotal Commands: {summary.get('total_commands', 0)}\")\nprint(f\"Successful: {summary.get('successful', 0)}\")\nprint(f\"Success Rate: {summary.get('success_rate', 0)*100:.1f}%\")\nprint(f\"Average Reward: {summary.get('avg_reward', 0):+.3f}\")\nprint(f\"Average Safety Score: {summary.get('avg_safety', 0):.3f}\")\nprint(f\"Total Actions Executed: {summary.get('total_actions_executed', 0)}\")\n\n# Export training data for fine-tuning\nif llm_robot_controller.execution_history:\n    llm_robot_controller.export_training_data(\"llm_robot_training_data.json\")\n    rlhf_reward_model.export_feedback_dataset(\"rlhf_feedback_data.json\")\n    \n    print(\"\\n‚úì Training datasets exported for fine-tuning:\")\n    print(\"  - llm_robot_training_data.json (for agent fine-tuning)\")\n    print(\"  - rlhf_feedback_data.json (for reward model fine-tuning)\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Example: Running LLM-Based Commands",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "class LLMBasedRobotController:\n    \"\"\"\n    Complete LLM-based robot control system integrating:\n    - Natural language understanding\n    - RAG-enhanced context\n    - LLM action planning\n    - RL policy execution\n    - RLHF reward evaluation\n    \"\"\"\n    \n    def __init__(self,\n                 knowledge_base: RobotKnowledgeBase,\n                 llm_agent: LLMAgentController,\n                 rlhf_model: RLHFRewardModel,\n                 robot_interface: Optional[G1RobotInterface] = None):\n        \"\"\"\n        Initialize integrated control system\n        \n        Args:\n            knowledge_base: RAG knowledge base\n            llm_agent: LLM planning agent\n            rlhf_model: Reward evaluation model\n            robot_interface: Connection to real robot (None for simulation)\n        \"\"\"\n        self.knowledge_base = knowledge_base\n        self.llm_agent = llm_agent\n        self.rlhf_model = rlhf_model\n        self.robot_interface = robot_interface\n        \n        # Simulation environment (used when no real robot)\n        self.use_simulation = robot_interface is None\n        if self.use_simulation:\n            self.sim_env = G1HighLevelEnv(use_controller=False, simulation=True)\n            print(\"‚úì Using simulation environment\")\n        else:\n            print(\"‚úì Using real robot interface\")\n        \n        # Execution history for learning\n        self.execution_history = []\n        \n        # Performance metrics\n        self.success_count = 0\n        self.total_commands = 0\n    \n    def execute_command(self, command: str, verbose: bool = True) -> Dict:\n        \"\"\"\n        Execute a natural language command end-to-end\n        \n        Args:\n            command: Natural language instruction\n            verbose: Whether to print detailed logs\n        \n        Returns:\n            result: Execution result with metrics and feedback\n        \"\"\"\n        self.total_commands += 1\n        \n        if verbose:\n            print(\"\\n\" + \"=\"*70)\n            print(f\"üí¨ Command: \\\"{command}\\\"\")\n            print(\"=\"*70)\n        \n        # Step 1: Get current robot state\n        if self.use_simulation:\n            state_before, _ = self.sim_env.reset()\n            robot_state = self.sim_env.state\n        else:\n            robot_state = self.robot_interface.get_state()\n        \n        if verbose:\n            print(f\"\\n1Ô∏è‚É£  Current State:\")\n            print(f\"   Goal distance: {robot_state.goal_distance:.2f}m\")\n            print(f\"   Base tilt: {np.rad2deg(robot_state.base_orientation[:2])}\")\n        \n        # Step 2: Plan actions using LLM (with RAG context)\n        if verbose:\n            print(f\"\\n2Ô∏è‚É£  Planning actions...\")\n        \n        action_plan = self.llm_agent.plan_actions(command, robot_state)\n        \n        if not action_plan.safety_check:\n            print(\"‚ö†Ô∏è  SAFETY CHECK FAILED - Aborting execution\")\n            return {\n                \"success\": False,\n                \"reason\": \"unsafe_plan\",\n                \"plan\": action_plan\n            }\n        \n        # Step 3: Execute actions\n        if verbose:\n            print(f\"\\n3Ô∏è‚É£  Executing actions...\")\n        \n        execution_success = True\n        state_after = robot_state\n        \n        try:\n            for i, action_name in enumerate(action_plan.actions):\n                if verbose:\n                    print(f\"   [{i+1}/{len(action_plan.actions)}] {action_name}...\", end=' ')\n                \n                # Map action name to HighLevelAction enum\n                action_mapping = {\n                    \"idle\": HighLevelAction.IDLE,\n                    \"walk_forward\": HighLevelAction.WALK_FORWARD,\n                    \"walk_backward\": HighLevelAction.WALK_BACKWARD,\n                    \"strafe_left\": HighLevelAction.STRAFE_LEFT,\n                    \"strafe_right\": HighLevelAction.STRAFE_RIGHT,\n                    \"rotate_left\": HighLevelAction.ROTATE_LEFT,\n                    \"rotate_right\": HighLevelAction.ROTATE_RIGHT,\n                    \"wave\": HighLevelAction.GESTURE_WAVE,\n                    \"handshake\": HighLevelAction.GESTURE_HANDSHAKE,\n                    \"clap\": HighLevelAction.GESTURE_CLAP,\n                }\n                \n                action_enum = action_mapping.get(action_name, HighLevelAction.IDLE)\n                \n                if self.use_simulation:\n                    # Simulate action\n                    action_dict = {\n                        'action_type': action_enum.value,\n                        'parameters': np.array([0.8, 0.0, 0.5])\n                    }\n                    obs, reward, terminated, truncated, info = self.sim_env.step(action_dict)\n                    state_after = self.sim_env.state\n                    \n                    if terminated or truncated:\n                        if info.get('fell', False):\n                            execution_success = False\n                            if verbose:\n                                print(\"FAILED (fell)\")\n                            break\n                else:\n                    # Execute on real robot\n                    action_cmd = ActionCommand(action_enum, 0.8, 0.0, 0.5)\n                    motion_cmd = action_cmd.to_motion_command()\n                    \n                    if motion_cmd['is_gesture']:\n                        self.robot_interface.execute_gesture(motion_cmd['gesture_id'])\n                    else:\n                        self.robot_interface.send_velocity(motion_cmd['velocity'])\n                    \n                    time.sleep(2.0)  # Wait for action to complete\n                    state_after = self.robot_interface.get_state()\n                \n                if verbose:\n                    print(\"‚úì\")\n            \n            if verbose and execution_success:\n                print(f\"   Execution complete!\")\n        \n        except Exception as e:\n            execution_success = False\n            if verbose:\n                print(f\"\\n   ‚ö†Ô∏è  Execution error: {e}\")\n        \n        # Step 4: Evaluate with RLHF\n        if verbose:\n            print(f\"\\n4Ô∏è‚É£  Evaluating performance...\")\n        \n        feedback = self.rlhf_model.evaluate_action_sequence(\n            command=command,\n            actions=action_plan.actions,\n            robot_state_before=robot_state,\n            robot_state_after=state_after,\n            execution_success=execution_success\n        )\n        \n        # Step 5: Log execution for future learning\n        if RAG_AVAILABLE:\n            self.knowledge_base.add_execution_log(\n                command=command,\n                actions=action_plan.actions,\n                outcome=feedback.feedback_text,\n                success=execution_success and feedback.reward_score > 0\n            )\n        \n        # Update metrics\n        if execution_success and feedback.reward_score > 0:\n            self.success_count += 1\n        \n        # Store in history\n        self.execution_history.append({\n            \"command\": command,\n            \"plan\": action_plan,\n            \"state_before\": robot_state,\n            \"state_after\": state_after,\n            \"feedback\": feedback,\n            \"success\": execution_success,\n            \"timestamp\": datetime.now().isoformat()\n        })\n        \n        # Results\n        result = {\n            \"success\": execution_success and feedback.reward_score > 0,\n            \"plan\": action_plan,\n            \"feedback\": feedback,\n            \"state_before\": robot_state,\n            \"state_after\": state_after\n        }\n        \n        if verbose:\n            print(f\"\\n5Ô∏è‚É£  Results:\")\n            print(f\"   Status: {'‚úì Success' if result['success'] else '‚úó Failed'}\")\n            print(f\"   Success Rate: {self.success_count}/{self.total_commands} ({100*self.success_count/self.total_commands:.1f}%)\")\n            print(f\"   RLHF Reward: {feedback.reward_score:+.3f}\")\n        \n        return result\n    \n    def export_training_data(self, filepath: str):\n        \"\"\"Export execution history for fine-tuning the LLM agent\"\"\"\n        training_data = []\n        \n        for entry in self.execution_history:\n            # Format as instruction-following examples\n            training_data.append({\n                \"instruction\": entry[\"command\"],\n                \"output\": {\n                    \"actions\": entry[\"plan\"].actions,\n                    \"reasoning\": entry[\"plan\"].reasoning\n                },\n                \"feedback\": {\n                    \"reward\": entry[\"feedback\"].reward_score,\n                    \"success\": entry[\"success\"]\n                }\n            })\n        \n        with open(filepath, 'w') as f:\n            json.dump(training_data, f, indent=2)\n        \n        print(f\"‚úì Exported {len(training_data)} training examples to {filepath}\")\n    \n    def get_performance_summary(self) -> Dict:\n        \"\"\"Get summary statistics\"\"\"\n        if not self.execution_history:\n            return {\"total\": 0}\n        \n        rewards = [e[\"feedback\"].reward_score for e in self.execution_history]\n        safety_scores = [e[\"feedback\"].safety_score for e in self.execution_history]\n        \n        return {\n            \"total_commands\": self.total_commands,\n            \"successful\": self.success_count,\n            \"success_rate\": self.success_count / self.total_commands if self.total_commands > 0 else 0,\n            \"avg_reward\": np.mean(rewards),\n            \"avg_safety\": np.mean(safety_scores),\n            \"total_actions_executed\": sum(len(e[\"plan\"].actions) for e in self.execution_history)\n        }\n\n# Initialize complete LLM-based control system\nprint(\"\\n\" + \"=\"*70)\nprint(\"Initializing Complete LLM-Based Robot Control System\")\nprint(\"=\"*70 + \"\\n\")\n\nllm_robot_controller = LLMBasedRobotController(\n    knowledge_base=knowledge_base if RAG_AVAILABLE else None,\n    llm_agent=llm_controller,\n    rlhf_model=rlhf_reward_model,\n    robot_interface=None  # Use simulation (set to G1RobotInterface for real robot)\n)\n\nprint(\"\\n‚úì System ready for commands!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Complete LLM-Based Control System\n\nNow let's integrate all components into a complete system that:\n1. Accepts natural language commands\n2. Retrieves relevant context from documentation (RAG)\n3. Plans action sequences (LLM Agent)\n4. Executes actions on robot (existing RL policy)\n5. Evaluates performance (RLHF Reward Model)\n6. Learns from feedback over time",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "@dataclass\nclass RewardFeedback:\n    \"\"\"Feedback from RLHF reward model\"\"\"\n    reward_score: float  # -1.0 to 1.0\n    safety_score: float\n    efficiency_score: float\n    goal_progress_score: float\n    naturalness_score: float\n    feedback_text: str\n    improvement_suggestions: List[str]\n\nclass RLHFRewardModel:\n    \"\"\"\n    LLM-based reward model for Reinforcement Learning from Human Feedback\n    \n    Provides learned rewards based on:\n    - Safety assessment\n    - Efficiency evaluation\n    - Goal progress estimation\n    - Motion naturalness\n    \"\"\"\n    \n    def __init__(self, \n                 model_name: str = \"meta-llama/Llama-3.1-70B-Instruct\",\n                 enable_feedback_collection: bool = True):\n        \"\"\"\n        Initialize RLHF reward model\n        \n        Args:\n            model_name: HuggingFace model for reward evaluation\n            enable_feedback_collection: Whether to store feedback for fine-tuning\n        \"\"\"\n        self.model_name = model_name\n        self.enable_feedback_collection = enable_feedback_collection\n        self.feedback_history = []\n        \n        # Initialize client\n        if HF_AVAILABLE:\n            hf_token = os.environ.get(\"HUGGINGFACE_TOKEN\")\n            if hf_token:\n                self.client = InferenceClient(token=hf_token)\n                print(f\"‚úì RLHF Reward Model initialized with {model_name}\")\n            else:\n                print(\"‚ö†Ô∏è  HUGGINGFACE_TOKEN not set\")\n                self.client = None\n        else:\n            self.client = None\n            print(\"‚ö†Ô∏è  HuggingFace not available\")\n        \n        # Reward model system prompt\n        self.system_prompt = \"\"\"You are an expert evaluator for humanoid robot actions.\n\nYour task is to assess the quality of robot action sequences based on multiple criteria.\n\nEvaluation criteria:\n1. Safety (0-1): Risk of falling, collision, or hardware damage\n2. Efficiency (0-1): Energy usage, time to complete, smoothness\n3. Goal Progress (0-1): How much the action advances toward the objective\n4. Naturalness (0-1): How human-like and graceful the motion appears\n\nFor each action sequence, provide:\n- Individual scores (0-1) for each criterion\n- Overall reward score (-1 to +1)\n- Textual feedback explaining your evaluation\n- Specific improvement suggestions\n\nOutput format (JSON):\n{\n    \"safety_score\": 0.0-1.0,\n    \"efficiency_score\": 0.0-1.0,\n    \"goal_progress_score\": 0.0-1.0,\n    \"naturalness_score\": 0.0-1.0,\n    \"reward_score\": -1.0 to +1.0,\n    \"feedback_text\": \"explanation\",\n    \"improvement_suggestions\": [\"suggestion1\", \"suggestion2\"]\n}\n\nBe strict but fair. Penalize unsafe actions heavily.\n\"\"\"\n    \n    def evaluate_action_sequence(self,\n                                 command: str,\n                                 actions: List[str],\n                                 robot_state_before: RobotState,\n                                 robot_state_after: RobotState,\n                                 execution_success: bool = True) -> RewardFeedback:\n        \"\"\"\n        Evaluate an executed action sequence\n        \n        Args:\n            command: Original natural language command\n            actions: Sequence of actions executed\n            robot_state_before: State before execution\n            robot_state_after: State after execution\n            execution_success: Whether execution completed without errors\n        \n        Returns:\n            reward_feedback: Detailed evaluation and reward\n        \"\"\"\n        if self.client is None:\n            # Fallback: simple heuristic reward\n            return self._fallback_reward(actions, robot_state_before, robot_state_after, execution_success)\n        \n        # Build evaluation prompt\n        state_delta = self._compute_state_delta(robot_state_before, robot_state_after)\n        \n        user_prompt = f\"\"\"Evaluate this robot action sequence:\n\nCommand: \"{command}\"\nActions executed: {' ‚Üí '.join(actions)}\nExecution status: {\"Success\" if execution_success else \"Failed\"}\n\nState changes:\n- Goal distance: {robot_state_before.goal_distance:.2f}m ‚Üí {robot_state_after.goal_distance:.2f}m (Œî: {state_delta['goal_progress']:.2f}m)\n- Orientation change: {np.rad2deg(state_delta['orientation_change']):.1f}¬∞\n- Base tilt: {np.rad2deg(robot_state_after.base_orientation[:2]):.1f}¬∞\n- Obstacle proximity: {np.min(robot_state_after.obstacle_proximity):.2f}m\n\nProvide your evaluation:\"\"\"\n        \n        try:\n            response = self.client.chat_completion(\n                messages=[\n                    {\"role\": \"system\", \"content\": self.system_prompt},\n                    {\"role\": \"user\", \"content\": user_prompt}\n                ],\n                model=self.model_name,\n                max_tokens=400,\n                temperature=0.2\n            )\n            \n            # Parse response\n            response_text = response.choices[0].message.content\n            \n            # Extract JSON\n            json_match = re.search(r'```json\\n(.*?)\\n```', response_text, re.DOTALL)\n            if json_match:\n                response_text = json_match.group(1)\n            elif '```' in response_text:\n                json_match = re.search(r'```\\n(.*?)\\n```', response_text, re.DOTALL)\n                if json_match:\n                    response_text = json_match.group(1)\n            \n            eval_data = json.loads(response_text)\n            \n            feedback = RewardFeedback(\n                reward_score=eval_data.get(\"reward_score\", 0.0),\n                safety_score=eval_data.get(\"safety_score\", 0.5),\n                efficiency_score=eval_data.get(\"efficiency_score\", 0.5),\n                goal_progress_score=eval_data.get(\"goal_progress_score\", 0.5),\n                naturalness_score=eval_data.get(\"naturalness_score\", 0.5),\n                feedback_text=eval_data.get(\"feedback_text\", \"No feedback\"),\n                improvement_suggestions=eval_data.get(\"improvement_suggestions\", [])\n            )\n            \n            # Store for potential fine-tuning\n            if self.enable_feedback_collection:\n                self.feedback_history.append({\n                    \"command\": command,\n                    \"actions\": actions,\n                    \"state_delta\": state_delta,\n                    \"feedback\": feedback,\n                    \"timestamp\": datetime.now().isoformat()\n                })\n            \n            print(f\"\\nüìä RLHF Evaluation:\")\n            print(f\"   Overall Reward: {feedback.reward_score:+.3f}\")\n            print(f\"   Safety: {feedback.safety_score:.2f} | Efficiency: {feedback.efficiency_score:.2f}\")\n            print(f\"   Goal Progress: {feedback.goal_progress_score:.2f} | Naturalness: {feedback.naturalness_score:.2f}\")\n            print(f\"   Feedback: {feedback.feedback_text}\")\n            \n            return feedback\n            \n        except Exception as e:\n            print(f\"‚ö†Ô∏è  RLHF evaluation failed: {e}\")\n            print(\"   Using fallback reward\")\n            return self._fallback_reward(actions, robot_state_before, robot_state_after, execution_success)\n    \n    def _compute_state_delta(self, state_before: RobotState, state_after: RobotState) -> Dict:\n        \"\"\"Compute key state changes\"\"\"\n        return {\n            \"goal_progress\": state_before.goal_distance - state_after.goal_distance,\n            \"orientation_change\": np.linalg.norm(state_after.base_orientation - state_before.base_orientation),\n            \"velocity_change\": np.linalg.norm(state_after.base_linear_velocity - state_before.base_linear_velocity),\n            \"stability_change\": np.abs(state_after.base_orientation[:2]).mean() - np.abs(state_before.base_orientation[:2]).mean()\n        }\n    \n    def _fallback_reward(self, actions: List[str], \n                        state_before: RobotState, \n                        state_after: RobotState,\n                        execution_success: bool) -> RewardFeedback:\n        \"\"\"Simple heuristic reward when LLM unavailable\"\"\"\n        \n        # Safety: penalize large tilts\n        tilt = np.abs(state_after.base_orientation[:2]).max()\n        safety_score = max(0, 1.0 - tilt / 0.5)  # 0.5 rad = ~30 degrees\n        \n        # Efficiency: penalize many actions\n        efficiency_score = max(0, 1.0 - len(actions) / 10.0)\n        \n        # Goal progress\n        progress = state_before.goal_distance - state_after.goal_distance\n        goal_progress_score = np.clip(progress / 1.0, -1, 1)  # Normalize by 1m\n        \n        # Naturalness: constant (can't assess without LLM)\n        naturalness_score = 0.7\n        \n        # Overall reward (weighted average)\n        reward_score = (\n            0.4 * safety_score +\n            0.2 * efficiency_score +\n            0.3 * goal_progress_score +\n            0.1 * naturalness_score\n        ) * (1.0 if execution_success else 0.5)  # Halve reward if failed\n        \n        return RewardFeedback(\n            reward_score=reward_score,\n            safety_score=safety_score,\n            efficiency_score=efficiency_score,\n            goal_progress_score=goal_progress_score,\n            naturalness_score=naturalness_score,\n            feedback_text=\"Heuristic reward (LLM unavailable)\",\n            improvement_suggestions=[]\n        )\n    \n    def export_feedback_dataset(self, filepath: str):\n        \"\"\"Export collected feedback for fine-tuning reward model\"\"\"\n        if not self.feedback_history:\n            print(\"‚ö†Ô∏è  No feedback collected yet\")\n            return\n        \n        with open(filepath, 'w') as f:\n            json.dump(self.feedback_history, f, indent=2)\n        \n        print(f\"‚úì Exported {len(self.feedback_history)} feedback samples to {filepath}\")\n\n# Initialize RLHF reward model\nrlhf_reward_model = RLHFRewardModel(\n    model_name=\"meta-llama/Llama-3.1-70B-Instruct\",\n    enable_feedback_collection=True\n)\n\n# Test reward evaluation\nprint(\"\\n\" + \"=\"*60)\nprint(\"Testing RLHF Reward Model\")\nprint(\"=\"*60)\n\n# Simulate execution\ntest_command = \"Walk forward and wave\"\ntest_actions = [\"walk_forward\", \"idle\", \"wave\"]\n\nstate_before = RobotState(\n    base_orientation=np.array([0.05, 0.02, 0.0]),\n    base_angular_velocity=np.zeros(3),\n    base_linear_velocity=np.zeros(3),\n    joint_positions=np.zeros(29),\n    joint_velocities=np.zeros(29),\n    foot_contacts=np.array([1, 1]),\n    goal_direction=np.array([1, 0, 0]),\n    goal_distance=5.0,\n    obstacle_proximity=np.ones(8) * 10.0,\n    cmd_velocity=np.zeros(3),\n    cmd_mode=1,\n    cmd_gesture=0\n)\n\nstate_after = RobotState(\n    base_orientation=np.array([0.03, 0.01, 0.0]),\n    base_angular_velocity=np.zeros(3),\n    base_linear_velocity=np.zeros(3),\n    joint_positions=np.zeros(29),\n    joint_velocities=np.zeros(29),\n    foot_contacts=np.array([1, 1]),\n    goal_direction=np.array([1, 0, 0]),\n    goal_distance=4.2,  # Made progress\n    obstacle_proximity=np.ones(8) * 10.0,\n    cmd_velocity=np.zeros(3),\n    cmd_mode=1,\n    cmd_gesture=0\n)\n\nfeedback = rlhf_reward_model.evaluate_action_sequence(\n    command=test_command,\n    actions=test_actions,\n    robot_state_before=state_before,\n    robot_state_after=state_after,\n    execution_success=True\n)\n\nprint(f\"\\nSuggestions:\")\nfor i, suggestion in enumerate(feedback.improvement_suggestions):\n    print(f\"  {i+1}. {suggestion}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### RLHF Reward Model\n\nThe reward model is a separate LLM that evaluates the quality of actions based on:\n- **Safety:** No dangerous movements or instability risks\n- **Efficiency:** Minimal energy consumption and time\n- **Goal Achievement:** How well the action advances toward the objective\n- **Naturalness:** How human-like and smooth the motion is\n\nThis provides a learned reward signal that complements the hand-crafted rewards from Section 5.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "@dataclass\nclass ActionPlan:\n    \"\"\"Planned action sequence from LLM\"\"\"\n    actions: List[str]\n    reasoning: str\n    safety_check: bool\n    estimated_duration: float\n    confidence: float\n\nclass LLMAgentController:\n    \"\"\"\n    LLM-based high-level controller for natural language robot control\n    \n    Uses HuggingFace inference API for:\n    - Task understanding and decomposition\n    - Action sequence planning\n    - Safety validation\n    \"\"\"\n    \n    def __init__(self, \n                 model_name: str = \"meta-llama/Llama-3.1-70B-Instruct\",\n                 knowledge_base: Optional[RobotKnowledgeBase] = None,\n                 use_rag: bool = True):\n        \"\"\"\n        Initialize LLM controller\n        \n        Args:\n            model_name: HuggingFace model ID (alternatives: mistralai/Mixtral-8x7B-Instruct-v0.1,\n                       microsoft/Phi-3-medium-4k-instruct)\n            knowledge_base: RAG knowledge base for context retrieval\n            use_rag: Whether to use RAG for enhanced context\n        \"\"\"\n        self.model_name = model_name\n        self.knowledge_base = knowledge_base\n        self.use_rag = use_rag and knowledge_base is not None\n        \n        # Initialize HuggingFace client\n        if HF_AVAILABLE:\n            hf_token = os.environ.get(\"HUGGINGFACE_TOKEN\")\n            if hf_token:\n                self.client = InferenceClient(token=hf_token)\n                print(f\"‚úì LLM Controller initialized with {model_name}\")\n            else:\n                print(\"‚ö†Ô∏è  HUGGINGFACE_TOKEN not set. Set it in environment or notebook.\")\n                print(\"   Get token from: https://huggingface.co/settings/tokens\")\n                self.client = None\n        else:\n            self.client = None\n            print(\"‚ö†Ô∏è  HuggingFace not available\")\n        \n        # Available actions (from our RL system)\n        self.available_actions = [\n            \"idle\", \"walk_forward\", \"walk_backward\", \n            \"strafe_left\", \"strafe_right\",\n            \"rotate_left\", \"rotate_right\",\n            \"wave\", \"handshake\", \"clap\"\n        ]\n        \n        # System prompt for robot control\n        self.system_prompt = f\"\"\"You are an expert robot control assistant for the Unitree G1 humanoid robot.\n\nYour task is to translate natural language commands into safe, executable action sequences.\n\nAvailable actions: {', '.join(self.available_actions)}\n\nFor each command, you must:\n1. Understand the user's intent\n2. Decompose into a sequence of primitive actions\n3. Validate safety (no dangerous movements, respect limits)\n4. Provide clear reasoning for your plan\n\nOutput format (JSON):\n{{\n    \"actions\": [\"action1\", \"action2\", ...],\n    \"reasoning\": \"explanation of your plan\",\n    \"safety_check\": true/false,\n    \"estimated_duration\": seconds,\n    \"confidence\": 0.0-1.0\n}}\n\nSafety rules:\n- Never suggest actions that could cause falling or collision\n- Respect velocity limits: vx[-0.5,0.8], vy[-0.4,0.4], vyaw[-1.0,1.0] m/s\n- Always check for obstacles before movement\n- Include 'idle' action to stop safely between major transitions\n\"\"\"\n    \n    def plan_actions(self, command: str, \n                    robot_state: Optional[RobotState] = None) -> ActionPlan:\n        \"\"\"\n        Plan action sequence from natural language command\n        \n        Args:\n            command: Natural language instruction\n            robot_state: Current robot state (optional, for context)\n        \n        Returns:\n            action_plan: Planned actions with reasoning\n        \"\"\"\n        if self.client is None:\n            # Fallback: simple keyword-based parsing\n            return self._fallback_planner(command)\n        \n        # Build context with RAG\n        context = \"\"\n        if self.use_rag:\n            relevant_docs = self.knowledge_base.retrieve_relevant_context(command, k=3)\n            if relevant_docs:\n                context = \"\\n\\nRelevant documentation:\\n\"\n                for doc in relevant_docs:\n                    context += f\"- [{doc['type']}] {doc['text'][:200]}...\\n\"\n        \n        # Build user prompt\n        state_info = \"\"\n        if robot_state:\n            state_info = f\"\"\"\nCurrent robot state:\n- Position relative to goal: {robot_state.goal_distance:.2f}m\n- Orientation: {np.rad2deg(robot_state.base_orientation).tolist()}¬∞\n- Obstacles nearby: {np.min(robot_state.obstacle_proximity):.2f}m\n\"\"\"\n        \n        user_prompt = f\"\"\"Command: {command}\n{state_info}\n{context}\n\nPlan the action sequence:\"\"\"\n        \n        # Query LLM\n        try:\n            response = self.client.chat_completion(\n                messages=[\n                    {\"role\": \"system\", \"content\": self.system_prompt},\n                    {\"role\": \"user\", \"content\": user_prompt}\n                ],\n                model=self.model_name,\n                max_tokens=500,\n                temperature=0.3  # Lower temperature for more deterministic planning\n            )\n            \n            # Parse response\n            response_text = response.choices[0].message.content\n            \n            # Extract JSON (handle markdown code blocks)\n            json_match = re.search(r'```json\\n(.*?)\\n```', response_text, re.DOTALL)\n            if json_match:\n                response_text = json_match.group(1)\n            elif '```' in response_text:\n                # Try without json tag\n                json_match = re.search(r'```\\n(.*?)\\n```', response_text, re.DOTALL)\n                if json_match:\n                    response_text = json_match.group(1)\n            \n            plan_data = json.loads(response_text)\n            \n            # Validate actions\n            valid_actions = [a for a in plan_data[\"actions\"] if a in self.available_actions]\n            \n            action_plan = ActionPlan(\n                actions=valid_actions,\n                reasoning=plan_data.get(\"reasoning\", \"No reasoning provided\"),\n                safety_check=plan_data.get(\"safety_check\", False),\n                estimated_duration=plan_data.get(\"estimated_duration\", len(valid_actions) * 2.0),\n                confidence=plan_data.get(\"confidence\", 0.5)\n            )\n            \n            print(f\"\\nü§ñ LLM Plan:\")\n            print(f\"   Actions: {' ‚Üí '.join(action_plan.actions)}\")\n            print(f\"   Reasoning: {action_plan.reasoning}\")\n            print(f\"   Safety: {'‚úì' if action_plan.safety_check else '‚úó'}\")\n            print(f\"   Confidence: {action_plan.confidence:.2f}\")\n            \n            return action_plan\n            \n        except Exception as e:\n            print(f\"‚ö†Ô∏è  LLM planning failed: {e}\")\n            print(\"   Falling back to rule-based planner\")\n            return self._fallback_planner(command)\n    \n    def _fallback_planner(self, command: str) -> ActionPlan:\n        \"\"\"\n        Simple keyword-based fallback planner\n        (used when LLM unavailable or fails)\n        \"\"\"\n        command_lower = command.lower()\n        actions = []\n        \n        # Pattern matching\n        if \"wave\" in command_lower or \"hello\" in command_lower:\n            actions = [\"wave\"]\n        elif \"handshake\" in command_lower or \"shake\" in command_lower:\n            actions = [\"handshake\"]\n        elif \"clap\" in command_lower:\n            actions = [\"clap\"]\n        elif \"forward\" in command_lower:\n            actions = [\"walk_forward\", \"idle\"]\n        elif \"backward\" in command_lower or \"back\" in command_lower:\n            actions = [\"walk_backward\", \"idle\"]\n        elif \"left\" in command_lower and \"turn\" in command_lower:\n            actions = [\"rotate_left\", \"idle\"]\n        elif \"right\" in command_lower and \"turn\" in command_lower:\n            actions = [\"rotate_right\", \"idle\"]\n        else:\n            actions = [\"idle\"]  # Default: do nothing if unclear\n        \n        return ActionPlan(\n            actions=actions,\n            reasoning=\"Fallback rule-based matching\",\n            safety_check=True,\n            estimated_duration=len(actions) * 2.0,\n            confidence=0.3\n        )\n\n# Initialize LLM controller\nllm_controller = LLMAgentController(\n    model_name=\"meta-llama/Llama-3.1-70B-Instruct\",  # Or use smaller model\n    knowledge_base=knowledge_base if RAG_AVAILABLE else None,\n    use_rag=True\n)\n\n# Test planning (will use fallback if HF token not set)\ntest_commands = [\n    \"Walk forward and wave hello\",\n    \"Turn around and return\",\n    \"Greet the visitor with a handshake\"\n]\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"Testing LLM Action Planning\")\nprint(\"=\"*60)\n\nfor cmd in test_commands:\n    print(f\"\\nüìù Command: \\\"{cmd}\\\"\")\n    plan = llm_controller.plan_actions(cmd)\n    print()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### LLM Agent Controller with HuggingFace\n\nThe LLM Agent translates natural language commands into actionable robot instructions using HuggingFace's inference API.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "class RobotKnowledgeBase:\n    \"\"\"\n    RAG-based knowledge retrieval system for robot control\n    \n    Stores and retrieves relevant documentation, code examples,\n    and past execution logs to assist LLM decision-making.\n    \"\"\"\n    \n    def __init__(self, persist_directory: str = \"./robot_knowledge_db\"):\n        self.persist_directory = persist_directory\n        self.embeddings = None\n        self.vectorstore = None\n        self.documents = []\n        \n        if RAG_AVAILABLE:\n            # Initialize embedding model\n            self.embeddings = HuggingFaceEmbeddings(\n                model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n            )\n            \n            # Initialize vector database\n            self._load_or_create_vectorstore()\n            \n            print(\"‚úì Knowledge base initialized\")\n        else:\n            print(\"‚ö†Ô∏è  RAG not available - knowledge base disabled\")\n    \n    def _load_or_create_vectorstore(self):\n        \"\"\"Load existing vectorstore or create new one\"\"\"\n        try:\n            self.vectorstore = Chroma(\n                persist_directory=self.persist_directory,\n                embedding_function=self.embeddings\n            )\n            print(f\"  Loaded {self.vectorstore._collection.count()} documents\")\n        except:\n            self.vectorstore = Chroma(\n                persist_directory=self.persist_directory,\n                embedding_function=self.embeddings\n            )\n            print(\"  Created new vectorstore\")\n    \n    def add_manual_documentation(self, text: str, source: str):\n        \"\"\"\n        Add documentation from user manuals\n        \n        Args:\n            text: Documentation content\n            source: Source identifier (e.g., \"G1_UserManual_v2.1\")\n        \"\"\"\n        if not RAG_AVAILABLE:\n            return\n        \n        # Split into chunks\n        text_splitter = RecursiveCharacterTextSplitter(\n            chunk_size=500,\n            chunk_overlap=50\n        )\n        chunks = text_splitter.split_text(text)\n        \n        # Add metadata\n        metadatas = [{\"source\": source, \"type\": \"manual\"} for _ in chunks]\n        \n        # Add to vectorstore\n        self.vectorstore.add_texts(texts=chunks, metadatas=metadatas)\n        self.vectorstore.persist()\n        \n        print(f\"‚úì Added {len(chunks)} chunks from {source}\")\n    \n    def add_github_code(self, code: str, repo: str, file_path: str):\n        \"\"\"\n        Add code examples from GitHub repositories\n        \n        Args:\n            code: Source code content\n            repo: Repository name\n            file_path: File path in repo\n        \"\"\"\n        if not RAG_AVAILABLE:\n            return\n        \n        # Split code into logical chunks (functions, classes)\n        chunks = self._split_code(code)\n        \n        # Add metadata\n        metadatas = [{\n            \"source\": f\"{repo}/{file_path}\",\n            \"type\": \"code\",\n            \"repo\": repo\n        } for _ in chunks]\n        \n        # Add to vectorstore\n        self.vectorstore.add_texts(texts=chunks, metadatas=metadatas)\n        self.vectorstore.persist()\n        \n        print(f\"‚úì Added {len(chunks)} code snippets from {repo}/{file_path}\")\n    \n    def add_api_documentation(self, api_docs: Dict[str, str]):\n        \"\"\"\n        Add SDK API documentation\n        \n        Args:\n            api_docs: Dictionary of {function_name: documentation}\n        \"\"\"\n        if not RAG_AVAILABLE:\n            return\n        \n        texts = []\n        metadatas = []\n        \n        for func_name, doc in api_docs.items():\n            texts.append(f\"Function: {func_name}\\n\\n{doc}\")\n            metadatas.append({\n                \"source\": \"SDK_API_Reference\",\n                \"type\": \"api\",\n                \"function\": func_name\n            })\n        \n        self.vectorstore.add_texts(texts=texts, metadatas=metadatas)\n        self.vectorstore.persist()\n        \n        print(f\"‚úì Added {len(texts)} API documentation entries\")\n    \n    def add_execution_log(self, command: str, actions: List[str], \n                         outcome: str, success: bool):\n        \"\"\"\n        Log a past execution for future reference\n        \n        Args:\n            command: Natural language command\n            actions: Sequence of actions taken\n            outcome: Result description\n            success: Whether execution succeeded\n        \"\"\"\n        if not RAG_AVAILABLE:\n            return\n        \n        log_text = f\"\"\"\nCommand: {command}\nActions: {' -> '.join(actions)}\nOutcome: {outcome}\nSuccess: {success}\nTimestamp: {datetime.now().isoformat()}\n\"\"\"\n        \n        metadata = {\n            \"source\": \"execution_log\",\n            \"type\": \"execution\",\n            \"success\": success,\n            \"timestamp\": datetime.now().isoformat()\n        }\n        \n        self.vectorstore.add_texts(texts=[log_text], metadatas=[metadata])\n        self.vectorstore.persist()\n    \n    def retrieve_relevant_context(self, query: str, k: int = 5) -> List[Dict[str, str]]:\n        \"\"\"\n        Retrieve most relevant documentation for a query\n        \n        Args:\n            query: Search query (e.g., user command)\n            k: Number of results to return\n        \n        Returns:\n            contexts: List of {text, source, type} dictionaries\n        \"\"\"\n        if not RAG_AVAILABLE or self.vectorstore is None:\n            return []\n        \n        # Similarity search\n        results = self.vectorstore.similarity_search_with_score(query, k=k)\n        \n        contexts = []\n        for doc, score in results:\n            contexts.append({\n                \"text\": doc.page_content,\n                \"source\": doc.metadata.get(\"source\", \"unknown\"),\n                \"type\": doc.metadata.get(\"type\", \"unknown\"),\n                \"relevance_score\": float(1 - score)  # Convert distance to similarity\n            })\n        \n        return contexts\n    \n    def _split_code(self, code: str) -> List[str]:\n        \"\"\"Split code into logical chunks (functions/classes)\"\"\"\n        # Simple regex-based splitting\n        # In production, use proper AST parsing\n        chunks = []\n        \n        # Split by function definitions\n        pattern = r'(def\\s+\\w+.*?(?=\\ndef\\s+|\\nclass\\s+|\\Z))'\n        matches = re.findall(pattern, code, re.DOTALL)\n        \n        if matches:\n            chunks.extend(matches)\n        else:\n            # Fallback: split by lines\n            lines = code.split('\\n')\n            chunk_size = 20\n            for i in range(0, len(lines), chunk_size):\n                chunks.append('\\n'.join(lines[i:i+chunk_size]))\n        \n        return [c.strip() for c in chunks if c.strip()]\n\n# Initialize knowledge base\nknowledge_base = RobotKnowledgeBase()\n\n# Add example documentation\nif RAG_AVAILABLE:\n    # Example: User manual excerpt\n    manual_text = \"\"\"\n    Unitree G1 Locomotion Control:\n    \n    The G1 robot supports multiple locomotion modes:\n    - Stand: Robot maintains upright posture with damping\n    - Walk: Controlled walking with velocity commands (vx, vy, vyaw)\n    - Balance: Active balance control for external disturbances\n    \n    Velocity Commands:\n    - vx: Forward/backward velocity in m/s (range: -0.5 to 0.8)\n    - vy: Left/right strafe velocity in m/s (range: -0.4 to 0.4)\n    - vyaw: Rotation velocity in rad/s (range: -1.0 to 1.0)\n    \n    Safety Limits:\n    - Maximum pitch/roll angle: 30 degrees\n    - Emergency stop: Available via controller or software command\n    - Obstacle detection: Uses depth cameras with 0.3m minimum distance\n    \"\"\"\n    \n    knowledge_base.add_manual_documentation(manual_text, \"G1_UserManual_v2.1\")\n    \n    # Example: API documentation\n    api_docs = {\n        \"Move(vx, vy, vyaw)\": \"Send velocity command to robot. Returns success status.\",\n        \"StandUp()\": \"Transition from sitting to standing posture. Duration ~3 seconds.\",\n        \"Damp()\": \"Enter damping mode (emergency stop). Robot becomes compliant.\",\n        \"ExecuteGesture(gesture_id)\": \"Perform pre-defined arm gesture. IDs: 0=wave, 1=handshake, 2=clap\"\n    }\n    \n    knowledge_base.add_api_documentation(api_docs)\n    \n    # Example: Code snippet\n    code_example = \"\"\"\ndef navigate_to_goal(robot, goal_position):\n    '''Navigate robot to goal using simple proportional control'''\n    current_pos = robot.get_position()\n    while distance(current_pos, goal_position) > 0.5:\n        # Calculate direction\n        dx = goal_position[0] - current_pos[0]\n        dy = goal_position[1] - current_pos[1]\n        \n        # Send velocity command\n        robot.Move(dx * 0.3, dy * 0.3, 0.0)\n        time.sleep(0.1)\n        current_pos = robot.get_position()\n    \n    robot.Move(0, 0, 0)  # Stop at goal\n\"\"\"\n    \n    knowledge_base.add_github_code(code_example, \"unitree/examples\", \"navigation.py\")\n    \n    # Example: Past execution\n    knowledge_base.add_execution_log(\n        command=\"Walk forward and wave\",\n        actions=[\"walk_forward\", \"stop\", \"wave\"],\n        outcome=\"Successfully completed greeting gesture\",\n        success=True\n    )\n    \n    print(\"\\n‚úì Example knowledge added to database\")\n\n# Test retrieval\nif RAG_AVAILABLE:\n    print(\"\\nTesting knowledge retrieval...\")\n    query = \"How do I make the robot wave?\"\n    contexts = knowledge_base.retrieve_relevant_context(query, k=3)\n    \n    print(f\"\\nQuery: '{query}'\")\n    print(f\"Retrieved {len(contexts)} relevant contexts:\\n\")\n    for i, ctx in enumerate(contexts):\n        print(f\"{i+1}. [{ctx['type']}] {ctx['source']}\")\n        print(f\"   Relevance: {ctx['relevance_score']:.3f}\")\n        print(f\"   {ctx['text'][:100]}...\")\n        print()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### RAG System for Robot Documentation\n\nThe RAG (Retrieval-Augmented Generation) system provides the LLM with contextual knowledge from:\n- **User Manuals:** Unitree G1 operation guides\n- **GitHub Repositories:** Code examples and best practices\n- **API Documentation:** SDK function references\n- **Past Executions:** Successful task completions and failure cases",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Install LLM and RAG dependencies\n!pip install huggingface_hub transformers sentence-transformers faiss-cpu langchain chromadb\n!pip install openai  # For alternative API access\n\n# Set your HuggingFace token (get from https://huggingface.co/settings/tokens)\n# You can use inference API (free tier) or serverless endpoints\nimport os\n# os.environ[\"HUGGINGFACE_TOKEN\"] = \"hf_xxxxxxxxxxxxxxxxxxxxx\"  # Uncomment and add your token\n\n# Imports for LLM control\ntry:\n    from huggingface_hub import InferenceClient\n    import transformers\n    HF_AVAILABLE = True\nexcept ImportError:\n    print(\"Warning: HuggingFace libraries not available\")\n    HF_AVAILABLE = False\n\ntry:\n    from sentence_transformers import SentenceTransformer\n    from langchain.text_splitter import RecursiveCharacterTextSplitter\n    from langchain.vectorstores import Chroma\n    from langchain.embeddings import HuggingFaceEmbeddings\n    RAG_AVAILABLE = True\nexcept ImportError:\n    print(\"Warning: RAG libraries not available\")\n    RAG_AVAILABLE = False\n\nimport re\nfrom typing import List, Dict, Tuple, Optional\nfrom dataclasses import dataclass, field\nfrom datetime import datetime\n\nprint(f\"‚úì LLM Setup Complete\")\nprint(f\"  - HuggingFace: {HF_AVAILABLE}\")\nprint(f\"  - RAG System: {RAG_AVAILABLE}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### LLM Dependencies & Setup",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "---\n<a id='llm-control'></a>\n## 10. LLM-Based Agentic Control with RLHF\n\nThis section implements a **language model-based controller** that enables high-level task planning and execution through natural language. The system combines:\n\n1. **LLM Agent Controller:** Uses HuggingFace models to interpret commands and plan actions\n2. **RLHF Reward Model:** Secondary model evaluates action quality for reward shaping\n3. **RAG System:** Retrieval-Augmented Generation from robot documentation, manuals, and repositories\n4. **Fine-tuned Commands:** Domain-specific instruction tuning for robot control\n\n### Architecture Overview\n\n```\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ                    LLM-Based Control System                     ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ                                                                 ‚îÇ\n‚îÇ  Natural Language Command: \"Walk to the door and wave hello\"   ‚îÇ\n‚îÇ            ‚Üì                                                    ‚îÇ\n‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îÇ\n‚îÇ  ‚îÇ  LLM Agent Controller (HuggingFace Inference API)     ‚îÇ     ‚îÇ\n‚îÇ  ‚îÇ  - Task decomposition                                 ‚îÇ     ‚îÇ\n‚îÇ  ‚îÇ  - Action sequence planning                           ‚îÇ     ‚îÇ\n‚îÇ  ‚îÇ  - Safety validation                                  ‚îÇ     ‚îÇ\n‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îÇ\n‚îÇ                   ‚îÇ                                             ‚îÇ\n‚îÇ         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                                  ‚îÇ\n‚îÇ         ‚îÇ  RAG System        ‚îÇ                                  ‚îÇ\n‚îÇ         ‚îÇ  - User manuals    ‚îÇ                                  ‚îÇ\n‚îÇ         ‚îÇ  - GitHub repos    ‚îÇ                                  ‚îÇ\n‚îÇ         ‚îÇ  - API docs        ‚îÇ                                  ‚îÇ\n‚îÇ         ‚îÇ  - Past executions ‚îÇ                                  ‚îÇ\n‚îÇ         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                                  ‚îÇ\n‚îÇ                   ‚îÇ                                             ‚îÇ\n‚îÇ         Action Sequence: [walk_forward, stop, wave]            ‚îÇ\n‚îÇ            ‚Üì                                                    ‚îÇ\n‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îÇ\n‚îÇ  ‚îÇ  RLHF Reward Model (Evaluator LLM)                   ‚îÇ     ‚îÇ\n‚îÇ  ‚îÇ  - Evaluates action appropriateness                  ‚îÇ     ‚îÇ\n‚îÇ  ‚îÇ  - Scores safety and efficiency                      ‚îÇ     ‚îÇ\n‚îÇ  ‚îÇ  - Provides feedback signal                          ‚îÇ     ‚îÇ\n‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îÇ\n‚îÇ                   ‚îÇ                                             ‚îÇ\n‚îÇ         Reward Score + Feedback                                ‚îÇ\n‚îÇ            ‚Üì                                                    ‚îÇ\n‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îÇ\n‚îÇ  ‚îÇ  Motion Execution (Existing RL Policy)               ‚îÇ     ‚îÇ\n‚îÇ  ‚îÇ  - Executes primitive actions                        ‚îÇ     ‚îÇ\n‚îÇ  ‚îÇ  - Monitors state                                    ‚îÇ     ‚îÇ\n‚îÇ  ‚îÇ  - Returns outcome                                   ‚îÇ     ‚îÇ\n‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îÇ\n‚îÇ                                                                 ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n```\n\n### Benefits\n\n- **Natural Language Interface:** Non-experts can control robot with plain English\n- **Contextual Understanding:** RAG provides domain knowledge from manuals and code\n- **Adaptive Learning:** RLHF continuously improves action selection from feedback\n- **Safe Execution:** LLM validates plans before execution\n- **Explainable:** Agent provides reasoning for its decisions",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
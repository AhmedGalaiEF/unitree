{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unitree G1 Academy â€“ Course Overview\n",
    "\n",
    "**Hands-on Intensive Training**  \n",
    "**Location:** Hannover, Germany  \n",
    "**Dates:** February 2â€“5, 2026  \n",
    "**Duration:** 4 Days (09:00â€“17:00 daily)  \n",
    "**Max Participants:** 12 persons\n",
    "\n",
    "---\n",
    "\n",
    "## Target Audience\n",
    "\n",
    "This training is designed for:\n",
    "- **Engineers & Technicians** from industry, logistics, and facility management\n",
    "- **Project Managers** responsible for robotics integration\n",
    "- **Robotics Integrators** working with humanoid platforms\n",
    "- Anyone who wants to understand, program, and deploy the Unitree G1 in real-world applications\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Basic knowledge of **Linux**\n",
    "- Programming experience in **Python** or **C++**\n",
    "- Ideally familiar with **ROS 2** (Robot Operating System)\n",
    "- Focus on practical robotics applications\n",
    "\n",
    "## Course Objectives\n",
    "\n",
    "By the end of this 4-day intensive training, participants will:\n",
    "\n",
    "âœ… Understand the **hardware architecture** and software components of the Unitree G1  \n",
    "âœ… Master **locomotion control** and dynamic walking algorithms  \n",
    "âœ… Implement **navigation** systems with mapping and localization  \n",
    "âœ… Integrate **perception** systems for object and environment recognition  \n",
    "âœ… Apply knowledge to **real-world applications** in industry, logistics, and cleaning robotics  \n",
    "âœ… Receive a **Unitree G1 Academy Certificate** as proof of competence\n",
    "\n",
    "---\n",
    "\n",
    "## Methodology\n",
    "\n",
    "The course combines:\n",
    "- ðŸ“š **Technical Lectures** (theory and concepts)\n",
    "- ðŸ› ï¸ **Workshops** (hands-on exercises)\n",
    "- ðŸ¤– **Live Demonstrations** with real G1 robots\n",
    "- ðŸ‘¥ **Team Exercises** (small groups working with actual hardware)\n",
    "- ðŸ’¼ **Case Studies** from EF Robotics practical experience\n",
    "\n",
    "---\n",
    "\n",
    "## Course Materials & Notebooks\n",
    "\n",
    "This course is supported by three main Jupyter notebooks:\n",
    "\n",
    "### ðŸ“˜ Notebook 1: `unitree_g1_sdk_tutorial.ipynb`\n",
    "**Basic SDK Tutorial** - Covers fundamental operations:\n",
    "- Setup & Installation\n",
    "- Audio Control\n",
    "- High-Level Locomotion Control\n",
    "- Arm Actions & Gestures\n",
    "- Low-Level Motor Control\n",
    "- Basic Data Visualization\n",
    "\n",
    "### ðŸ“— Notebook 2: `unitree_g1_advanced_usecases.ipynb`\n",
    "**Advanced Use Cases** - Covers sophisticated applications:\n",
    "- Object Manipulation (Coffee Cup Example)\n",
    "- Reinforcement Learning with MimicKit\n",
    "- Dynamic Balance & Impedance Control\n",
    "- Vision-Based Grasping (RealSense Integration)\n",
    "- Whole-Body Motion Planning\n",
    "- Human-Robot Interaction\n",
    "- Teleoperation Interfaces\n",
    "\n",
    "### ðŸ“™ Notebook 3: Additional Topics (To Be Created)\n",
    "The following topics from the course require additional notebooks:\n",
    "- ROS 2 Integration & Architecture\n",
    "- Navigation Stack (Nav2)\n",
    "- Mapping & Localization (SLAM)\n",
    "- Path Planning & Obstacle Avoidance\n",
    "- Industrial Applications (Logistics, Cleaning)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4-Day Training Agenda\n",
    "\n",
    "Below is the detailed breakdown of each training day, with references to relevant notebook sections.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 1: Introduction & Fundamentals\n",
    "\n",
    "**Focus:** Understanding the G1 platform, hardware architecture, and getting started with the robot.\n",
    "\n",
    "---\n",
    "\n",
    "## Morning Session (09:00â€“12:00)\n",
    "\n",
    "### 1.1 Welcome & Introduction to EF Robotics\n",
    "- Introduction to instructors and participants\n",
    "- Overview of EF Robotics and company philosophy\n",
    "- Course structure and learning objectives\n",
    "- Safety briefing for working with robots\n",
    "\n",
    "### 1.2 Unitree G1 Overview\n",
    "**ðŸ“˜ Covered in:** `unitree_g1_sdk_tutorial.ipynb` - Introduction sections\n",
    "\n",
    "**Key Topics:**\n",
    "- History and evolution of Unitree robots\n",
    "- G1 specifications and variants (23-DOF vs 29-DOF)\n",
    "- Comparison with other humanoid platforms (Boston Dynamics Atlas, Agility Digit, Tesla Optimus)\n",
    "- Target markets and use cases\n",
    "\n",
    "**G1 Technical Specifications:**\n",
    "```\n",
    "Height: ~1.3m (adjustable standing height)\n",
    "Weight: ~47 kg\n",
    "DOF Options: 23-DOF or 29-DOF (with wrist pitch/yaw)\n",
    "Battery: ~2 hours runtime (varies by activity)\n",
    "Walking Speed: Up to 2 m/s\n",
    "Payload: Up to 5 kg per arm\n",
    "Sensors: IMU, force/torque sensors, optional cameras (RealSense)\n",
    "Communication: DDS (CycloneDDS), Ethernet/WiFi\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Afternoon Session (13:00â€“17:00)\n",
    "\n",
    "### 1.3 Hardware Architecture & Components\n",
    "**ðŸ“™ Requires new notebook:** Hardware Deep-Dive\n",
    "\n",
    "**Topics to Cover:**\n",
    "\n",
    "#### Mechanical Structure\n",
    "- **Kinematic Chain:** Hip-Knee-Ankle leg design for humanoid locomotion\n",
    "- **Actuators:** High-torque brushless motors with integrated planetary gearboxes\n",
    "- **Joints:** 29 total joints breakdown:\n",
    "  - 12 leg joints (6 per leg)\n",
    "  - 3 waist joints (yaw, roll, pitch)\n",
    "  - 14 arm joints (7 per arm for 29-DOF variant)\n",
    "- **Materials:** Aluminum alloy frame, carbon fiber panels, 3D-printed components\n",
    "\n",
    "#### Electronic Systems\n",
    "- **Main Computer:** Onboard PC (typically x86 or ARM-based SBC)\n",
    "- **Motor Controllers:** Distributed CAN-bus architecture\n",
    "- **Power Distribution:** 48V battery system with voltage regulators\n",
    "- **Communication:** Ethernet backbone for high-bandwidth sensor data\n",
    "\n",
    "#### Sensor Suite\n",
    "- **IMU (Inertial Measurement Unit):** 9-axis orientation tracking\n",
    "- **Joint Encoders:** Absolute position sensors in each motor\n",
    "- **Force/Torque Sensors:** Current-based torque estimation\n",
    "- **Optional Vision:** Intel RealSense D435 depth camera\n",
    "- **Optional LiDAR:** For navigation applications\n",
    "\n",
    "![G1 Hardware Architecture](./images/g1_hardware_architecture.png)\n",
    "*Figure: Unitree G1 Hardware Architecture (Placeholder)*\n",
    "\n",
    "---\n",
    "\n",
    "### 1.4 Software Components & SDK\n",
    "**ðŸ“˜ Covered in:** `unitree_g1_sdk_tutorial.ipynb` - Setup & Installation (Section 1)\n",
    "\n",
    "**Software Stack Overview:**\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚   User Applications / Custom Code      â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚   Unitree SDK2 Python / C++ API        â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚   DDS Middleware (CycloneDDS)         â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚   Robot Operating System (ROS 2)      â”‚  (Optional)\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚   Low-Level Control Firmware          â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚   Motor Controllers & Sensors         â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "**Key Software Layers:**\n",
    "\n",
    "1. **Firmware Layer:** Real-time motor control (1 kHz update rate)\n",
    "2. **DDS Middleware:** Publish-subscribe communication (CycloneDDS)\n",
    "3. **SDK Layer:** Python/C++ APIs for high-level and low-level control\n",
    "4. **ROS 2 Integration:** (Optional) for navigation, planning, and perception\n",
    "5. **Application Layer:** Custom user code\n",
    "\n",
    "**Unitree SDK2 Features:**\n",
    "- High-level motion APIs (walk, stand, gestures)\n",
    "- Low-level motor control (position, velocity, torque)\n",
    "- State subscription (IMU, joint states, battery)\n",
    "- Audio/LED control\n",
    "- Motion switching (sport mode, damping, SDK control)\n",
    "\n",
    "---\n",
    "\n",
    "### 1.5 Setup, Commissioning & Safety\n",
    "**ðŸ“˜ Covered in:** `unitree_g1_sdk_tutorial.ipynb` - Setup & Installation (Section 1)\n",
    "\n",
    "**Initial Setup Steps:**\n",
    "\n",
    "1. **Network Configuration**\n",
    "   - Connect PC to robot via Ethernet\n",
    "   - Configure static IP address (robot typically at 192.168.123.10)\n",
    "   - Test connectivity with `ping`\n",
    "\n",
    "2. **SDK Installation**\n",
    "   ```bash\n",
    "   # Install dependencies\n",
    "   pip install cyclonedds numpy opencv-python\n",
    "   \n",
    "   # Clone and install Unitree SDK2\n",
    "   git clone https://github.com/unitreerobotics/unitree_sdk2_python.git\n",
    "   cd unitree_sdk2_python\n",
    "   pip install -e .\n",
    "   ```\n",
    "\n",
    "3. **Environment Variables**\n",
    "   ```bash\n",
    "   export CYCLONEDDS_HOME=/path/to/cyclonedds\n",
    "   export UNITREE_SDK_PATH=/path/to/unitree_sdk2_python\n",
    "   ```\n",
    "\n",
    "**Safety Protocols:**\n",
    "\n",
    "âš ï¸ **Critical Safety Rules:**\n",
    "\n",
    "1. **Workspace Safety**\n",
    "   - Minimum 3m x 3m clear area around robot\n",
    "   - No obstacles within 1.5m during locomotion tests\n",
    "   - Soft flooring or mats recommended for early testing\n",
    "\n",
    "2. **Power-On Procedure**\n",
    "   - Place robot in stable standing or sitting position\n",
    "   - Turn on main power switch\n",
    "   - Wait for boot sequence (LED indicators)\n",
    "   - Verify communication before commanding motion\n",
    "\n",
    "3. **Emergency Stop**\n",
    "   - Physical E-stop button on robot (if available)\n",
    "   - Software damping mode: `robot.Damp()`\n",
    "   - Kill power as last resort (may cause fall)\n",
    "\n",
    "4. **Motion Testing Protocol**\n",
    "   - Always start with `Damp()` mode (motors relaxed)\n",
    "   - Test in simulation first when possible\n",
    "   - Use low gains (Kp/Kd) for initial tests\n",
    "   - One person as spotter during autonomous motion\n",
    "\n",
    "5. **Battery Safety**\n",
    "   - Monitor voltage (critical below 44V)\n",
    "   - Do not operate below 20% charge\n",
    "   - Charge in ventilated area\n",
    "   - Use official charger only\n",
    "\n",
    "6. **Joint Limits**\n",
    "   - Respect mechanical joint limits\n",
    "   - Waist joints may be locked on some variants\n",
    "   - Avoid rapid direction changes in low-level control\n",
    "\n",
    "**Troubleshooting Common Issues:**\n",
    "- **Communication timeout:** Check network interface name, verify robot IP\n",
    "- **CycloneDDS errors:** Ensure correct version (0.10.2), set CYCLONEDDS_HOME\n",
    "- **Robot unresponsive:** Reboot robot, check battery level\n",
    "- **Unexpected motion:** Immediately call `Damp()`, check coordinate frames\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ› ï¸ Hands-On Workshop: First Contact with G1\n",
    "\n",
    "**Objectives:**\n",
    "- Connect to the robot successfully\n",
    "- Run first example program\n",
    "- Verify all sensors are working\n",
    "- Practice emergency stop procedures\n",
    "\n",
    "**Exercise 1: Hello Robot**\n",
    "```python\n",
    "from unitree_sdk2py.g1.loco.g1_loco_client import LocoClient\n",
    "from unitree_sdk2py.core.channel import ChannelFactoryInitialize\n",
    "\n",
    "# Initialize\n",
    "ChannelFactoryInitialize(0, \"eth0\")\n",
    "robot = LocoClient()\n",
    "robot.Init()\n",
    "\n",
    "# Safe test: Wave hand\n",
    "robot.WaveHand()\n",
    "print(\"Hello from Unitree G1!\")\n",
    "```\n",
    "\n",
    "**Exercise 2: Sensor Check**\n",
    "- Read IMU data (orientation, acceleration)\n",
    "- Monitor joint positions and torques\n",
    "- Check battery voltage\n",
    "- Verify camera feed (if equipped)\n",
    "\n",
    "---\n",
    "\n",
    "## Day 1 Summary\n",
    "\n",
    "âœ… Understanding of G1 hardware architecture  \n",
    "âœ… Software stack and SDK overview  \n",
    "âœ… Successful setup and first connection  \n",
    "âœ… Safety protocols internalized  \n",
    "âœ… First hands-on experience with the robot\n",
    "\n",
    "**Homework:** Review basic Python examples, ensure personal laptop setup is complete for Day 2.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 2: Motion Control & Locomotion\n",
    "\n",
    "**Focus:** Understanding and implementing dynamic walking, stability control, and reinforcement learning approaches.\n",
    "\n",
    "---\n",
    "\n",
    "## Morning Session (09:00â€“12:00)\n",
    "\n",
    "### 2.1 Fundamentals of Bipedal Locomotion\n",
    "**ðŸ“™ Requires new notebook:** Locomotion Theory & Control\n",
    "\n",
    "**Key Concepts:**\n",
    "\n",
    "#### Walking Gait Cycle\n",
    "A walking gait consists of repeating phases:\n",
    "\n",
    "1. **Double Support:** Both feet on ground (10-20% of cycle)\n",
    "2. **Single Support:** One foot on ground, other swinging (30-40% per leg)\n",
    "3. **Swing Phase:** Foot leaves ground and moves forward\n",
    "4. **Heel Strike:** Swing foot contacts ground\n",
    "\n",
    "```\n",
    "Walking Cycle (Right Leg Perspective):\n",
    "â”œâ”€ Right Heel Strike (0%)\n",
    "â”œâ”€ Double Support (0-10%)\n",
    "â”œâ”€ Left Toe Off (10%)\n",
    "â”œâ”€ Right Single Support (10-50%)\n",
    "â”œâ”€ Right Toe Off / Left Heel Strike (50%)\n",
    "â”œâ”€ Double Support (50-60%)\n",
    "â”œâ”€ Left Single Support (60-100%)\n",
    "â””â”€ Right Heel Strike (100% = 0%)\n",
    "```\n",
    "\n",
    "#### Center of Pressure (CoP) & Zero Moment Point (ZMP)\n",
    "\n",
    "**Zero Moment Point (ZMP):** Point on the ground where the net moment of inertial and gravitational forces equals zero.\n",
    "\n",
    "**ZMP Stability Criterion:**\n",
    "- Robot is stable if ZMP lies within support polygon (convex hull of ground contact points)\n",
    "- If ZMP moves outside support polygon â†’ robot will tip over\n",
    "\n",
    "**Support Polygon:**\n",
    "- Single support: Area of one foot (~10cm x 20cm)\n",
    "- Double support: Rectangle connecting both feet\n",
    "\n",
    "![ZMP Illustration](./images/zmp_stability.png)\n",
    "*Figure: ZMP and Support Polygon (Placeholder)*\n",
    "\n",
    "---\n",
    "\n",
    "### 2.2 Dynamic Walking & Stability Control\n",
    "**ðŸ“˜ Partially covered in:** `unitree_g1_sdk_tutorial.ipynb` - Locomotion (Section 3)  \n",
    "**ðŸ“— Partially covered in:** `unitree_g1_advanced_usecases.ipynb` - Dynamic Balance (Section 4)\n",
    "\n",
    "**Control Strategies:**\n",
    "\n",
    "#### 1. Model Predictive Control (MPC)\n",
    "- Predicts robot motion over future time horizon\n",
    "- Optimizes footstep locations and timing\n",
    "- Commonly used for stable humanoid locomotion\n",
    "\n",
    "#### 2. Whole-Body Control (WBC)\n",
    "- Coordinates all joints simultaneously\n",
    "- Prioritizes tasks (e.g., balance > arm motion)\n",
    "- Uses quadratic programming (QP) to solve constraints\n",
    "\n",
    "#### 3. Virtual Model Control (VMC)\n",
    "- Models robot as inverted pendulum\n",
    "- Computes required ground reaction forces\n",
    "- Maps forces to joint torques via Jacobian\n",
    "\n",
    "**Unitree G1 Implementation:**\n",
    "The G1 uses a proprietary locomotion controller combining:\n",
    "- MPC for footstep planning\n",
    "- WBC for torque distribution\n",
    "- RL-trained policies for rough terrain adaptation\n",
    "\n",
    "**High-Level API:**\n",
    "```python\n",
    "# Simple velocity command\n",
    "robot.Move(vx=0.3, vy=0.0, vyaw=0.0)  # Walk forward at 0.3 m/s\n",
    "\n",
    "# Adjusting gait parameters (if supported)\n",
    "robot.SetGaitParams(\n",
    "    step_height=0.05,      # How high to lift feet\n",
    "    step_length=0.20,      # Stride length\n",
    "    swing_duration=0.3     # Time foot is in air\n",
    ")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 2.3 Reinforcement Learning for Locomotion\n",
    "**ðŸ“— Covered in:** `unitree_g1_advanced_usecases.ipynb` - Reinforcement Learning (Section 3)\n",
    "\n",
    "**Why RL for Legged Robots?**\n",
    "\n",
    "Traditional model-based control struggles with:\n",
    "- Model inaccuracies (friction, flexibility, backlash)\n",
    "- Unmodeled terrain (stairs, uneven ground)\n",
    "- Complex dynamics (29 DOF system)\n",
    "\n",
    "**RL Advantages:**\n",
    "- Learns from trial-and-error in simulation\n",
    "- Discovers robust control strategies\n",
    "- Adapts to terrain variations\n",
    "- Handles sensor noise and delays\n",
    "\n",
    "**Training Pipeline:**\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  IsaacGym/MuJoCo   â”‚  â† Physics Simulation (GPU-accelerated)\n",
    "â”‚  (4096 envs)       â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "           â”‚\n",
    "           â”‚ states, rewards\n",
    "           â†“\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚   PPO Algorithm     â”‚  â† Policy Optimization\n",
    "â”‚   (Neural Network)  â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "           â”‚\n",
    "           â”‚ actions\n",
    "           â†“\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  Trained Policy     â”‚  â† Deploy to Real Robot\n",
    "â”‚  (PyTorch Model)    â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "**Reward Function Design:**\n",
    "```python\n",
    "reward = (\n",
    "    w1 * velocity_tracking_reward      # Follow desired velocity\n",
    "    + w2 * orientation_reward          # Keep upright\n",
    "    + w3 * energy_penalty              # Minimize power consumption\n",
    "    - w4 * joint_limit_penalty         # Stay within safe ranges\n",
    "    - w5 * collision_penalty           # Avoid self-collision\n",
    ")\n",
    "```\n",
    "\n",
    "**Sim-to-Real Transfer:**\n",
    "To ensure policies work on real hardware:\n",
    "1. **Domain Randomization:** Randomize physics parameters in simulation\n",
    "2. **Observation Noise:** Add sensor noise to training\n",
    "3. **Action Delays:** Simulate communication latency\n",
    "4. **Terrain Variation:** Train on diverse surfaces\n",
    "\n",
    "**References:**\n",
    "- **MimicKit:** https://github.com/xbpeng/MimicKit\n",
    "- **IsaacGym:** https://developer.nvidia.com/isaac-gym\n",
    "- **Unitree RL Examples:** (Check official repos for latest)\n",
    "\n",
    "---\n",
    "\n",
    "## Afternoon Session (13:00â€“17:00)\n",
    "\n",
    "### 2.4 Low-Level Motor Control\n",
    "**ðŸ“˜ Covered in:** `unitree_g1_sdk_tutorial.ipynb` - Low-Level Control (Section 7)\n",
    "\n",
    "**PD Control Fundamentals:**\n",
    "\n",
    "Each motor is controlled via PD (Proportional-Derivative) controller:\n",
    "\n",
    "```\n",
    "Ï„ = Kp * (q_desired - q_actual) + Kd * (dq_desired - dq_actual) + Ï„_feedforward\n",
    "```\n",
    "\n",
    "Where:\n",
    "- `Ï„` = output torque (Nm)\n",
    "- `Kp` = position gain (stiffness)\n",
    "- `Kd` = velocity gain (damping)\n",
    "- `q` = joint position (rad)\n",
    "- `dq` = joint velocity (rad/s)\n",
    "\n",
    "**Tuning Guidelines:**\n",
    "\n",
    "| Joint Type | Kp Range | Kd Range | Use Case |\n",
    "|------------|----------|----------|----------|\n",
    "| Leg (Hip/Knee) | 60-120 | 1-3 | Locomotion, weight bearing |\n",
    "| Ankle | 40-60 | 1-2 | Fine balance adjustments |\n",
    "| Arm (Shoulder/Elbow) | 30-60 | 1-2 | Manipulation, reaching |\n",
    "| Wrist | 20-40 | 0.5-1.5 | Dexterous tasks |\n",
    "\n",
    "**Safety Considerations:**\n",
    "- Higher Kp = stiffer = faster response but potential instability\n",
    "- Higher Kd = more damping = smoother but slower\n",
    "- Always start with low gains and increase gradually\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ› ï¸ Workshop: Simple Walking & Control Task\n",
    "\n",
    "**Team Exercise 1: Velocity Control**\n",
    "\n",
    "**Objective:** Program the robot to walk in a square pattern (2m x 2m)\n",
    "\n",
    "**Tasks:**\n",
    "1. Walk forward 2 meters\n",
    "2. Turn 90 degrees in place\n",
    "3. Repeat for 4 sides\n",
    "4. Return to start position (measure error)\n",
    "\n",
    "**Starter Code:**\n",
    "```python\n",
    "import time\n",
    "from unitree_sdk2py.g1.loco.g1_loco_client import LocoClient\n",
    "from unitree_sdk2py.core.channel import ChannelFactoryInitialize\n",
    "\n",
    "# Initialize\n",
    "ChannelFactoryInitialize(0, \"eth0\")\n",
    "robot = LocoClient()\n",
    "robot.Init()\n",
    "\n",
    "# Stand up\n",
    "robot.Squat2StandUp()\n",
    "time.sleep(2)\n",
    "\n",
    "# TODO: Implement square walking pattern\n",
    "# Hint: Use robot.Move(vx, vy, vyaw) and time.sleep()\n",
    "# Challenge: How do you estimate distance traveled?\n",
    "```\n",
    "\n",
    "**Team Exercise 2: Adaptive Gait**\n",
    "\n",
    "**Objective:** Adjust walking speed based on terrain slope (simulated via IMU)\n",
    "\n",
    "**Tasks:**\n",
    "1. Read IMU pitch angle\n",
    "2. If uphill (pitch > 5Â°) â†’ slow down\n",
    "3. If downhill (pitch < -5Â°) â†’ slow down\n",
    "4. If flat â†’ normal speed\n",
    "\n",
    "**Challenge Questions:**\n",
    "- How do you determine current position without GPS?\n",
    "- What happens if the robot loses balance mid-walk?\n",
    "- Can you implement a \"pause and recover\" behavior?\n",
    "\n",
    "---\n",
    "\n",
    "## Day 2 Summary\n",
    "\n",
    "âœ… Understanding of bipedal locomotion theory  \n",
    "âœ… ZMP and stability concepts  \n",
    "âœ… RL-based locomotion approaches  \n",
    "âœ… Low-level motor control with PD tuning  \n",
    "âœ… Hands-on: Programmed basic walking patterns\n",
    "\n",
    "**Homework:** Research SLAM algorithms for Day 3 navigation topics.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 3: Navigation & Environment Perception\n",
    "\n",
    "**Focus:** Autonomous navigation, mapping, localization, and obstacle avoidance.\n",
    "\n",
    "---\n",
    "\n",
    "## Morning Session (09:00â€“12:00)\n",
    "\n",
    "### 3.1 Introduction to Robot Navigation\n",
    "**ðŸ“™ Requires new notebook:** Navigation & SLAM\n",
    "\n",
    "**Navigation Stack Overview:**\n",
    "\n",
    "Autonomous navigation requires:\n",
    "1. **Mapping:** Building a representation of the environment\n",
    "2. **Localization:** Determining robot position on the map\n",
    "3. **Path Planning:** Computing collision-free path to goal\n",
    "4. **Motion Control:** Following the planned path\n",
    "5. **Obstacle Avoidance:** Reacting to dynamic obstacles\n",
    "\n",
    "```\n",
    "Navigation Pipeline:\n",
    "\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚   Sensors    â”‚ (LiDAR, Camera, IMU)\n",
    "â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "       â”‚\n",
    "       â†“\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚ SLAM/Mapping â”‚ â† Build/Update Map\n",
    "â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "       â”‚\n",
    "       â†“\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚ Localization â”‚ â† \"Where am I?\"\n",
    "â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "       â”‚\n",
    "       â†“\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚ Path Planner â”‚ â† Global: A*, Dijkstra\n",
    "â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "       â”‚\n",
    "       â†“\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚ Local Plannerâ”‚ â† Obstacle Avoidance (DWA, TEB)\n",
    "â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "       â”‚\n",
    "       â†“\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚ Motion Ctrl  â”‚ â† Send velocity commands\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 3.2 Mapping & SLAM (Simultaneous Localization and Mapping)\n",
    "**ðŸ“™ Requires new notebook:** Navigation & SLAM\n",
    "\n",
    "**SLAM Problem:**\n",
    "> \"How can a robot build a map of an unknown environment while simultaneously determining its location within that map?\"\n",
    "\n",
    "**Common SLAM Algorithms:**\n",
    "\n",
    "#### 1. LiDAR-based SLAM\n",
    "- **Gmapping:** Grid-based, uses particle filter\n",
    "- **Cartographer:** Google's SLAM, handles large environments\n",
    "- **LOAM (Lidar Odometry and Mapping):** Real-time 6-DOF SLAM\n",
    "\n",
    "#### 2. Visual SLAM\n",
    "- **ORB-SLAM3:** Feature-based, works with mono/stereo/RGB-D\n",
    "- **RTAB-Map:** RGB-D SLAM with loop closure detection\n",
    "- **LSD-SLAM:** Direct method, dense reconstruction\n",
    "\n",
    "#### 3. Sensor Fusion SLAM\n",
    "- **VINS-Fusion:** Visual-Inertial (camera + IMU)\n",
    "- **LIO-SAM:** LiDAR-Inertial Odometry\n",
    "\n",
    "**For G1 Humanoid:**\n",
    "- **Recommended:** RTAB-Map with RealSense D435 depth camera\n",
    "- **Alternative:** Cartographer with 2D/3D LiDAR\n",
    "- **Benefit:** Handles dynamic walking motion (IMU-aided odometry)\n",
    "\n",
    "**Map Representations:**\n",
    "\n",
    "1. **Occupancy Grid (2D):**\n",
    "   - Cell-based: Each cell = free / occupied / unknown\n",
    "   - Resolution: typically 5cm per cell\n",
    "   - Format: `.pgm` image + `.yaml` metadata\n",
    "\n",
    "2. **Octomap (3D):**\n",
    "   - Octree structure for 3D voxels\n",
    "   - Memory-efficient\n",
    "   - Good for multi-floor environments\n",
    "\n",
    "3. **Point Cloud:**\n",
    "   - Raw 3D points from sensors\n",
    "   - Large data volume\n",
    "   - Used for detailed reconstruction\n",
    "\n",
    "![SLAM Example](./images/slam_example.png)\n",
    "*Figure: SLAM mapping process (Placeholder)*\n",
    "\n",
    "---\n",
    "\n",
    "### 3.3 Localization Techniques\n",
    "**ðŸ“™ Requires new notebook:** Navigation & SLAM\n",
    "\n",
    "**Localization Methods:**\n",
    "\n",
    "#### 1. Odometry-based\n",
    "- **Wheel Odometry:** Not applicable to humanoids (no wheels)\n",
    "- **Visual Odometry:** Track features between camera frames\n",
    "- **Inertial Odometry:** Integrate IMU acceleration (drifts over time)\n",
    "\n",
    "**For G1:** Joint encoder odometry (forward kinematics from leg joints)\n",
    "\n",
    "#### 2. Particle Filter Localization (Monte Carlo)\n",
    "- Represent belief as set of particles (samples)\n",
    "- Each particle = hypothesis of robot pose\n",
    "- Update particles based on sensor measurements\n",
    "- Used by: `AMCL` (Adaptive Monte Carlo Localization) in ROS\n",
    "\n",
    "#### 3. Kalman Filter Localization\n",
    "- Assumes Gaussian distributions\n",
    "- Efficient for unimodal belief\n",
    "- Extended Kalman Filter (EKF) for nonlinear systems\n",
    "\n",
    "#### 4. Graph-based Localization\n",
    "- Optimize pose graph (nodes = poses, edges = constraints)\n",
    "- Handles loop closures effectively\n",
    "- Used in modern SLAM systems\n",
    "\n",
    "**ROS 2 Localization Stack:**\n",
    "```bash\n",
    "# Launch AMCL for localization\n",
    "ros2 launch nav2_bringup localization_launch.py \\\n",
    "  map:=/path/to/map.yaml \\\n",
    "  use_sim_time:=false\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Afternoon Session (13:00â€“17:00)\n",
    "\n",
    "### 3.4 Path Planning Algorithms\n",
    "**ðŸ“™ Requires new notebook:** Navigation & SLAM\n",
    "\n",
    "**Global Planners (Complete Path):**\n",
    "\n",
    "#### 1. A* (A-Star)\n",
    "- Heuristic search algorithm\n",
    "- Guarantees shortest path (if admissible heuristic)\n",
    "- Commonly used for grid-based maps\n",
    "\n",
    "```python\n",
    "# Pseudocode\n",
    "def a_star(start, goal, map):\n",
    "    open_set = [start]\n",
    "    came_from = {}\n",
    "    \n",
    "    g_score = {start: 0}\n",
    "    f_score = {start: heuristic(start, goal)}\n",
    "    \n",
    "    while open_set:\n",
    "        current = node with lowest f_score in open_set\n",
    "        \n",
    "        if current == goal:\n",
    "            return reconstruct_path(came_from, current)\n",
    "        \n",
    "        for neighbor in neighbors(current):\n",
    "            tentative_g = g_score[current] + distance(current, neighbor)\n",
    "            \n",
    "            if tentative_g < g_score.get(neighbor, infinity):\n",
    "                came_from[neighbor] = current\n",
    "                g_score[neighbor] = tentative_g\n",
    "                f_score[neighbor] = g_score[neighbor] + heuristic(neighbor, goal)\n",
    "```\n",
    "\n",
    "#### 2. Dijkstra's Algorithm\n",
    "- Special case of A* (heuristic = 0)\n",
    "- Explores uniformly in all directions\n",
    "- Slower but guarantees optimal path\n",
    "\n",
    "#### 3. RRT (Rapidly-exploring Random Tree)\n",
    "- Samples random points, grows tree toward goal\n",
    "- Good for high-dimensional spaces\n",
    "- RRT* variant provides asymptotic optimality\n",
    "\n",
    "**Local Planners (Obstacle Avoidance):**\n",
    "\n",
    "#### 1. Dynamic Window Approach (DWA)\n",
    "- Samples velocity commands (vx, vy, vÏ‰)\n",
    "- Simulates trajectories forward in time\n",
    "- Scores based on: goal heading, obstacle clearance, velocity\n",
    "- Selects best trajectory\n",
    "\n",
    "#### 2. Timed Elastic Band (TEB)\n",
    "- Optimizes trajectory as elastic band\n",
    "- Considers kinematic constraints\n",
    "- Smoother paths than DWA\n",
    "\n",
    "#### 3. Model Predictive Control (MPC)\n",
    "- Solves optimization problem over future horizon\n",
    "- Can incorporate complex constraints\n",
    "- Computationally expensive\n",
    "\n",
    "**ROS 2 Nav2 Stack:**\n",
    "```bash\n",
    "# Launch full navigation stack\n",
    "ros2 launch nav2_bringup navigation_launch.py \\\n",
    "  map:=/path/to/map.yaml \\\n",
    "  params_file:=/path/to/nav2_params.yaml\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 3.5 Sensor Integration for Navigation\n",
    "**ðŸ“— Partially covered in:** `unitree_g1_advanced_usecases.ipynb` - Vision-Based Grasping (Section 5)\n",
    "**ðŸ“™ Requires new notebook:** Navigation & SLAM\n",
    "\n",
    "**Intel RealSense D435 Integration:**\n",
    "\n",
    "**Sensor Specifications:**\n",
    "- Depth Range: 0.3m â€“ 3.0m\n",
    "- Resolution: 640x480 @ 30 FPS (depth), 1920x1080 @ 30 FPS (RGB)\n",
    "- Field of View: 87Â° Ã— 58Â° (depth)\n",
    "- Technology: Stereo depth (active IR pattern)\n",
    "\n",
    "**ROS 2 Integration:**\n",
    "```bash\n",
    "# Install RealSense ROS wrapper\n",
    "sudo apt install ros-humble-realsense2-camera\n",
    "\n",
    "# Launch camera node\n",
    "ros2 launch realsense2_camera rs_launch.py \\\n",
    "  enable_depth:=true \\\n",
    "  enable_color:=true \\\n",
    "  depth_module.profile:=640x480x30 \\\n",
    "  pointcloud.enable:=true\n",
    "```\n",
    "\n",
    "**Topics Published:**\n",
    "- `/camera/color/image_raw` â€“ RGB image\n",
    "- `/camera/depth/image_rect_raw` â€“ Depth image\n",
    "- `/camera/depth/color/points` â€“ Point cloud (XYZ + RGB)\n",
    "- `/camera/imu` â€“ Built-in IMU data (if D435i variant)\n",
    "\n",
    "**Obstacle Detection Pipeline:**\n",
    "```python\n",
    "import rclpy\n",
    "from sensor_msgs.msg import PointCloud2\n",
    "import numpy as np\n",
    "\n",
    "class ObstacleDetector:\n",
    "    def pointcloud_callback(self, msg: PointCloud2):\n",
    "        # Convert to numpy array\n",
    "        points = self.pointcloud2_to_array(msg)\n",
    "        \n",
    "        # Filter by height (obstacles in robot path)\n",
    "        obstacles = points[(points[:, 2] > 0.1) & (points[:, 2] < 1.5)]\n",
    "        \n",
    "        # Check distance\n",
    "        close_obstacles = obstacles[obstacles[:, 0] < 2.0]  # Within 2m\n",
    "        \n",
    "        if len(close_obstacles) > 0:\n",
    "            print(f\"Warning: {len(close_obstacles)} obstacles detected!\")\n",
    "```\n",
    "\n",
    "**Alternative Sensors:**\n",
    "- **2D LiDAR:** (e.g., RPLIDAR A1) â€“ Good for 2D mapping, limited height info\n",
    "- **3D LiDAR:** (e.g., Velodyne VLP-16) â€“ Expensive but precise, large range\n",
    "- **RGB-D Cameras:** (Kinect, Intel RealSense, Orbbec Astra)\n",
    "- **Stereo Cameras:** (ZED 2, OAK-D) â€“ Wider FOV than RealSense\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ› ï¸ Workshop: Navigation Task with Obstacle Field\n",
    "\n",
    "**Scenario:** Navigate G1 through a 5m x 5m area with randomly placed obstacles.\n",
    "\n",
    "**Setup:**\n",
    "- Cardboard boxes as obstacles (varying heights: 20cm, 50cm, 100cm)\n",
    "- Start position: (0, 0)\n",
    "- Goal position: (4, 4)\n",
    "- Robot must avoid all obstacles\n",
    "\n",
    "**Team Exercise 1: Manual Teleoperation**\n",
    "\n",
    "**Objective:** Navigate manually while observing sensor data\n",
    "\n",
    "**Tasks:**\n",
    "1. Set up teleoperation control (keyboard or joystick)\n",
    "2. Monitor RealSense depth feed\n",
    "3. Navigate to goal, avoiding obstacles\n",
    "4. Record: time taken, number of collisions, path efficiency\n",
    "\n",
    "```python\n",
    "# Simple keyboard teleoperation\n",
    "import sys, tty, termios\n",
    "from unitree_sdk2py.g1.loco.g1_loco_client import LocoClient\n",
    "\n",
    "def get_key():\n",
    "    fd = sys.stdin.fileno()\n",
    "    old_settings = termios.tcgetattr(fd)\n",
    "    try:\n",
    "        tty.setraw(sys.stdin.fileno())\n",
    "        ch = sys.stdin.read(1)\n",
    "    finally:\n",
    "        termios.tcsetattr(fd, termios.TCSADRAIN, old_settings)\n",
    "    return ch\n",
    "\n",
    "robot = LocoClient()\n",
    "robot.Init()\n",
    "\n",
    "print(\"Use W/A/S/D to move, Q to quit\")\n",
    "while True:\n",
    "    key = get_key()\n",
    "    if key == 'w': robot.Move(0.3, 0, 0)\n",
    "    elif key == 's': robot.Move(-0.3, 0, 0)\n",
    "    elif key == 'a': robot.Move(0, 0, 0.3)\n",
    "    elif key == 'd': robot.Move(0, 0, -0.3)\n",
    "    elif key == 'q': break\n",
    "    else: robot.Move(0, 0, 0)\n",
    "```\n",
    "\n",
    "**Team Exercise 2: Autonomous Navigation**\n",
    "\n",
    "**Objective:** Implement simple obstacle avoidance algorithm\n",
    "\n",
    "**Algorithm Suggestion (Reactive):**\n",
    "1. Read depth image from RealSense\n",
    "2. Divide view into 5 sectors (left, center-left, center, center-right, right)\n",
    "3. Compute average depth in each sector\n",
    "4. Turn away from closest obstacle\n",
    "5. Move forward if center is clear\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import pyrealsense2 as rs\n",
    "\n",
    "# Initialize RealSense\n",
    "pipeline = rs.pipeline()\n",
    "config = rs.config()\n",
    "config.enable_stream(rs.stream.depth, 640, 480, rs.format.z16, 30)\n",
    "pipeline.start(config)\n",
    "\n",
    "robot = LocoClient()\n",
    "robot.Init()\n",
    "robot.Squat2StandUp()\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        frames = pipeline.wait_for_frames()\n",
    "        depth_frame = frames.get_depth_frame()\n",
    "        depth_image = np.asanyarray(depth_frame.get_data())\n",
    "        \n",
    "        # Divide into sectors\n",
    "        width = depth_image.shape[1]\n",
    "        sector_width = width // 5\n",
    "        \n",
    "        sectors = [\n",
    "            depth_image[:, i*sector_width:(i+1)*sector_width].mean() \n",
    "            for i in range(5)\n",
    "        ]\n",
    "        \n",
    "        # Decision logic\n",
    "        center_clear = sectors[2] > 1000  # 1m threshold (depth in mm)\n",
    "        \n",
    "        if center_clear:\n",
    "            robot.Move(0.2, 0, 0)  # Move forward\n",
    "        else:\n",
    "            # Turn toward most open sector\n",
    "            best_sector = np.argmax(sectors)\n",
    "            turn_rate = (best_sector - 2) * 0.2  # -0.4 to +0.4 rad/s\n",
    "            robot.Move(0, 0, turn_rate)\n",
    "        \n",
    "        time.sleep(0.1)\n",
    "\n",
    "finally:\n",
    "    pipeline.stop()\n",
    "    robot.Damp()\n",
    "```\n",
    "\n",
    "**Challenge Questions:**\n",
    "- How do you handle narrow passages?\n",
    "- What if obstacle is too close before detected?\n",
    "- How to reach specific goal coordinates (not just avoid obstacles)?\n",
    "\n",
    "**Bonus Challenge:** Implement A* path planning\n",
    "- Build occupancy grid from RealSense scans\n",
    "- Plan path with A*\n",
    "- Follow waypoints while avoiding dynamic obstacles\n",
    "\n",
    "---\n",
    "\n",
    "## Day 3 Summary\n",
    "\n",
    "âœ… Understanding of navigation stack architecture  \n",
    "âœ… SLAM concepts and algorithms  \n",
    "âœ… Localization techniques (AMCL, odometry)  \n",
    "âœ… Path planning (A*, DWA)  \n",
    "âœ… Sensor integration (RealSense for depth perception)  \n",
    "âœ… Hands-on: Navigated through obstacle field\n",
    "\n",
    "**Homework:** Review computer vision basics (OpenCV) for Day 4 perception tasks.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 4: Perception, Applications & Project Work\n",
    "\n",
    "**Focus:** Visual perception, object recognition, real-world applications, and final team project.\n",
    "\n",
    "---\n",
    "\n",
    "## Morning Session (09:00â€“12:00)\n",
    "\n",
    "### 4.1 Visual Perception Fundamentals\n",
    "**ðŸ“— Partially covered in:** `unitree_g1_advanced_usecases.ipynb` - Vision-Based Grasping (Section 5)\n",
    "**ðŸ“™ Requires new notebook:** Computer Vision & Perception\n",
    "\n",
    "**Perception Pipeline:**\n",
    "\n",
    "```\n",
    "Camera â†’ Image Processing â†’ Feature Detection â†’ Object Recognition â†’ Scene Understanding\n",
    "```\n",
    "\n",
    "**Key Tasks:**\n",
    "1. **Image Acquisition** â€“ Capture RGB/depth images\n",
    "2. **Preprocessing** â€“ Noise reduction, normalization\n",
    "3. **Segmentation** â€“ Separate objects from background\n",
    "4. **Feature Extraction** â€“ Detect edges, corners, keypoints\n",
    "5. **Classification** â€“ Identify what objects are\n",
    "6. **Localization** â€“ Determine 3D position of objects\n",
    "\n",
    "---\n",
    "\n",
    "### 4.2 Object Detection & Recognition\n",
    "**ðŸ“™ Requires new notebook:** Computer Vision & Perception\n",
    "\n",
    "**Classical Computer Vision:**\n",
    "\n",
    "#### Color-based Segmentation\n",
    "```python\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Detect red objects\n",
    "def detect_red_objects(image):\n",
    "    hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "    \n",
    "    # Red color range in HSV\n",
    "    lower_red = np.array([0, 100, 100])\n",
    "    upper_red = np.array([10, 255, 255])\n",
    "    \n",
    "    mask = cv2.inRange(hsv, lower_red, upper_red)\n",
    "    contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    return contours\n",
    "```\n",
    "\n",
    "#### Edge Detection\n",
    "```python\n",
    "# Canny edge detector\n",
    "edges = cv2.Canny(gray_image, threshold1=50, threshold2=150)\n",
    "```\n",
    "\n",
    "**Deep Learning Approaches:**\n",
    "\n",
    "#### 1. Object Detection Networks\n",
    "- **YOLO (You Only Look Once):** Real-time, single-stage detector\n",
    "- **Faster R-CNN:** Two-stage, more accurate but slower\n",
    "- **EfficientDet:** Balanced speed/accuracy\n",
    "\n",
    "**Example: YOLOv8 with Ultralytics**\n",
    "```python\n",
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "\n",
    "# Load pretrained model\n",
    "model = YOLO('yolov8n.pt')  # Nano model (fast)\n",
    "\n",
    "# Detect objects in image\n",
    "results = model(image)\n",
    "\n",
    "# Parse results\n",
    "for result in results:\n",
    "    boxes = result.boxes\n",
    "    for box in boxes:\n",
    "        cls = int(box.cls[0])\n",
    "        conf = float(box.conf[0])\n",
    "        xyxy = box.xyxy[0].cpu().numpy()\n",
    "        \n",
    "        print(f\"Detected {model.names[cls]} with confidence {conf:.2f}\")\n",
    "```\n",
    "\n",
    "#### 2. Instance Segmentation\n",
    "- **Mask R-CNN:** Pixel-level object segmentation\n",
    "- **YOLACT:** Real-time instance segmentation\n",
    "\n",
    "#### 3. 3D Object Detection\n",
    "- **PointNet/PointNet++:** Directly process point clouds\n",
    "- **VoxelNet:** 3D CNN on voxelized point clouds\n",
    "\n",
    "**Integration with G1:**\n",
    "```python\n",
    "import pyrealsense2 as rs\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Initialize camera and detector\n",
    "pipeline = rs.pipeline()\n",
    "config = rs.config()\n",
    "config.enable_stream(rs.stream.color, 640, 480, rs.format.bgr8, 30)\n",
    "config.enable_stream(rs.stream.depth, 640, 480, rs.format.z16, 30)\n",
    "pipeline.start(config)\n",
    "\n",
    "model = YOLO('yolov8n.pt')\n",
    "\n",
    "# Main loop\n",
    "while True:\n",
    "    frames = pipeline.wait_for_frames()\n",
    "    color_frame = frames.get_color_frame()\n",
    "    depth_frame = frames.get_depth_frame()\n",
    "    \n",
    "    color_image = np.asanyarray(color_frame.get_data())\n",
    "    \n",
    "    # Detect objects\n",
    "    results = model(color_image)\n",
    "    \n",
    "    # For each detected object, get 3D position\n",
    "    for result in results:\n",
    "        for box in result.boxes:\n",
    "            # Get center pixel\n",
    "            x_center = int((box.xyxy[0][0] + box.xyxy[0][2]) / 2)\n",
    "            y_center = int((box.xyxy[0][1] + box.xyxy[0][3]) / 2)\n",
    "            \n",
    "            # Get depth at center\n",
    "            depth = depth_frame.get_distance(x_center, y_center)\n",
    "            \n",
    "            # Convert to 3D coordinates\n",
    "            intrinsics = color_frame.profile.as_video_stream_profile().intrinsics\n",
    "            point_3d = rs.rs2_deproject_pixel_to_point(intrinsics, [x_center, y_center], depth)\n",
    "            \n",
    "            print(f\"Object at 3D position: {point_3d}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 4.3 Environment Understanding\n",
    "**ðŸ“™ Requires new notebook:** Computer Vision & Perception\n",
    "\n",
    "**Semantic Segmentation:**\n",
    "- Label every pixel with class (floor, wall, obstacle, person, etc.)\n",
    "- **Models:** DeepLabV3, U-Net, SegFormer\n",
    "- **Use Case:** Identify traversable surfaces for locomotion\n",
    "\n",
    "**Scene Understanding Tasks:**\n",
    "1. **Floor Detection:** Where can the robot walk?\n",
    "2. **Stair Detection:** Identify steps for climbing\n",
    "3. **Door Detection:** Recognize openings to pass through\n",
    "4. **Person Detection:** Avoid humans, enable HRI\n",
    "5. **Object Affordance:** What can be grasped/manipulated?\n",
    "\n",
    "**Example: Floor Segmentation**\n",
    "```python\n",
    "# Using depth camera to find floor plane\n",
    "def detect_floor_plane(depth_image):\n",
    "    # Convert to point cloud\n",
    "    points = depth_image_to_pointcloud(depth_image)\n",
    "    \n",
    "    # RANSAC plane fitting\n",
    "    plane_model, inliers = fit_plane_ransac(points)\n",
    "    \n",
    "    # Plane equation: ax + by + cz + d = 0\n",
    "    a, b, c, d = plane_model\n",
    "    \n",
    "    # Normal vector should point up (z-axis)\n",
    "    if c < 0:\n",
    "        a, b, c, d = -a, -b, -c, -d\n",
    "    \n",
    "    return plane_model, inliers\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Afternoon Session (13:00â€“17:00)\n",
    "\n",
    "### 4.4 Real-World Applications\n",
    "\n",
    "**Application 1: Industrial Logistics**\n",
    "**ðŸ“™ Requires new notebook:** Industrial Applications\n",
    "\n",
    "**Use Cases:**\n",
    "- **Warehouse Navigation:** Move between storage locations\n",
    "- **Inventory Inspection:** Visual inspection of shelves\n",
    "- **Item Retrieval:** Pick objects from shelves/bins\n",
    "- **Delivery Tasks:** Transport items to workstations\n",
    "\n",
    "**Example Workflow:**\n",
    "```\n",
    "1. Receive order (pick item from location A5)\n",
    "2. Navigate to location A5 (using Nav2 stack)\n",
    "3. Detect target item (YOLO + depth)\n",
    "4. Compute grasp pose (6-DOF)\n",
    "5. Execute grasp (whole-body control)\n",
    "6. Navigate to delivery point\n",
    "7. Place item (compliant control)\n",
    "8. Return to home position\n",
    "```\n",
    "\n",
    "**Challenges:**\n",
    "- Dynamic environment (people, forklifts)\n",
    "- Varying lighting conditions\n",
    "- Precise manipulation required\n",
    "- Long operational hours (battery management)\n",
    "\n",
    "![Warehouse Logistics](./images/warehouse_logistics.png)\n",
    "*Figure: G1 in warehouse environment (Placeholder)*\n",
    "\n",
    "---\n",
    "\n",
    "**Application 2: Facility Cleaning Robotics**\n",
    "**ðŸ“™ Requires new notebook:** Industrial Applications\n",
    "\n",
    "**Use Cases:**\n",
    "- **Floor Cleaning:** Mopping, vacuuming (with tool attachment)\n",
    "- **Window Cleaning:** High-reach areas\n",
    "- **Surface Wiping:** Desks, handrails, touchpoints\n",
    "- **Trash Collection:** Empty bins, replace bags\n",
    "\n",
    "**Example: Autonomous Floor Cleaning**\n",
    "```python\n",
    "class CleaningRobot:\n",
    "    def __init__(self):\n",
    "        self.robot = LocoClient()\n",
    "        self.map = None\n",
    "        self.cleaned_areas = set()\n",
    "    \n",
    "    def coverage_path_planning(self, map_data):\n",
    "        \"\"\"\n",
    "        Generate path that covers all floor area\n",
    "        Common patterns: boustrophedon (back-and-forth), spiral\n",
    "        \"\"\"\n",
    "        # Decompose map into cells\n",
    "        cells = decompose_into_cells(map_data, cell_size=1.0)\n",
    "        \n",
    "        # Generate back-and-forth pattern\n",
    "        path = []\n",
    "        for row in range(len(cells)):\n",
    "            if row % 2 == 0:\n",
    "                path.extend(cells[row])  # Left to right\n",
    "            else:\n",
    "                path.extend(reversed(cells[row]))  # Right to left\n",
    "        \n",
    "        return path\n",
    "    \n",
    "    def execute_cleaning(self, path):\n",
    "        for waypoint in path:\n",
    "            # Navigate to waypoint\n",
    "            self.navigate_to(waypoint)\n",
    "            \n",
    "            # Activate cleaning tool (via GPIO or service call)\n",
    "            self.activate_mop()\n",
    "            \n",
    "            # Slow walking for thorough cleaning\n",
    "            self.robot.Move(vx=0.1, vy=0, vyaw=0)\n",
    "            time.sleep(10)  # Clean 1mÂ² area\n",
    "            \n",
    "            self.cleaned_areas.add(waypoint)\n",
    "```\n",
    "\n",
    "**Challenges:**\n",
    "- Detecting dirty vs. clean areas\n",
    "- Avoiding wet floors\n",
    "- Tool manipulation (attaching/detaching equipment)\n",
    "- Working around humans (24/7 operation)\n",
    "\n",
    "---\n",
    "\n",
    "### 4.5 Implementing Enterprise Solutions\n",
    "\n",
    "**\"How do I deploy this in my company?\"**\n",
    "\n",
    "**Step 1: Needs Assessment**\n",
    "- What tasks will the robot perform?\n",
    "- What is the ROI (return on investment)?\n",
    "- What are the technical requirements?\n",
    "- What are safety/regulatory constraints?\n",
    "\n",
    "**Step 2: Proof of Concept**\n",
    "- Start with simplified task in controlled environment\n",
    "- Test for 1-2 weeks with supervision\n",
    "- Measure performance metrics (success rate, time, accuracy)\n",
    "- Identify failure modes\n",
    "\n",
    "**Step 3: System Integration**\n",
    "- Connect to existing IT infrastructure (ERP, WMS)\n",
    "- Develop custom interfaces (dashboards, mobile apps)\n",
    "- Implement fleet management (if multiple robots)\n",
    "- Set up monitoring and alerts\n",
    "\n",
    "**Step 4: Pilot Deployment**\n",
    "- Deploy in limited area (e.g., one warehouse aisle)\n",
    "- Train staff on supervision and intervention\n",
    "- Collect data and iterate\n",
    "\n",
    "**Step 5: Scale-Up**\n",
    "- Expand to full facility\n",
    "- Add more robots if needed\n",
    "- Continuous improvement based on data\n",
    "\n",
    "**Key Success Factors:**\n",
    "âœ… Clear definition of success metrics  \n",
    "âœ… Executive sponsorship  \n",
    "âœ… Cross-functional team (engineering, operations, safety)  \n",
    "âœ… Iterative approach (start small, scale up)  \n",
    "âœ… Change management (staff training, communication)\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ› ï¸ Final Project Challenge\n",
    "\n",
    "**Duration:** 3 hours (13:00â€“16:00)\n",
    "\n",
    "**Objective:** Teams implement a complete robotic application combining locomotion, perception, and manipulation.\n",
    "\n",
    "### Project Options:\n",
    "\n",
    "#### Option A: Warehouse Pick-and-Place\n",
    "**Scenario:** Robot must navigate to a shelf, identify a target object (colored cube), pick it up, and deliver to a designated drop-off point.\n",
    "\n",
    "**Requirements:**\n",
    "- Autonomous navigation (use Nav2 or simple obstacle avoidance)\n",
    "- Object detection (color-based or YOLO)\n",
    "- 3D position estimation (RealSense depth)\n",
    "- Grasp execution (IK + low-level control)\n",
    "- Return to home\n",
    "\n",
    "**Scoring:**\n",
    "- Object successfully picked: 30 points\n",
    "- Object delivered to correct location: 30 points\n",
    "- No collisions: 20 points\n",
    "- Time bonus (< 2 minutes): 20 points\n",
    "\n",
    "---\n",
    "\n",
    "#### Option B: Facility Inspection\n",
    "**Scenario:** Robot patrols a defined area, detects anomalies (e.g., object on floor that shouldn't be there), and reports findings.\n",
    "\n",
    "**Requirements:**\n",
    "- Coverage path planning (visit all waypoints)\n",
    "- Visual anomaly detection (objects out of place)\n",
    "- Logging (save images and locations)\n",
    "- Generate inspection report\n",
    "\n",
    "**Scoring:**\n",
    "- Complete coverage (all waypoints visited): 30 points\n",
    "- Anomalies detected (3 hidden objects): 30 points (10 each)\n",
    "- Report quality (images, coordinates, descriptions): 20 points\n",
    "- Time efficiency: 20 points\n",
    "\n",
    "---\n",
    "\n",
    "#### Option C: Human-Robot Collaboration\n",
    "**Scenario:** Robot assists a human worker by following them, carrying a tool, and handing it over on gesture command.\n",
    "\n",
    "**Requirements:**\n",
    "- Person detection and tracking (YOLO + depth)\n",
    "- Following behavior (maintain 1.5m distance)\n",
    "- Gesture recognition (hand raised = stop and approach)\n",
    "- Object handover (extend arm with object)\n",
    "\n",
    "**Scoring:**\n",
    "- Successful person tracking: 30 points\n",
    "- Gesture recognition accuracy: 30 points\n",
    "- Safe handover (no collisions): 20 points\n",
    "- Smooth motion (no jerky movements): 20 points\n",
    "\n",
    "---\n",
    "\n",
    "### Project Deliverables:\n",
    "\n",
    "1. **Live Demonstration** (5 min per team)\n",
    "2. **Code Repository** (GitHub link)\n",
    "3. **Brief Presentation** (3 slides: approach, challenges, results)\n",
    "4. **Video Recording** (backup if live demo fails)\n",
    "\n",
    "---\n",
    "\n",
    "## Team Presentations (16:00â€“17:00)\n",
    "\n",
    "Each team presents:\n",
    "- **Problem Approach:** What strategy did you choose?\n",
    "- **Technical Challenges:** What obstacles did you face?\n",
    "- **Solutions:** How did you overcome challenges?\n",
    "- **Results:** Demo + metrics\n",
    "- **Lessons Learned:** What would you do differently?\n",
    "\n",
    "**Judging Criteria:**\n",
    "- Functionality (does it work?): 40%\n",
    "- Code quality (clean, documented): 20%\n",
    "- Innovation (creative solutions): 20%\n",
    "- Presentation (clear communication): 20%\n",
    "\n",
    "---\n",
    "\n",
    "## Certificate Ceremony\n",
    "\n",
    "All participants receive:\n",
    "\n",
    "**\"Unitree G1 Academy Certificate\"**\n",
    "\n",
    "Certifying completion of:\n",
    "- Hardware architecture and setup\n",
    "- Motion control and locomotion\n",
    "- Navigation and mapping\n",
    "- Perception and computer vision\n",
    "- Real-world application development\n",
    "\n",
    "---\n",
    "\n",
    "## Day 4 Summary\n",
    "\n",
    "âœ… Visual perception and object detection  \n",
    "âœ… Environment understanding (segmentation, SLAM)  \n",
    "âœ… Real-world applications (logistics, cleaning)  \n",
    "âœ… Enterprise deployment strategies  \n",
    "âœ… Hands-on project: Complete robotic application  \n",
    "âœ… Certificate of completion\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Course Summary & Next Steps\n",
    "\n",
    "## What You've Learned\n",
    "\n",
    "Over 4 intensive days, you have:\n",
    "\n",
    "### Technical Skills\n",
    "âœ… Mastered the Unitree SDK2 Python API  \n",
    "âœ… Implemented locomotion control (high-level and low-level)  \n",
    "âœ… Developed navigation systems with SLAM  \n",
    "âœ… Integrated computer vision for perception  \n",
    "âœ… Built complete robotic applications\n",
    "\n",
    "### Practical Experience\n",
    "âœ… Hands-on work with real G1 hardware  \n",
    "âœ… Team collaboration on robotics projects  \n",
    "âœ… Troubleshooting and problem-solving  \n",
    "âœ… Safety protocols and best practices\n",
    "\n",
    "### Business Knowledge\n",
    "âœ… Understanding of ROI and deployment strategies  \n",
    "âœ… Real-world use cases in industry  \n",
    "âœ… Integration with existing systems  \n",
    "âœ… Scalability considerations\n",
    "\n",
    "---\n",
    "\n",
    "## Continuing Your Journey\n",
    "\n",
    "### Recommended Next Steps:\n",
    "\n",
    "1. **Practice with Simulation**\n",
    "   - Set up Gazebo/IsaacGym simulation at your facility\n",
    "   - Test algorithms before deploying to real robot\n",
    "   - Reduce risk and development time\n",
    "\n",
    "2. **Join the Community**\n",
    "   - Unitree Developer Forum: https://www.unitree.com/community/\n",
    "   - GitHub Discussions: https://github.com/unitreerobotics/\n",
    "   - ROS Discourse: https://discourse.ros.org/\n",
    "\n",
    "3. **Advanced Topics**\n",
    "   - Deep reinforcement learning (DeepMimic, MimicKit)\n",
    "   - Multi-robot coordination\n",
    "   - Cloud robotics and remote operation\n",
    "   - Custom tool integration (grippers, sensors)\n",
    "\n",
    "4. **Build Your First Project**\n",
    "   - Start small with proof-of-concept\n",
    "   - Document everything (code, learnings, failures)\n",
    "   - Share with community for feedback\n",
    "\n",
    "---\n",
    "\n",
    "## Resources\n",
    "\n",
    "### Official Documentation\n",
    "- **Unitree Developer Portal:** https://support.unitree.com/home/en/developer\n",
    "- **SDK2 Python GitHub:** https://github.com/unitreerobotics/unitree_sdk2_python\n",
    "- **ROS 2 Humble Docs:** https://docs.ros.org/en/humble/\n",
    "\n",
    "### Books & Papers\n",
    "- *\"Humanoid Robotics: A Reference\"* by Goswami & Vadakkepat\n",
    "- *\"Probabilistic Robotics\"* by Thrun, Burgard, Fox\n",
    "- *\"Deep Reinforcement Learning\"* by Pieter Abbeel\n",
    "\n",
    "### Online Courses\n",
    "- **ROS 2 for Beginners:** https://www.udemy.com/course/ros2-for-beginners/\n",
    "- **Computer Vision (Stanford CS231n):** http://cs231n.stanford.edu/\n",
    "- **RL Course (DeepMind):** https://www.deepmind.com/learning-resources/\n",
    "\n",
    "---\n",
    "\n",
    "## Support & Contact\n",
    "\n",
    "**EF Robotics Support:**\n",
    "- Email: academy@ef-robotics.de (placeholder)\n",
    "- Phone: +49 (0) XXX XXXXXXX (placeholder)\n",
    "- Follow-up consultations available\n",
    "\n",
    "**Alumni Network:**\n",
    "- Join the Unitree G1 Academy alumni group\n",
    "- Monthly virtual meetups\n",
    "- Share projects and collaborate\n",
    "\n",
    "---\n",
    "\n",
    "## Final Thoughts\n",
    "\n",
    "Humanoid robotics is a rapidly evolving field. What you learned this week is just the beginning. The real learning happens when you:\n",
    "\n",
    "- **Build real projects** that solve actual problems\n",
    "- **Fail and iterate** (failure is the best teacher)\n",
    "- **Share knowledge** with the community\n",
    "- **Stay curious** and keep learning\n",
    "\n",
    "The future of humanoid robots in industry is being written now. You are now equipped to be part of that story.\n",
    "\n",
    "**Congratulations on completing the Unitree G1 Academy!** ðŸŽ“ðŸ¤–\n",
    "\n",
    "---\n",
    "\n",
    "*Course materials prepared by EF Robotics*  \n",
    "*For questions or feedback: academy@ef-robotics.de*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Appendix: Notebook Coverage Map\n",
    "\n",
    "This table shows which course topics are covered in existing notebooks:\n",
    "\n",
    "| Course Topic | Notebook | Section | Status |\n",
    "|--------------|----------|---------|--------|\n",
    "| **Day 1: Fundamentals** |\n",
    "| G1 Overview | Tutorial | Introduction | âœ… Complete |\n",
    "| Hardware Architecture | - | - | âš ï¸ Needs dedicated notebook |\n",
    "| SDK Setup & Installation | Tutorial | Section 1 | âœ… Complete |\n",
    "| Safety Protocols | Tutorial | Section 1 | âœ… Complete |\n",
    "| **Day 2: Motion Control** |\n",
    "| High-Level Locomotion | Tutorial | Section 3 | âœ… Complete |\n",
    "| Low-Level Motor Control | Tutorial | Section 7 | âœ… Complete |\n",
    "| Reinforcement Learning | Advanced | Section 3 | âœ… Complete |\n",
    "| Dynamic Balance | Advanced | Section 4 | âœ… Complete |\n",
    "| **Day 3: Navigation** |\n",
    "| SLAM & Mapping | - | - | âš ï¸ Needs dedicated notebook |\n",
    "| Localization | - | - | âš ï¸ Needs dedicated notebook |\n",
    "| Path Planning | - | - | âš ï¸ Needs dedicated notebook |\n",
    "| RealSense Integration | Advanced | Section 5 | âœ… Complete |\n",
    "| Obstacle Avoidance | - | - | âš ï¸ Needs dedicated notebook |\n",
    "| **Day 4: Perception** |\n",
    "| Object Detection | Advanced | Section 5 | âš ï¸ Partial (expand needed) |\n",
    "| Visual Perception | - | - | âš ï¸ Needs dedicated notebook |\n",
    "| Scene Understanding | - | - | âš ï¸ Needs dedicated notebook |\n",
    "| Industrial Applications | - | - | âš ï¸ Needs dedicated notebook |\n",
    "| Enterprise Deployment | - | - | âš ï¸ Needs dedicated notebook |\n",
    "\n",
    "**Legend:**\n",
    "- âœ… Complete: Fully covered in existing notebook\n",
    "- âš ï¸ Partial: Some content exists but needs expansion\n",
    "- âŒ Missing: Not covered, new notebook needed\n",
    "\n",
    "---\n",
    "\n",
    "## Suggested Additional Notebooks\n",
    "\n",
    "To complete the course material, the following notebooks should be created:\n",
    "\n",
    "1. **`unitree_g1_hardware_deepdive.ipynb`**\n",
    "   - Mechanical design details\n",
    "   - Electronics and power systems\n",
    "   - Communication architecture\n",
    "   - Maintenance and troubleshooting\n",
    "\n",
    "2. **`unitree_g1_ros2_navigation.ipynb`**\n",
    "   - ROS 2 setup and basics\n",
    "   - Nav2 stack integration\n",
    "   - SLAM (Cartographer, RTAB-Map)\n",
    "   - Path planning algorithms\n",
    "   - Costmap configuration\n",
    "\n",
    "3. **`unitree_g1_computer_vision.ipynb`**\n",
    "   - Image processing with OpenCV\n",
    "   - Object detection (YOLO, Faster R-CNN)\n",
    "   - Semantic segmentation\n",
    "   - 3D perception from depth cameras\n",
    "   - Custom model training\n",
    "\n",
    "4. **`unitree_g1_industrial_applications.ipynb`**\n",
    "   - Warehouse logistics implementation\n",
    "   - Facility cleaning robotics\n",
    "   - Inspection and monitoring\n",
    "   - Fleet management\n",
    "   - ROI calculations\n",
    "\n",
    "5. **`unitree_g1_system_integration.ipynb`**\n",
    "   - Enterprise software integration (ERP, WMS)\n",
    "   - Cloud connectivity\n",
    "   - Remote monitoring dashboards\n",
    "   - Data logging and analytics\n",
    "   - Safety and compliance\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistics: Course Coverage Analysis\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Course topic distribution\n",
    "days = ['Day 1\\nFundamentals', 'Day 2\\nMotion Control', 'Day 3\\nNavigation', 'Day 4\\nPerception']\n",
    "total_topics = [4, 4, 5, 5]  # Total topics per day\n",
    "covered_topics = [3, 4, 1, 1]  # Topics covered in existing notebooks\n",
    "partial_topics = [0, 0, 1, 1]  # Topics partially covered\n",
    "\n",
    "# Create stacked bar chart\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "x = np.arange(len(days))\n",
    "width = 0.6\n",
    "\n",
    "p1 = ax.bar(x, covered_topics, width, label='Fully Covered', color='#2ecc71')\n",
    "p2 = ax.bar(x, partial_topics, width, bottom=covered_topics, label='Partially Covered', color='#f39c12')\n",
    "missing_topics = [t - c - p for t, c, p in zip(total_topics, covered_topics, partial_topics)]\n",
    "p3 = ax.bar(x, missing_topics, width, bottom=[c+p for c, p in zip(covered_topics, partial_topics)], \n",
    "            label='Needs New Notebook', color='#e74c3c')\n",
    "\n",
    "ax.set_ylabel('Number of Topics', fontsize=12)\n",
    "ax.set_title('Unitree G1 Academy - Course Material Coverage', fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(days)\n",
    "ax.legend(loc='upper right')\n",
    "ax.set_ylim(0, 6)\n",
    "\n",
    "# Add percentage labels\n",
    "for i, (day, total, covered, partial) in enumerate(zip(days, total_topics, covered_topics, partial_topics)):\n",
    "    coverage_pct = (covered + 0.5*partial) / total * 100\n",
    "    ax.text(i, total + 0.2, f'{coverage_pct:.0f}%', ha='center', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# Summary statistics\n",
    "total_course_topics = sum(total_topics)\n",
    "total_covered = sum(covered_topics)\n",
    "total_partial = sum(partial_topics)\n",
    "total_missing = sum(missing_topics)\n",
    "\n",
    "print(\"ðŸ“Š Course Material Coverage Statistics\\n\")\n",
    "print(f\"Total Topics in 4-Day Course: {total_course_topics}\")\n",
    "print(f\"Fully Covered in Existing Notebooks: {total_covered} ({total_covered/total_course_topics*100:.1f}%)\")\n",
    "print(f\"Partially Covered: {total_partial} ({total_partial/total_course_topics*100:.1f}%)\")\n",
    "print(f\"Require New Notebooks: {total_missing} ({total_missing/total_course_topics*100:.1f}%)\")\n",
    "print(f\"\\nOverall Coverage: {(total_covered + 0.5*total_partial)/total_course_topics*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder: Load and display course timeline\n",
    "from IPython.display import Image, display\n",
    "import os\n",
    "\n",
    "# Check if images directory exists\n",
    "images_dir = './images'\n",
    "if not os.path.exists(images_dir):\n",
    "    os.makedirs(images_dir)\n",
    "    print(f\"Created {images_dir} directory for course images\")\n",
    "\n",
    "# List of placeholder images referenced in this notebook\n",
    "placeholder_images = [\n",
    "    'g1_hardware_architecture.png',\n",
    "    'zmp_stability.png',\n",
    "    'slam_example.png',\n",
    "    'warehouse_logistics.png'\n",
    "]\n",
    "\n",
    "print(\"ðŸ“ Placeholder Images Required:\\n\")\n",
    "for img in placeholder_images:\n",
    "    img_path = os.path.join(images_dir, img)\n",
    "    exists = \"âœ…\" if os.path.exists(img_path) else \"âŒ\"\n",
    "    print(f\"{exists} {img}\")\n",
    "\n",
    "print(\"\\nðŸ’¡ Tip: Add actual images to ./images/ directory for full course materials\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
